# Analysing Spatial Patterns II: Point Pattern Analysis
This week, we will be looking at Point Pattern Analysis. With point pattern analysis, we look to detect clusters or patterns across a set of points, including measuring density, dispersion and homogeneity in our point structures. There are several approaches to calculating and detecting these clusters and today we explore several of these techniques using our [bike theft dataset from last week](06-operations.html#bike-theft-w06).

## Lecture slides {#slides-w07}
The slides for this week's lecture can be downloaded here: [\[Link\]]({{< var slides.week07 >}})

## Reading list {#reading-w07}
#### Essential readings
- Arribas-Bel, D., Garcia-López, M.-À., Viladecans-Marsal, E. 2021. Building(s and) cities: Delineating urban areas with a machine learning algorithm. *Journal of Urban Economics* 125: 103217. [[Link]](https://doi.org/10.1016/j.jue.2019.103217)
- Cheshire, J. and Longley, P. 2011. Identifying spatial concentrations of surnames. *International Journal of Geographical Information Science* 26(2), pp.309-325. [[Link]](https://doi.org/10.1080/13658816.2011.591291)
- Longley, P. *et al.* 2015. Geographic Information Science & systems, **Chapter 12**: *Geovisualization*. [[Link]](https://ucl.rl.talis.com/link?url=https%3A%2F%2Fapp.knovel.com%2Fhotlink%2Ftoc%2Fid%3AkpGISSE001%2Fgeographic-information-science%3Fkpromoter%3Dmarc&sig=e437927b963cc591dcb65491eccdd3869cc31aef80e1443cb2ba12d8f3bb031a)

#### Suggested readings
- Van Dijk, J. and Longley, P. 2020. Interactive display of surnames distributions in historic and contemporary Great Britain. *Journal of Maps* 16, pp.58-76. [[Link]](https://doi.org/10.1080/17445647.2020.1746418)
- Shi, X. 2010. Selection of bandwidth type and adjustment side in kernel density estimation over inhomogeneous backgrounds. *International Journal of Geographical Information Science* 24(5), pp.643-660. [[Link]](https://doi.org/10.1080/13658810902950625)
- Yin, P. 2020. *Kernels and density estimation*. The Geographic Information Science & Technology Body of Knowledge. [[Link]](https://doi.org/10.22224/gistbok/2020.1.12)

## Bike theft in London II {#bike-theft-w07}
This week, we again investigate bike theft in London in 2021 as we continue to look to confirm our very simple hypothesis: *that bike theft cluster in space.* This week, instead of looking at the distance of individual bike thefts from train stations, we will look to analyse the distribution of clusters and identify hotspots of bike theft.

### Getting started {#setup-w07}
Open a new script within your GEOG0030 project and save this script as `wk7-bike-theft-ppa.r`. At the top of your script, add the following metadata:

```{r}
#| label: 07-script-title
#| classes: styled-output
#| echo: True
#| eval: True
#| tidy: False
#| filename: "R code"
# Analysing bike theft in London using point pattern analysis
# Date: January 2024
```

All of the geometric operations and spatial queries we will use are contained within the `sf` library. For our Point Pattern Analysis, we will be using the `spatstat` library ("spatial statistics"). The `spatstat` library contains the different Point Pattern Analysis techniques we will want to use in this practical. We will also need the `terra` library, which provides classes and functions to manipulate geographic (spatial) data in raster format. We will use this package briefly today, but look into it in more detail in [Week 9](09-raster.html). Lastly, you will also need to load the `dbscan` package.
Now let us add all of the libraries we will be using today to the top of our script:

```{r}
#| label: 07-load-libraries
#| classes: styled-output
#| echo: True
#| eval: False
#| tidy: True
#| filename: "R code"
# load libraries
library(tidyverse)
library(sf)
library(tmap)
library(spatstat)
library(terra)
library(dbscan)
```

```{r}
#| label: 07-load-libraries-bg
#| echo: False
#| eval: True
#| warning: False
# load libraries
library(tidyverse)
library(sf)
library(tmap)
library(spatstat)
library(terra)
library(dbscan)
```

### Data preparation {#data-preparation-w07}
This week, we will continue to use the data we extracted from OpenStreetMap [last week](06-operations.html#data-preparation-w06) as well as the 2021 bike theft data that we prepared. Let's go ahead and load all of our data at once. We will also load the MSOA 

```{r}
#| label: 07-load-all-files
#| classes: styled-output
#| echo: True
#| eval: True
#| tidy: True
#| filename: "R code"
# read in our MSOA GeoPackage, create outline
msoa_london <- st_read("data/raw/boundaries/MSOA2021_London.gpkg")
outline_london <- msoa_london |>
  st_union()

# read in OSM tube and trains stations
stations_london <- st_read("data/data/LondonStations.gpkg")

# read in bike theft
bike_theft_london <- st_read("data/data/LondonBikeTheft2021.gpkg")
```
Let's create a quick map of our data to check it loaded correctly:

```{r tidy="styler"}
#| label: fig-07-check-data
#| fig-cap: Quick plot to see whether all data loaded correctly. 
#| echo: True
#| eval: True
#| cache: True
#| message: False
#| filename: "R code"
# plot our London MSOAs
tm_shape(outline_london) + 
  tm_fill() +
# then add bike crime as blue
tm_shape(bike_theft_london) + 
  tm_dots(
    col = "blue"
    ) +
# then add our stations as red
tm_shape(stations_london) + 
  tm_dots(
    col = "red"
    ) +
  # then add a north arrow
  tm_compass(
    type = "arrow", 
    position = c("right", "bottom")
    ) +
  # then add a scale bar
  tm_scale_bar(
    breaks = c(0, 5, 10, 15, 20), 
    position = c("left", "bottom")
    )
```

Great - that looks familiar! This means we can move forward with our data analysis and theoretical content for this week.

## Point pattern analysis
Point pattern analysis (PPA) studies the spatial distribution of points. PPA uses the **density**, **dispersion** and **homogeneity** in our point datasets to assess, quantify and characterise its distribution. Over the last fifty years, various methods and measurements have been developed to analyze, model, visualise, and interpret these properties of point patterns ([Qiang et al, 2020](https://gistbok.ucgis.org/bok-topics/point-pattern-analysis)).

There are three main categories of PPA techniques:

- **Descriptive statistics**: The use of descriptive statistics will provide a summary of the basic characteristics of a point pattern, such as its central tendency and dispersion. Descriptive statistics provide a simple way of visualising a dataset as a whole, from plotting the median or mean centre, or, often preferably, a standard deviational ellipse for those datasets that display a directional pattern.
- **Density-based methods**: Density-based methods focus on the *first-order* properties of a dataset, i.e. the variation in the individual locations of the points in the dataset across the area of interest, and will characterise our dataset's distribution accordingly in terms of density.
- **Distanced-based methods**: Distanced-based methods focus on the *second-order* properties of a dataset, i.e. the interactions between points within our data and whether they appear to have influence on one another and form clusters, and will characterise our dataset's distribution accordingly in terms of dispersion.

The main library to use when it comes to point pattern analysis in R is the `spatstat` library, developed by Baddeley, Rubak and Turner since 2005. As their documentation states:

> "*Spatstat is a package for the statistical analysis of spatial data. Its main focus is the analysis of spatial patterns of points in two-dimensional space*".

::: {callout-note}
According to the [Get Started with `spatstat` documentation](https://cran.r-project.org/web/packages/spatstat/vignettes/getstart.pdf) `spatstat` supports a very wide range of popular techniques for statistical analysis for spatial point patterns, including:

- Kernel estimation of density/intensity\
- Quadrat counting and clustering indices
- Detection of clustering using Ripley’s K-function
- Model-fitting
- Monte Carlo tests

We will only cover a small amount of the functionality the package offers - it has almost 1,800 pages of documentation and over 1,000 functions, so it would be near impossible to cover everything even if we had a full module dedicated just to PPA. 
::: 

Before we get started with our analysis, you need to know one critical piece of information in order to use `spatstat`: we need our data to be in the format of a `ppp` object. There are some spatial packages in R that require us to convert our data from an `sf` simple features object (e.g. for point data, a SpatialPoints object) into a different spatial object class. `spatstat` is one of them.

::: {.callout-tip}
The `ppp` format is specific to `spatstat`, but you may find it used in other spatial libraries. An object of the class `ppp` represents a two-dimensional point dataset within a pre-defined area, known as the **window of observation**, a class in its own right, known as `owin` in `spatstat`. We can either directly create a `ppp` object from a list of coordinates (as long as they are supplied with a window of observation) **or** convert from another data type.
:::

Let's turn our `bike_theft_london` dataframe into a `ppp` object:

```{r}
#| label: fig-07-bike-theft-to-ppp
#| fig-cap: "Bike theft in London represented as `ppp` object."
#| classes: styled-output
#| echo: True
#| eval: True
#| tidy: True
#| cache: True
#| filename: "R code"
# clip bike theft to the London outline
bike_theft_london <- bike_theft_london |> st_intersection(outline_london) 

# sf to ppp
window = as.owin(outline_london)
bike_theft_ppp <- ppp(st_coordinates(bike_theft_london)[,1],
                      st_coordinates(bike_theft_london)[,2],
                      window = window)

# inspect
plot(bike_theft_ppp)
```

Our plot shows us our `bike_theft_ppp` object, which includes both the coordinate points of our bike theft data and our observation window. You should also see your `bike_theft_ppp` object variable appear in your **Environment** window. You should also see a message stating that our data contains duplicated points.

One of the **key assumptions** underlying many analytical methods is that **all events are unique**. In fact, some statistical procedures actually may return very wrong results if duplicate points are found within the data. In terms of our bike theft data, it is likely that it contains duplicates. The Police service use [snapping points](https://data.police.uk/about/#anonymisation), to which crimes are snapped to in order to preserve the anonymity and privacy of those involved. This is an issue in spatial point pattern analysis as we need our `events`, i.e. each record of a theft and its respective location, to **be unique** in order for our analysis to be accurate. Let's investigate by how many duplicated points we have:

```{r}
#| label: 07-check-duplicates
#| classes: styled-output
#| echo: True
#| eval: True
#| tidy: True
#| filename: "R code"
# check for any duplicates
anyDuplicated(bike_theft_ppp)

# count number of duplicated points
sum(multiplicity(bike_theft_ppp) > 1)
```

::: {.callout-note}
We clearly have a large number of duplicated points. To account for these issues within our dataset, we have three options: 

1. We can remove the duplicates and pretend they simply are not there. However, this is feasible only when your research problem allows for this, i.e. the number of points at each location is not as important as the locations themselves or the number of duplicated points is incredibly small and will not affect any analysis.
2. Create and assign a weighting schema to our points, where each point will have an attribute that details the number of events that occur in that location and utilise this weight within our PPA techniques. Weights, however, can only be used with certain PPA techniques.
3. Force all points to be unique by utilising a function that offsets our points randomly from their current location. If the **precise** location is not important for your analysis or, for example, you are dealing with data that in our case is already slightly offset, we can introduce a "jitter" to our dataset that slightly adjusts all coordinates so that the event locations do not exactly coincide anymore. 
:::

Each approach will have a specific compromise, which you will have to decide upon depending on the type of analysis you are completing. In our case, we will choose the jitter approach to keep all of our bike theft events. We know that already the location of our bike thefts are not precise locations of the original theft, therefore adding minimal (~5 metres) additional offset will not affect our analysis.

```{r}
#| label: 07-jitter-jitter-jitter
#| classes: styled-output
#| echo: True
#| eval: True
#| tidy: True
#| filename: "R code"
# add an offset to our points
bike_theft_ppp_jitter <- rjitter(bike_theft_ppp, radius=5, retry=TRUE, nsim=1, drop=TRUE)

# check for any duplicates
anyDuplicated(bike_theft_ppp_jitter)

# count number of duplicated points
sum(multiplicity(bike_theft_ppp_jitter) > 1)
```

Great, we now have our bike theft data in a format ready to be analysed with our different PPA techniques using the `spatstat` library.

::: {.callout-tip}
One additional thing to note about the `ppp` data object is that a `ppp` object does not necessarily have to have any attributes associated with the events each point our point data represents. If your data does have attributes, these attributes are referred to as **marks** within the `spatstat` environment. Be aware that some functions do require these marks to be present.
:::

## Density-based methods
Density-based techniques are used to characterise the pattern of a point dataset utilising its general distribution. We can calculate densities at both the **global** and **local** scale. However, global densities do not tell us much about the distribution of our data. This is where local density techniques such as **Quadrat Analysis** and **Kernel Density Estimation** can help us.

### Quadrat analysis
We can create a simple understanding of our data's distribution by first understanding its **global density**. This is simply the ratio of the observed number of points $(n$ to the study region's surface area $a$:

$$\widehat{\lambda} =\frac{n}{a}$$

Calculate the global density of our bike theft point data relative to London:

```{r}
#| label: 07-global-density
#| classes: styled-output
#| echo: True
#| eval: True
#| tidy: True
#| filename: "R code"
# global density
length(bike_theft_london$geom)/sum(st_area(msoa_london))*1000
```

We can see that we have a global density of ~13 bike thefts per square kilometre. This simple density analysis could be supported with further descriptive statistics, however, we still would know little about the local density of our points. The most basic approach to understanding a point layer's local density is to simply measure the density at different locations within the study area. This approach helps us assess if the density is constant across the study area. The simplest approach to this measurement is through **Quadrat Analysis**, where the study area is divided into sub-regions. The point density is then computed for each quadrat, by dividing the number of points in each quadrat by the quadrat's area.

::: {.callout-note}
Quadrats can take on many different shapes such as hexagons or rectangles. The standard approach, however, is a grid of squares The choice of quadrat numbers and quadrat shape can influence the measure of local density and therefore must be chosen with care.
:::

We will start with a simple quadrat count by dividing the observation window into 10 x 10 sections and then counting the number of bicycle thefts:

```{r}
#| label: fig-07-quadrat-count
#| fig-cap: Quadratcount for our bike theft point dataset.
#| classes: styled-output
#| echo: True
#| eval: True
#| tidy: True
#| cache: True
#| filename: "R code"
# quadratcount
biketheft_quadrat <- quadratcount(bike_theft_ppp_jitter,nx=10,ny=10)

# inspect
plot(biketheft_quadrat)
```

Our resulting quadrat count shows total counts of bike theft. We can see quite quickly that the quadrats in central London are likely to have a higher local density as their count is much higher than those on the outskirts of London. If we divided our count by the area covered by each quadrat, we would also be able to calculate a precise local density. We will not do this for now, as realistically quadrat analysis is not used very often. The reason why we look at this technique is that it provides us with an easy way to think about how to compare our data distribution and how this relates to the [Poisson distribution](https://en.wikipedia.org/wiki/Poisson_distribution) of Complete Spatial Randomness (CSR).

::: {.callout-note}
When looking at the distribution of our points and the respective patterns they show, the key question we often want to answer as geographers is: are our points clustered, randomly distributed, uniform or dispersed? Whilst we can visually assess this distribution, to be able to statistically quantify our data's distribution, we can compare its distribution to that of the [Poisson distribution](https://en.wikipedia.org/wiki/Poisson_distribution). The Poisson distribution describes the probability or rate of an event happening over a fixed interval of time or space.

The Poisson Distribution can be used when:

- The events are discrete and can be counted in integers
- Events are independent of one another
- The average number of events over space or time is known

Point data that contains a random distribution of points is said to follow a Poisson distribution. The Poisson distribution is very useful as it allows us to compare a random expected model to our observations. Essentially, if our data's distribution does not fit the Poisson distribution, then we can infer that something interesting might be going on and our events might not actually be independent of each other. Instead, they might be clustered or dispersed and there is likely to be underlying processes influencing these patterns.
:::

We can use a Poisson distribution to generate a randomly generated point pattern dataset with the same number of points and the same observation window. We then can compare our quadrat analysis with the theoretical quadrat analysis using a [chi-squared test](https://en.wikipedia.org/wiki/Chi-squared_test), with the null hypotheses that our point data are randomly distributed. 

```{r}
#| label: 07-quadrat-test
#| classes: styled-output
#| echo: True
#| eval: True
#| tidy: True
#| cache: True
#| filename: "R code"
# quadrat test
quadrat.test(bike_theft_ppp_jitter,nx=10,ny=10)
```

Our $p$ value is well below 0.01, which means there is a statistically significant difference between the expected random distribution as drawn from a Poisson distribution and the observed distribution. 

### Kernel Density Estimation
We now have a basic confirmation that our data is not randomly distributed. Instead of looking at the distribution of our bike theft with the boundaries of our quadrats, we can analyse our points using a **Kernel Density Estimation (KDE)**. KDE is a statistical technique to generate a smooth continuous distribution between data points that represent the density of the underlying pattern.

::: {.callout-tip}
A KDE will produce a raster surface that details the estimated distribution of our event point data over space. Each cell within our raster contains a value that is this estimated density at that location; when visualised in its entirety as the whole raster, we can quickly identify areas of high and low density, i.e. where are clusters are located in our dataset. To create this surface, a KDE computes a localised density for small subsets of our study area but unlike quadrat analysis, these subsets overlap one another to create a *moving* sub-region window, defined by a **kernel**. A kernel defines the shape and size of the window and can also weight the points, using a defined kernel function. The simplest kernel function is a basic kernel where each point in the kernel window is assigned equal weight. The kernel density approach generates a grid of density values whose cell size is smaller than that of the kernel window. Each cell is assigned the density value computed for the kernel window centered on that cell. The resulting surface is created from these individually, locally calculated density values.
:::

Producing a KDE in R is very straight-forward in `spatstat`, using your `ppp` object and the `density.ppp()` function. However, you will need to consider both the **bandwidth** or diameter of your Kernel (`sigma`) and whether you want to apply a weighting to your points using a function. First, let's go ahead and create a simple KDE of bike theft with our bandwidth set to 100m:

```{r}
#| label: fig-07-bike-theft-kde100
#| fig-cap: "Kernel density estimation - bandwidth 100m."
#| classes: styled-output
#| echo: True
#| eval: True
#| tidy: True
#| cache: True
#| filename: "R code"
# kernel density estimation
plot(density.ppp(bike_theft_ppp_jitter, sigma=100))
```

We can see from just our KDE that there are visible clusters present within our bike theft data, particularly in and around central London. We can go ahead and vary our bandwidth to to see how that affects the density estimate:

```{r}
#| label: fig-07-bike-theft-kde500
#| fig-cap: "Kernel density estimation - bandwidth 500m."
#| classes: styled-output
#| echo: True
#| eval: True
#| tidy: True
#| cache: True
#| filename: "R code"
# kernel density estimation
plot(density.ppp(bike_theft_ppp_jitter, sigma=500))
```

Our clusters now appear brighter and larger than our KDE with a 100m bandwidth. This is because changing the bandwidth enables your KDE to take into account more points within its calculation, resulting in a smoother surface. However, there are issues with oversmoothing your data - as you can see above, our clusters are not as well defined and therefore we may attribute high levels of bike theft to areas where there actually is not that much. Smaller bandwidths will lead to a more irregular shaped surface, where we have more precision in our defined clusters but, once again, there are issues of undersmoothing. 

::: {.callout-tip}
Whilst there are automated functions (e.g. based on maximum-likelihood estimations) that can help you with selecting an appropriate bandwidth, in the end you will have to make a decision on what is most appropriate for your dataset.
:::

Although bandwidth typically has a more pronounced effect upon the density estimation than the type of kernel used, kernel types can affect the result too. When we use a different kernel type, we are looking to weight the points within our kernel differently:

```{r}
#| label: fig-kernels
#| echo: False 
#| fig-cap: "Kernel types and their distributions. [[Enlarge image]](https://jtvandijk.github.io/GEOG0030/images/w07/kerneltypes.png){target='_blank'}"
knitr::include_graphics('images/w07/kerneltypes.png')
```

Each function will result in [a slightly different estimation. Deciding which function is most suitable for your analysis will all depend on what you are trying to capture. We can compare and see the impact of different functions on our current dataset looking at the default kernel in `density.ppp()`, which *gaussian*, alongside the *epanechnikov*, *quartic* or *disc* kernels. 

::: {.callout-tip}
To change the kernel within your KDE, you simply need to add the `kernel` parameter and set it to one of the kernels available, denoted as a string, e.g. *epanechnikov*, *quartic*, *disc*. Ultimately, however, bandwidth will have a more marked effect upon the density estimation than kernel type.
:::

```{r}
#| label: fig-07-bike-theft-kde500-gaussian
#| fig-cap: "Kernel density estimation - bandwidth 400m, Gaussian kernel."
#| classes: styled-output
#| echo: True
#| eval: True
#| tidy: True
#| cache: True
#| filename: "R code"
# kernel density estimation
plot(density.ppp(bike_theft_ppp_jitter, sigma=400, kernel ="gaussian"), main='Gaussian')
```

```{r}
#| label: fig-07-bike-theft-kde500-epanechnikov
#| fig-cap: "Kernel density estimation - bandwidth 400m, Epanechnikov kernel."
#| classes: styled-output
#| echo: True
#| eval: True
#| tidy: True
#| cache: True
#| filename: "R code"
# kernel density estimation
plot(density.ppp(bike_theft_ppp_jitter, sigma=400, kernel ="epanechnikov"), main='Epanechnikov')
```

```{r}
#| label: fig-07-bike-theft-kde500-quartic
#| fig-cap: "Kernel density estimation - bandwidth 400m, Quartic kernel."
#| classes: styled-output
#| echo: True
#| eval: True
#| tidy: True
#| cache: True
#| filename: "R code"
# kernel density estimation
plot(density.ppp(bike_theft_ppp_jitter, sigma=400, kernel ="quartic"), main='Quartic')
```

```{r}
#| label: fig-07-bike-theft-kde500-disc
#| fig-cap: "Kernel density estimation - bandwidth 400m, Disc kernel."
#| classes: styled-output
#| echo: True
#| eval: True
#| tidy: True
#| cache: True
#| filename: "R code"
# kernel density estimation
plot(density.ppp(bike_theft_ppp_jitter, sigma=400, kernel ="disc"), main='Disc')
```

We can be quite confident in stating that bike theft in London in 2021 is not a spatially random process and we can clearly see the areas where bicycle theft is most concentrated. How can we use this new data in our original analysis that looks to find out whether bike theft primarily occurs near tube and train stations? KDEs are primarily used for visual analysis of point data distribution. What we can do, however, is improve our visualisation and making it into a proper map using `tmap`. 

```{r}
#| label: 07-to-raster
#| classes: styled-output
#| echo: True
#| eval: True
#| tidy: True
#| cache: True
#| filename: "R code"
# to raster, clip by London outline
kde_400g_raster <- density.ppp(bike_theft_ppp_jitter, sigma = 400) |>
  rast()
```

We now have a standalone raster we can use with any function in the `tmap` library. Before we go ahead, one issue we will face is that our resulting raster does not have a Coordinate Reference System, so we need to manually add this information to the raster object:

```{r}
#| label: 07-to-raster-crs
#| classes: styled-output
#| echo: True
#| eval: True
#| tidy: True
#| cache: True
#| filename: "R code"
# check CRS
st_crs(kde_400g_raster)
```

You should see an `NA` appear within our CRS arguments, so we need to fix this:

```{r}
#| label: 07-to-raster-crs-set
#| classes: styled-output
#| echo: True
#| eval: True
#| tidy: True
#| cache: True
#| filename: "R code"
# add CRS
crs(kde_400g_raster) <- 'epsg:27700'

# check CRS
st_crs(kde_400g_raster)
```

Now we have our raster data ready to map:

```{r tidy="styler"}
#| label: fig-kde-raster-map
#| fig-cap: Basic map using raster data using the `tmap` library. 
#| echo: True
#| eval: True
#| message: False
#| filename: "R code"
# map kde raster
tm_shape(kde_400g_raster) + 
  tm_raster(
    col = 'lyr.1', 
    palette = 'Blues'
    ) 
```

With our `kde_400g_raster` using `tmap`, we can go ahead and customise our map as we would do with our vector mapping such as changing the legend, adding a title, north arrow, etc. --- and, of course, our `stations_london` spatial layer.

## Distance-based methods
We have spent a good amount of time looking at using density-based methods to quantify whether our point data is randomly distributed and visualise and identify high and low areas of density, showing if our data is clustered and where. **Distance-based** methods, on the other hands, allow us to quantify the second order properties of our data, i.e. the influence the location of these points have on one another.

::: {.callout-note}
Distance-based measures analyse the spatial distribution of points using distances between point pairs, with the majority using Euclidean distance, to determine quantitatively whether our data is, again, randomly distributed or shows sign of clustering or dispersion. These methods are a more rigorous alternative to using the Quadrat Analysis approach, and enables us to assess clustering within our point data at both a global and local scale. For the remainder of this practical, we look at one distance-based measure (**average nearest neighbour**) as well as a well-known hybrid technique (**DBSCAN**) 
:::

### Average Nearest Neighbour
Average Nearest Neighbour (ANN) is the average distance between all points within a dataset and their individual nearest point. ANN is used as a global indicator to measure the overall pattern of a point set. The ANN of a given point collection can be compared with the expected ANN from points following a random distribution to test whether our point data is clustered or dispersed. The approach is similar to that of the Quadrat Analysis simulation we saw above, but by using distance rather than density grouped to arbitrary quadrats, ANN is likely to be a more robust quantification of our point distribution. 

We can calculate the ANN for our dataset by using the `nndist()` function from the `spatstat` library:

```{r}
#| label: 07-ann
#| classes: styled-output
#| echo: True
#| eval: True
#| tidy: True
#| cache: True
#| filename: "R code"
# calculate the average distance to nearest neighbour
mean(nndist(bike_theft_ppp_jitter, k=1))
```

We can see that the average nearest neighbour for all points is 53.5 metres. To get an insight into the spatial ordering of all our points relative to one another we can plot the ANN values for different order neighbours (i.e. first closest point, second closest point, etc. For point patterns that are highly clustered, we would expect the average distances between points to be very small. 

Calculate the average nearest neighbour to the $k$ nearest neighbours for our bike theft data:

```{r}
#| label: fig-07-ann
#| fig-cap: "Average distances to the nearest 100 neighbours."
#| classes: styled-output
#| echo: True
#| eval: True
#| tidy: True
#| cache: True
#| filename: "R code"
# calculate the average distance to the nearest 100 neighbours
bike_theft_ann <- apply(nndist(bike_theft_ppp_jitter, k=1:100),2,FUN=mean)

# plot
plot(bike_theft_ann ~ seq(1:100))
```

In our case, the plot does not reveal anything interesting in particular except that higher order points seem to be slightly closer than lower order points. Overall, the ANN is a good approach to conduct statistical tests on large datasets (e.g. if we were to compare the result to a theoretical distribution), but visually it does not tell us a huge amount about our dataset!

### DBSCAN
The techniques above are useful exploratory techniques for telling us if we have spatial clusters present in our point data, but they are not able to tell us precisely *where* in our area of interest the clusters are occurring. One popular technique for discovering precise clusters in space is an algorithm known as **DBSCAN**, an algorithm that incorporates both distance and density. 

::: {.callout-tip}
For the complete overview of the DBSCAN algorithm, you can refer to the original paper by [Ester *et al*. (1996)](https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf). Whilst DBSCAN is a relatively old algorithm, there has been a substantial [resurgence](https://dl.acm.org/doi/10.1145/3068335) in its use within spatial data science (e.g. see [Arribas-Bel *et al.* 2019](https://doi.org/10.1016/j.jue.2019.103217)).
:::

We can use DBSCAN to detect clusters within our bike theft dataset and then use the clusters to further answer our original research question. DBSCAN takes two parameters:

- The minimum number of points: `MinPts`.
- The distance: `epsilon`.

Across a set of points, DBSCAN will group together points that are close to each other based on a distance measurement and a minimum number of points. It also marks as outliers the points that are in low-density regions. The algorithm can be used to find associations and structures in data that are hard to find through visual observation alone, but that can be relevant and useful to find patterns and predict trends. However, DBSCAN will only work well if you are able to successfully define the distance and minimum points parameters and your clusters do not vary considerably in density.

We can conduct a DBSCAN analysis using the `dbscan` library. For our analysis, we will set our `epsilon` to 200m and then set our minimum cluster size to 20 bike thefts.

```{r}
#| label: 07-dbscan
#| classes: styled-output
#| echo: True
#| eval: True
#| tidy: True
#| cache: True
#| filename: "R code"
# run dbscan
bike_theft_dbscan <- bike_theft_london |>
  st_coordinates() |>
  dbscan(eps = 200, minPts = 20) 
```

::: {.callout-tip}
The `dbscan()` function only accepts a data matrix or dataframe of points as input rather than a spatial dataframe. This is why in the above code we pass the `st_coordinates()` function to extract the projected coordinates.
:::

The DBSCAN output contains three objects, including a vector detailing the cluster for each of our bike theft observations. To be able to work with our DBSCAN output effectively and, for example, plot the clusters as individual polygons, we need to add our cluster groups back to into our original point dataset. Because the DBSCAN output does not change the order of points, we can simply stick the cluster output on the `bike_theft_london` spatial dataframe.

```{r}
#| label: 07-dbscan-add-clusters
#| classes: styled-output
#| echo: True
#| eval: True
#| tidy: True
#| cache: True
#| filename: "R code"
# add the cluster numbers as column
bike_theft_london <- bike_theft_london |>
  mutate(dbcluster = bike_theft_dbscan$cluster)
```

Now we have each of our bike theft points in London associated with a specific cluster, we can generate a polygon that represents these clusters in space, as we can sort of see in our above plot. To do so, we will utilise a geometric operation from the `sf` package: the `st_convex_hull()` function. The `st_convex_hull()` function can be used to create a polygon that represents the minimum coverage of our individual clusters. We can use this function to create a polygon that represents the geometry of all points within each cluster. To enable this, we will use something called a `for` loop. A `for` loop is used to repeat a specific block of code a known number of times.

::: {.callout-warning}
The code below is a bit complex, but essentially we repeat the following steps for each cluster that the DBSCAN algorithm identified:

- Filter the `bike_theft_london` spatial dataframe by cluster number.
- Combine all points belonging to the same cluster into a single set of geometry observations.
- Calculate the convex hull of that single set of geometry observations.
- Store the resulting polygon in a list.

Once we have looped through all clusters, we end up with a final list containing all the geometries of our cluster polygons which we can convert into a spatial dataframe.
:::

Run a `for` loop to generate a polygon dataset that represents our bike theft clusters:

```{r}
#| label: 07-dbscan-for-loop
#| classes: styled-output
#| echo: True
#| eval: True
#| tidy: True
#| cache: True
#| filename: "R code"
# create an empty list to store the resulting convex hull geometries
# set the length of this list to the total number of clusters found
geometry_list <- vector(mode = "list", length = max(bike_theft_london$dbcluster))

# create a counter to keep track
counter <-0

# begin loop
for (cluster_index in seq(0, max(bike_theft_london$dbcluster))) {

   # filter to only return points for belonging to cluster n
   biketheft_cluster_subset <- bike_theft_london |>
     filter(dbcluster == cluster_index)

   # union points, calculate convex hull
   cluster_polygon <- biketheft_cluster_subset |>
     st_union() |>
     st_convex_hull()

   # add the geometry of the polygon to our list
   geometry_list[counter] <- (cluster_polygon)

   # update the counter
   counter <- counter + 1
}

# combine the list
bike_theft_clusters <- st_sfc(geometry_list, crs = 27700)
```

We now have a polygon spatial dataframe, `bike_theft_clusters`, that show the general location and distribution of bike theft clusters in London. Let's put them on a map:

```{r tidy="styler"}
#| label: fig-07-map-clusters
#| fig-cap: Basic map of identified DBSCAN bike theft clusters. 
#| echo: True
#| eval: True
#| message: False
#| filename: "R code"
# add London outline
tm_shape(outline_london) + 
    tm_borders() +
# add bike theft clusters
tm_shape(bike_theft_clusters) + 
  tm_polygons() +
  # add basemap
  tm_basemap(c(StreetMap = "OpenStreetMap"))
```

## Assignment {#assignment-w07}
We have conducted a lot of analysis today. What we have failed to do, however, is to make proper maps. For this week's assignment, create two publishable maps with all the trimmings using the main outputs from today's tutorial:

1. A Kernel Density Map of bike theft in London.
2. A cluster map of bike theft in London, using the DBSCAN output.

::: {.callout-tip}
- Because the thefts concentrate in central London why not consider zooming into a specific area?
- Why not contextualise the densities and clusters by including the `stations_london` layer?
:::

## Want more? [Optional] {#wm-w07}
### Reproducible research {.unnumbered}
Last week you might have had a look at the book [Data Skills for Reproducible Research](https://psyteachr.github.io/reprores-v3/index.html). A great tool that can be used to create reproducible workflows is found in the `targets` package. The targets package is a [Make](https://www.gnu.org/software/make/)-like pipeline tool for Statistics and data science in R. With targets, you can maintain a reproducible workflow without repeating yourself. To get started: have a look at the [Walkthrough](https://books.ropensci.org/targets/walkthrough.html) chapter to see `targets` in action. 

## Before you leave {#byl-w07}
As geographers we are keen to understand our point data's distribution and understand whether are our points clustered, randomly distributed, uniform or dispersed. We have looked at various techniques that enable us to statistically and visually assess our data's distribution and understand whether our data is randomly distributed or clustered in space. And that is [us done for this week](https://www.youtube.com/watch?v=-zxtbwGogyY). Reading list anyone?
