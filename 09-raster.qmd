# Rasters, Zonal Statistics, and Interpolation
The majority of our module has focused on the use of vector data and tabular data. This week, we switch it up by focusing primarily on raster data and its analysis using map algebra and zonal statistics.

## Lecture slides {#slides-w09}
The slides for this week's lecture can be downloaded here: [\[Link\]]({{< var slides.week09 >}})

## Reading list {#reading-w09}
#### Essential readings 
- Gimond, M. 2021. Intro to GIS and spatial analysis. **Chapter 14**: *Spatial Interpolation*. [[Link]](https://mgimond.github.io/Spatial/spatial-interpolation.html)
- Heris, M., Foks, N., Bagstad, K. 2020. A rasterized building footprint dataset for the United States. *Scientific Data* 7: 207. [[Link]](https://doi.org/10.1038/s41597-020-0542-3)
- Thomson, D., Leasure, D., Bird, T. *et al*. 2022. How accurate are WorldPop-Global-Unconstrained gridded population data at the cell-level?: A simulation analysis in urban Namibia. *Plos ONE* 17:7: e0271504. [[Link]](https://doi.org/10.1371/journal.pone.0271504)

#### Suggested readings 
- Mellander, C., Lobo, J., Stolarick, K. *et al*. 2015. Night-time light data: a good proxy measure for economic activity? *PLoS ONE* 10(10): e0139779. [[Link]](https://doi.org/10.1371/journal.pone.0139779)

## Raster data
In previous weeks, we have predominantly worked with **vector data** and/or **tabular data** that we then join to vector data for analysis. However, depending on the nature of your research problem, you may also encounter **raster data**. This week's content introduces you to raster data, map algebra and interpolation. After first looking at population change in London using raster data, we will then look at generating pollution maps in London from individual point readings taken from air quality monitoring sites across London. To complete this analysis, we will be using several new datasets:

1. **Population rasters for Great Britain**: [WorldPop](https://hub.worldpop.org/) raster on estimated population counts for Great Britain in 2010 and 2020 at a spatial resolution of 1km.
2. **NO~2~ readings across London**: A dataset contain readings of NO~2~ for individual air quality monitoring sites in London.

```{r}
#| label: fig-raster-vector
#| echo: False
#| fig-cap: "A hypothetical raster and a vector model of landuse. [[Enlarge image]](https://jtvandijk.github.io/GEOG0030/images/w09/raster-vector.png){target='_blank'}"
knitr::include_graphics('images/w09/raster-vector.png')
```

::: {.callout-note}
The main difference between vector and raster models is how they are structured. Our vectors are represented by three different types of geometries: points, lines and polygons. We have used point data in the form of our stations and bike theft, and polygons in the form of our ward and borough boundaries. In comparison, our raster datasets are composed of pixels (or grid cells) --- a bit like an image. This means that a raster dataset represents a geographic phenomenon by dividing the world into a set of rectangular cells that are laid out in a grid. Each cell holds one value that represents the value of that phenomena at the location, e.g. a population density at that grid cell location. In comparison to vector data, we do not have an *attribute table* containing fields to analyse. All analysis conducted on a raster dataset therefore is primarily conducted on the cell values of a raster, rather than on the attribute values of the observations contained within our dataset or the precise geometries of our dataset. Probably one of the most common or well-known types of raster data are those that we can derive from remote sensing, including satellite and RADAR/LIDAR imagery that we see used in many environmental modelling applications, such as land use and pollution monitoring.
::: 

### Getting started {#setup-w08}
Open a new script within your GEOG0030 project and save this script as `wk9-raster-analysis.r`. At the top of your script, add the following metadata:

```{r}
#| label: 09-script-title
#| classes: styled-output
#| echo: True
#| eval: True
#| tidy: False
#| filename: "R code"
# Raster analysis 
# Date: January 2024
```

Now let us add all of the libraries we will be using today to the top of our script:

```{r}
#| label: 09-load-libraries
#| classes: styled-output
#| echo: True
#| eval: False
#| tidy: True
#| filename: "R code"
# load libraries
library(tidyverse)
library(sf)
library(tmap)
library(stars)
library(terra)
```

```{r}
#| label: 08-load-libraries-bg
#| echo: False
#| eval: True
#| warning: False
# load libraries
library(tidyverse)
library(sf)
library(tmap)
library(stars)
library(terra)
```

For the first part of this week's practical material we will be using raster datasets from [WorldPop](https://hub.worldpop.org/): 

> "*WorldPop develops peer-reviewed research and methods for the contstruction of open and high-resolution geosapatial data on population distributions, demogrpaphics and dynamics, with a focus on low and middle income countries.*"

These population surfaces are estimates of counts of people, displayed within a regular grid raster of a spatial resolution of up to 100m. These surfaces can be used to explore, for example, changes in the demographic profiles of small areas, area deprivation, or country of birth.

1. Navigate to the WorldPop Hub*: [[Link]](https://hub.worldpop.org/)
2. Go to **Population Count** -> **Unconstrained individual countries 2000-2020 (1km resoultion)**.
3. Type *United Kingdom* in the search bar.
4. Download the [GeoTIFF](https://en.wikipedia.org/wiki/GeoTIFF) files for **2010** and **2020**: `gbr_ppp_2010_1km_Aggregated` and `gbr_ppp_2020_1km_Aggregated`.
5. Save the file in your `population` folder.

### Map algebra
**Map algebra** is a set-based algebra for manipulating geographic data, coined by [Dana Tomlin](https://en.wikipedia.org/wiki/Dana_Tomlin) in the early 1980s. Map algebra uses maths-like operations, including addition, subtraction and multiplication to update raster cell values - depending on the output you're looking to achieve. The most common type of map algebra is to apply these operations using *a cell-by-cell function*. These operations might include:

- **Arithmetic operations** that use basic mathematical functions like addition, subtraction, multiplication and division.
- **Statistical operations** that use statistical operations such as minimum, maximum, average and median.
- **Relational operations** which compare cells using functions such as greater than, smaller than or equal to.

::: {.callout-note}
The utilisation of these functions can enable many different types of specialised raster analysis, such as recoding or reclassifying indivdual rasters to reduce complexity in their data values, generating the [Normalised Difference Vegetation Index](https://en.wikipedia.org/wiki/Normalized_difference_vegetation_index) for a satellite imagery dataset or calculating [Least Cost Surfaces](https://pro.arcgis.com/en/pro-app/latest/tool-reference/spatial-analyst/creating-a-cost-surface-raster.htm) to find the most efficient path from one cell in a raster to another. Furthermore, using multiple raster datasets, it is possible to combine these data through mathematical overlays, from the basic mathematical operations mentioned above to more complex modelling..
::: 

We will be using some simple map algebra to look at population change in London between 2010 and 2020. Let's get started and take a look at our data. First we need to load it into R (using the `terra` library) and then we can quickly plot it using the `base` plot function:

```{r}
#| label: 09-load-raster-data
#| classes: styled-output
#| echo: True
#| eval: True
#| tidy: True
#| filename: "R code"
# load data
pop2010 <- rast('data/raw/population/gbr_ppp_2010_1km_Aggregated.tif')
pop2020 <- rast('data/raw/population/gbr_ppp_2020_1km_Aggregated.tif')

# transform projection
pop2010 <- pop2010 |> project("epsg:27700")
pop2020 <- pop2020 |> project("epsg:27700")
```

```{r}
#| label: fig-09-load-raster-data-2010
#| fig-cap: "WorldPop 2010 population estimates for the UK."
#| classes: styled-output
#| echo: True
#| eval: True
#| tidy: True
#| filename: "R code"
# plot 2010
plot(pop2010)
```

```{r}
#| label: fig-09-load-raster-data-2020
#| fig-cap: "WorldPop 2020 population estimates for the UK."
#| classes: styled-output
#| echo: True
#| eval: True
#| tidy: True
#| filename: "R code"
# plot 2020
plot(pop2020)
```

You should see that whilst your maps look very similar, the legend certainly shows that the values associated with each cell has grown over the 10 years between 2010 and 2021: we see our maximum increase from about 12,000 people per cell to well-over 14,000 people per cell. Now we have our raster data loaded, we want to reduce it to show only the extent of London. 

::: {.callout-note}
The `terra` package does not take in `sf` objects, so once we have loaded the London MSOA file we need to transform the file into a `SpatRaster` or `SpatVector`. The process of turning a vector dataset into a raster dataset is called rasterising. 

```{r}
#| label: fig-rasterise-a-file
#| echo: False
#| fig-cap: "Rasterising a line geometry. Source: [Lovelace *et al*. 2023](https://r.geocompx.org/raster-vector#rasterization)."
knitr::include_graphics('images/w09/rtovector.png')
```
:::

```{r}
#| label: 09-confe-them-area
#| classes: styled-output
#| echo: True
#| eval: True
#| tidy: True
#| filename: "R code"
# load data, get outline, rasterise
msoa_london <- st_read('data/raw/boundaries/MSOA2021_London.gpkg') |>
  vect()

# crop
pop2010_london <- crop(pop2010, msoa_london)
pop2020_london <- crop(pop2020, msoa_london)

# mask
pop2010_london <- mask(pop2010_london, msoa_london)
pop2020_london <- mask(pop2020_london, msoa_london)
```

```{r}
#| label: fig-09-load-raster-data-2010-lon
#| fig-cap: "WorldPop 2010 population estimates for London."
#| classes: styled-output
#| echo: True
#| eval: True
#| tidy: True
#| filename: "R code"
# plot 2010
plot(pop2010_london)
```

```{r}
#| label: fig-09-load-raster-data-2020-lon
#| fig-cap: "WorldPop 2020 population estimates for London."
#| classes: styled-output
#| echo: True
#| eval: True
#| tidy: True
#| filename: "R code"
# plot 2020
plot(pop2020_london)
```

Now we have our two London population rasters, we can calculate population change between the two timeperiods by subtracting our 2010 population raster from our 2020 population raster:

```{r}
#| label: fig-09-substract-london
#| fig-cap: "Population change in London 2010-2020."
#| classes: styled-output
#| echo: True
#| eval: True
#| tidy: True
#| filename: "R code"
# subtract
lonpop_change <- pop2020_london - pop2010_london

# plot
plot(lonpop_change)
```

### Zonal statistics
To further analyse our population change raster, we can create a smoothed version of our `lonpop_change` raster by using the `focal()` function. Using the `focal()` function, we generate a raster that summarises the average (mean) value of the **9** nearest neighbours for each cell, using a weight matrix defined in our `w` parameter and set to a `matrix`:

```{r}
#| label: fig-09-focus-on-the-hood
#| fig-cap: "Smoothed version of population change in London 2010-2020."
#| classes: styled-output
#| echo: True
#| eval: True
#| tidy: True
#| filename: "R code"
# subtract
lonpop_smooth <- focal(lonpop_change,w=matrix(1,3,3),fun=mean) 

# plot 
plot(lonpop_change)
```

The differences are not very noticeable, but you were to subtract the smoothed raster from the original raster you will see that definitely something happened:

```{r}
#| label: fig-09-focus-on-the-smooth
#| fig-cap: "Difference smoothed population change with original population change raster."
#| classes: styled-output
#| echo: True
#| eval: True
#| tidy: True
#| filename: "R code"
# plot the results
plot(lonpop_change - lonpop_smooth)
```

We can also look to use zonal functions to better represent our population change by aggregating our data to coarser resolutions. For example, we can resize our raster's spatial resolution to contain larger grid cells which will, of course, simplify our data, making larger trends more visible in our data but may similarly end up obfuscating smaller trends. 

::: {.callout-note}
We can resize our `lonpop_change` raster by using the `aggregate()` function and setting the `fact` (factor) parameter to the *order* of rescaling we would like (e.g. increase both the width and height of a cell by a factor of two). We then provide the `fun` (function) by which to aggregate our data, in this case, we will continue to use the `mean` but we could also use the `min` or `max` depending on our application.
:::

```{r}
#| label: fig-09-aggregate-the-raster
#| fig-cap: "Aggregated cell values." 
#| classes: styled-output
#| echo: True
#| eval: True
#| tidy: True
#| filename: "R code"
# aggregate
lonpop_agg <- aggregate(lonpop_change, fact=2, fun=mean) 

# plot 
plot(lonpop_agg)
```
Where we transformed a vector dataset into a raster dataset earlier, in some cases you would want to aggregate move from raster to vector. For example, in our case, we can aggregate the `lonpop_change` raster to our actual London MSOA boundaries, i.e. calculate for each MSOA in our dataset the average (or other function) population change,. We can, of course, use other functions other than the `mean`. What function you use will simply depend on your application. 

```{r tidy="styler"}
#| label: fig-09-aggregate-the-raster-to-vector
#| fig-cap: "Aggregating raster values to a vector geography."
#| classes: styled-output
#| echo: True
#| eval: True
# aggregate 
london_msoa_pop <- extract(lonpop_change, msoa_london, fun=mean)

# add to spatial dataframe
msoa_london <- msoa_london |> 
  st_as_sf() |>
  mutate(pop_change = london_msoa_pop$gbr_ppp_2020_1km_Aggregated)

# plot
tm_shape(msoa_london) +
  tm_fill(
    col = 'pop_change'
  )
```

We now have a vector dataset that we could go ahead and run many of the analyses that we have completed in previous weeks. Furthermore, we can use this data within other analyses we might want to complete. 

::: {.callout-tip}
Trying to calculate population change, particularly across decades as we have done here, can be quite challenging with changing administrative boundaries. Using raster data can be a good workaround to these issues, provided that the different rasters are of same size and extent.
:::

## Interpolation

<!-- #### Pollution data -->
<!-- For the second part of this week's practical material, we will explore several methods of interpolation by looking at air pollution in London by getting data from the [Londonair](https://www.londonair.org.uk/LondonAir/General/about.aspx) website. **Londonair** is the website of the London Air Quality Network (LAQN), and shows air pollution in London and south east England that is provided by the [Environmental Research Group](https://www.imperial.ac.uk/school-public-health/environmental-research-group/) of Imperial College London. The data are captured by hundreds of sensors at various continuous monitoring sites in London and the south east of England. The data are publicly available for download and we can use an R package to directly interact with the data without needing to download it. The `openair` [R package](https://davidcarslaw.github.io/openair/) enables us to import data directly form the Londonair website.  -->

<!-- :::note -->
<!-- **Note** <br /> -->
<!-- The `openair` library can be a bit fiddly at times: to make things easy for us, you can simply download a copy of the data below. -->
<!-- ::: -->

<!-- | Pollution Data                            | Type         | Link | -->
<!-- | :------                                   | :------      | :------ | -->
<!-- | Air pollution in London for 2019 (NO~2~)  | `csv`        | [Download](https://github.com/jtvandijk/GEOG0030/tree/master/data/zip/no2_london_2019.zip) | -->

<!-- Once downloaded, copy over these files into a `data/raw/pollution` folder. Please note that the file is rather larger (~170 MB) and it is best to keep it as `.zip` file. -->

<!-- The second half of this week's tutorial focuses on interpolation. Spatial interpolation is the prediction of a given phenomenon in unmeasured locations. There are many reasons why we may wish to interpolate point data across a map. It could be because we are trying to predict a variable across space, including in areas where there are little to no data. We might also want to smooth the data across space so that we cannot interpret the results of individuals, but still identify the general trends from the data. This is particularly useful when the data corresponds to individual persons and disclosing their locations is unethical.  -->

<!-- To predict the values of the cells of our resulting raster, we need to determine how to interpolate between our points, i.e. develop a set of procedures that enable us to calculate predicted values of the variable of interest with confidence     and, of course, repetitively. -->
<!-- We will put some techniques into action by interpolating our air quality point data into a raster surface to understand further how air pollution varies across London. -->

<!-- ### Loading data {#loading-data-w09-2} -->
<!-- Before we get going within interpolating our pollution dataset, let's first take a look at the distribution of the London Air monitoring sites in London. What are your thoughts about the distribution of the sites? Do you think they will provide enough data for an accurate enough interpolation?  -->

<!-- ```{r 09-monitoring-in-london, out.width="550pt", echo=FALSE, fig.align='center', fig.cap='Locations of the London Air monitoring sites in London. Source: [Londonair 2020](https://www.londonair.org.uk/london/asp/publicdetails.asp).'} -->
<!-- knitr::include_graphics('images/w09/london_air.png') -->
<!-- ``` -->

<!-- Ultimately, monitoring sites and the sensor stations present at them can be expensive to install and run, therefore, identifying the most important places for data collection will somewhat determine their location, alongside trying to create a somewhat even distribution over London. As we can see in the locations of the stations above, there are certainly some areas in London that do not have a station nearby, whilst others (such as central London) where there are many stations available. -->

<!-- When using interpolation, the distribution and density of our data points will impact the accuracy of our final raster and we may end up with a level of uncertainty in the areas where data is more sparse, such as the north-west and the south-east of London. Despite this, we can still create an interpolated surface for our pollutant of interest, we just need to interpret our final raster with acknowledgement of these limitations. For this week's practical, we will go ahead and use the Londonair's data to study the levels of **Nitrogen Dioxide (NO~2~) in London for 2019**. Once we have our data loaded and processed in the right format, we will start interpolating our data using at first two models: Thiessen Polygons and Inverse Distance Weighting. -->

<!-- ```{r 09-get-data-from-zip, warnings=FALSE, message=FALSE, cache=TRUE, tidy='styler'} -->
<!-- # read in downloaded data  -->
<!-- # as the file is quite large, we will read it directly from zip -->
<!-- pollution <- read_csv("data/raw/pollution/no2_london_2019.zip") -->

<!-- # pollution dataframe dimensions -->
<!-- dim(pollution) -->
<!-- ``` -->

<!-- Reading in the `csv` might take a little time: we have 1,596,509 observations with 7 variables. -->

<!-- ### Analysing air pollution -->
<!-- Let's take a look at why it's so large and inspect the first five rows of our dataframe: -->

<!-- ```{r 09-inspect-inspect, warnings=FALSE, message=FALSE, cache=TRUE, tidy=TRUE} -->
<!-- # return first five rows of our pollution dataframe -->
<!-- head(pollution) -->
<!-- ``` -->

<!-- We can see that in our first five rows we have data for the **same site** and if we look at the **date** field, we can see we have a reading observation for every hour. With 24 hours in the day, 365 days in a year and potentially hundreds of sites, it should therefore be of no surprise that we have such a big `csv`. In the end, for this practical, we only want to create one raster, so to make our data more useable we will go ahead and aggregate the data and get the average NO~2~ value for each monitoring site over 2019.  -->

<!-- Use the `dplyr` library functions to return the mean NO~2~ value for each monitoring site over 2019. Let's also make sure that we retain the latitude and longitude of our monitoring sites: -->

<!-- ```{r 09-that-is-mean, warnings=FALSE, message=FALSE, cache=TRUE, tidy=TRUE} -->
<!-- # aggregate data to unique latitude and longitude combinations, remove monitoring sites without coordinates, summarise the no2 by the mean -->
<!-- avg_pollution <- pollution %>% group_by(latitude,longitude) %>%  -->
<!--                          summarise(no2=mean(no2)) %>%  -->
<!--                          filter(!is.na(latitude | longitude)) -->

<!-- # return the first five rows of our new dataframe -->
<!-- head(avg_pollution) -->

<!-- # return the histogram of our no2 values -->
<!-- hist(avg_pollution$no2) -->
<!-- ``` -->

<!-- We should now see that we only have our latitude and longitude coordinates and the average NO~2~ value associated with each. Our histogram also shows us the general distribution of our values: we can see that we have a slight positive skew to our dataset. To use this data within our different interpolation methods, we will need to transform our data into a point spatial dataframe using the `st_as_sf()` function that we have come across before. One thing you should notice is that the latitude and longitude are projected in WGS84 and, therefore, we need to reproject our resulting spatial dataframe into British National Grid. We will also make sure that all of our points are within our London ward extent, using the `st_intersection()` function. -->

<!-- Create a spatial dataframe containing our London monitoring sites and their average NO~2~ reading: -->

<!-- ``` {r 09-prepare-air-pol-data-further, warnings=FALSE, message=FALSE, cache=TRUE, tidy=TRUE} -->
<!-- # load London wards for reference map -->
<!-- london_ward <- read_sf("data/raw/boundaries/2018/London_Ward.shp") -->

<!-- # create a point spatial dataframe, project -->
<!-- pollution_points <- st_as_sf(avg_pollution,coords=c('longitude','latitude'), crs=4326)[,1] %>% st_transform(27700) -->

<!-- # ensure all points are within the boundaries of Greater London -->
<!-- lonpollution_points <- pollution_points %>% st_intersection(london_ward) -->
<!-- ```    -->

<!-- Let's create a **proportional symbol** map to visualise these points: -->

<!-- ``` {r 09-those-symbols-are-in-proportion, warnings=FALSE, message=FALSE, cache=TRUE, tidy='styler'} -->
<!-- # ensure tmap mode is set to plot  -->
<!-- tmap_mode("plot") -->

<!-- # plot our London wards in grey -->
<!-- tm_shape(london_ward) +  -->
<!--   tm_polygons(palette = "grey", border.col = "white") + -->
<!-- # plot our pollution_points as bubbles, using the NO2 field to determine size -->
<!-- tm_shape(lonpollution_points) +  -->
<!--   tm_bubbles(size = "no2", col = "mediumorchid", style = "pretty",  -->
<!--              scale =1, border.col = "white",  -->
<!--              title.size="Average NO2 ug/m3 reading in 2019") + -->
<!--   # set legend -->
<!--   tm_layout(legend.position = c("left", "top")) + -->
<!--   # add a north arrow -->
<!--   tm_compass(type = "arrow", position = c("right", "top")) +  -->
<!--   # add a scale bar -->
<!--   tm_scale_bar(breaks = c(0, 5, 10, 15, 20), position = c("left", "bottom")) + -->
<!--   # add a data statement -->
<!--   tm_credits("Air quality data from London Air.")  -->
<!-- ``` -->

<!-- Our proportional symbols map already tells us a little about our dataset - we can see that NO~2~ levels are much higher towards the centre of London, although we can see some anomalies in the south-west, for example. But we can also see how and why a smoothed surface of our data **could** be really useful for further interpretation - and this is where interpolating our data comes in. -->

<!-- #### Thiessen polygons -->
<!-- The first step we can take to interpolate the data across space is to create Thiessen polygons. Thiessen polygons are formed to assign boundaries of the areas closest to each unique point. Therefore, for every point in a dataset, it has a corresponding Thiessen polygon.  -->

<!-- ```{r 09-thiessen, echo=FALSE, fig.align='center', out.width = "550pt", fig.cap='Creating a set of Thiessen polygons. Source: Esri 2020.'} -->
<!-- knitr::include_graphics('images/w09/thiessen.jpg') -->
<!-- ``` -->

<!-- :::note -->
<!-- **Note** <br /> -->
<!-- You may come across the term Voronoi polygons: these are the same thing as Thiessen polygons. Both terms are used interchangeably to describe this type of geometry created from point data. In the field of GIS we tend to refer to them as Thiessen polygons, after the American meteorologist who frequented their use. In other fields, particularly mathematics and computer science, they are generally referred to as Voronoi diagrams, in honour of the mathematician Georgy Voronoy. -->
<!-- ::: -->

<!-- We can create Thiessen polygons using the `sf` library with a bit of code: we will create a simple function called `st_thiessen_point()` that we can use to generate Thiessen polygons directly from a point dataset. -->

<!-- :::tip -->
<!-- **Tip** <br/> -->
<!-- Do not worry about fully understanding the code behind the function, but simply understand what input (a point spatial dataframe) and output (a Thiessen polygon spatial dataframe) it will provide. -->
<!-- ::: -->

<!-- You need to copy over both the function and the code underneath. Copying the function stores this function in your computer's memory for this R session and means the function itself can be used time and time again within the same session or script. -->

<!-- The first of the two lines of code below the function then "call" this function on our `lonpollutions_points` spatial dataframe. The second essentially joins the attribute fields of our `lonpollutions_points` spatial dataframe to our new Thiessen polygon spatial dataframe and stores this as a new variable.  -->

<!-- ```{r 09-thiessen-pols, warnings=FALSE, message=FALSE, cache=TRUE, tidy='styler'} -->
<!-- # function to create Thiessen polygons from point input -->
<!-- st_thiessen_point <- function(points){ -->
<!--     # input check -->
<!--     if(!all(st_geometry_type(points) == "POINT")){ -->
<!--         stop("Input not POINT geometries") -->
<!--     } -->

<!--     # make multipoint -->
<!--     g = st_combine(st_geometry(points))  -->

<!--     # create thiessen polygons -->
<!--     v = st_voronoi(g) -->
<!--     v = st_collection_extract(v) -->

<!--     # return -->
<!--     return(v[unlist(st_intersects(points, v))]) -->
<!-- } -->

<!-- # call function -->
<!-- lon_points_voronoi = st_thiessen_point(lonpollution_points) -->

<!-- # add attribute data  -->
<!-- lonpollution_tv = st_set_geometry(lonpollution_points, lon_points_voronoi) -->

<!-- # inspect -->
<!-- lonpollution_tv -->
<!-- ``` -->

<!-- We can now visualise these Thiessen polygons with their associated NO~2~ value: -->

<!-- ```{r 09-visualise-those-thiessen-pols, warnings=FALSE, message=FALSE, cache=TRUE, tidy='styler'} -->
<!-- # visualise thiessen polygons -->
<!-- tm_shape(lonpollution_tv) +  -->
<!--   tm_fill(col = 'no2', palette = "Purples") -->
<!-- ``` -->

<!-- We can go ahead tidy this up further by clipping our Thiessen polygons to the extent of London: -->

<!-- ```{r 09-thiessen-pols-map-after-crop, warnings=FALSE, message=FALSE, cache=TRUE, tidy='styler'} -->
<!-- # generate London outline through st_union -->
<!-- london_outline <- london_ward %>%  -->
<!--   st_union() -->

<!-- # clip our thiessen polygons to our london outline -->
<!-- lonpollution_tv <- st_intersection(lonpollution_tv, london_outline) -->

<!-- # visualise thiessen polygons -->
<!-- tm_shape(lonpollution_tv) +  -->
<!--   tm_fill(col = 'no2', palette = "Purples") -->
<!-- ``` -->

<!-- And that's it! We now have our values interpolated using our Thiessen polygon approach. However, as you can see, our approach is quite coarse. Whilst we, of course, can see areas of high and low pollution, it really does not offer us as much spatial detail as we would like, particularly when we know there are better methods out there to use. -->

<!-- #### Inverse Distance Weighting -->
<!-- A second method to interpolate point data is **Inverse Distance Weighting** (IDW). An IDW is a means of converting point data of numerical values into a continuous surface to visualise how the data may be distributed across space. The technique interpolates point data by using a weighted average of a variable from nearby points to predict the value of that variable for each location. The weighting of the points is determined by their inverse distances drawing on [Tobler’s first law of geography](https://en.wikipedia.org/wiki/Tobler%27s_first_law_of_geography#:~:text=The%20First%20Law%20of%20Geography,specifically%20for%20the%20inverse%20distance) that "*everything is related to everything else, but near things are more related than distant thing*".  -->

<!-- The distance weighting is done by a power function: the larger the power coefficient, the stronger the weight of nearby point. The output is most commonly represented as a raster surface. We will use the `idw()` function within the `gstat` library to conduct an IDW on our `lonpollution_points` spatial dataframe. -->

<!-- Before we can run IDW, we must first generate an empty grid within which to store our data. To do so, we can use the `spsample()` function from the `sp` library. We will go ahead and create a grid that covers the entirety of our `london_outline`, which we will transform into the `sp` format using the `as()` function. We then run the `gstat` `idw()` function on an `sp` version of our `lonpollution_points` dataset, specifying the cell size. We then specify that our IDW result is a gridded format that we then coerce into a raster. Once we have our raster, we can reset its CRS and of course utilise other functions from the `raster` library to process (e.g. the `mask` function) and then visualise our dat aset within `tmap`. -->

<!-- :::note -->
<!-- **Note** <br /> -->
<!-- Some of the following code will unfortunately not work as intended for some of you due to changes in underlying libraries. If the `gsat` below does not work for you, you can try to conduct the interpolation using the`spatstat` library instead.  -->
<!-- ::: -->

<!-- ```{r 09-inverse-distance-weights, warnings=FALSE, message=FALSE, cache=TRUE, tidy=TRUE} -->
<!-- # convert our lonpollution_points into the sp format -->
<!-- lonpollution_pointsSP <- lonpollution_points %>% as(., 'Spatial') -->

<!-- # convert our london_outline into the sp format -->
<!-- london_outlineSP <- london_outline %>% as(., 'Spatial') -->

<!-- # create an empty raster grid the size for our london_outline over which to interpolate the pollution values   -->
<!-- grid <-spsample(lonpollution_pointsSP, type ='regular', cellsize=450, bb=bbox(london_outlineSP)) -->

<!-- # run an IDW for the NO2 value with a power value of 2 -->
<!-- idw <- gstat::idw(lonpollution_pointsSP$no2 ~ 1, lonpollution_pointsSP, newdata = grid, idp = 2) -->

<!-- # specify idw spatial data as being gridded -->
<!-- gridded(idw) <- TRUE -->

<!-- # coerce to our gridded idw to the raster format -->
<!-- lon_poll_raster_idw <- raster(idw) -->

<!-- # set our raster CRS to BNG -->
<!-- crs(lon_poll_raster_idw) <- CRS("epsg:27700") -->

<!-- # mask our raster to only the london outline -->
<!-- lon_idw_gstat <- rasterize(london_ward, lon_poll_raster_idw, mask=TRUE)  -->

<!-- # plot the resulting raster -->
<!-- plot(lon_idw_gstat) -->
<!-- ``` -->

<!-- Great. Ff this code has worked for you and you have generated an IDW raster, you can move onto the next task which is to create a proper map of our resulting IDW. -->

<!-- #### Alternative: IDW with spatstat [Optional] -->
<!-- For those of you that cannot run the code above, we can look to `spatstat` as an alternative option although it just brings with it its few complications in terms of converting our datasets into our `ppp` object: we will first have to convert our data to the `ppp` object type and then use this within the `idw()` function `spatstat` offers. -->

<!-- ```{r 09-inverse-distance-weights-spatstat, warnings=FALSE, message=FALSE, cache=TRUE, eval=TRUE, tidy=TRUE} -->
<!-- # set our window of observation to London -->
<!-- window <- as.owin(london_outline) -->

<!-- # extract the coordinates of our pollution points sdf -->
<!-- points_xy <- lonpollution_points %>% st_coordinates() -->

<!-- # create a ppp object, setting x and y equal to the respective columns in our matrix -->
<!-- # set the window equal to our window variable -->
<!-- # set our 'marks' equal to the NO2 column in our points -->
<!-- pollution_ppp <- ppp(x=points_xy[,1], y= points_xy[,2], marks=lonpollution_points$no2, window=window) -->

<!-- # run the IDW  -->
<!-- ss_idw <- spatstat.core::idw(pollution_ppp, power = 2, at="pixels") -->

<!-- # coerce our im output to raster -->
<!-- lon_idw_sp <- raster(ss_idw) -->

<!-- # set our raster CRS to BNG -->
<!-- crs(lon_idw_sp) <- CRS("epsg:27700") -->

<!-- # plot the resulting raster -->
<!-- plot(lon_idw_sp) -->
<!-- ``` -->

<!-- You should see we actually get a similar result to the IDW of the `gstat` library. This is because our cell sizes resolutions are similar in both cases. We set our cell resolution as 450 x 450m above and we can check the cell size of our `spatstat` IDW raster using a very simple command: `res(lon_idw)`. You will see that the IDW `spatstat` auto-generated a 456m cell size. -->

<!-- ##### Mapping the IDW raster -->
<!-- We now have our final predicted raster surface. To map it, we can again use the `tm_raster()` as we have done previously. For our raster, the name of the layer we need to provide is `var1.pred` for those using the `gstat` result and simply `layer` for those using the `spatstat` result. -->

<!-- ```{r 09-idw-map-2, warnings=FALSE, message=FALSE, cache=TRUE, tidy='styler', echo=TRUE} -->
<!-- # plot the gstat raster -->
<!-- tm_shape(lon_idw_gstat) +  -->
<!--   tm_raster("var1.pred", style = "quantile", n = 100, palette = "Reds", -->
<!--             legend.show = FALSE) +  -->
<!-- tm_shape(london_ward) +  -->
<!--   tm_borders(col = "white", lwd = 0.1) -->
<!-- ```   -->
<!-- ```{r 09-idw-map, warnings=FALSE, message=FALSE, cache=TRUE, tidy='styler', eval=TRUE} -->
<!-- # plot the spatstat raster -->
<!-- tm_shape(lon_idw_sp) +  -->
<!--   tm_raster("layer", style = "quantile", n = 100, palette = "Reds", -->
<!--             legend.show = FALSE) +  -->
<!-- tm_shape(london_ward) +  -->
<!--   tm_borders(col = "white", lwd = 0.1) -->

<!-- ``` -->

<!-- And that's it. For those of you able to use the `gstat` code, it is highly worth playing around with the cell size to look at how it changes the smoothness of our resulting IDW. A smaller cell size will create a smoother IDW output, but it does add uncertainty to these estimates as we do not exactly have a substantial amount of data points to interpolate from. -->

<!-- To help with minimising this uncertainty, there are two additional steps you can take with your IDW output: -->

<!-- 1) Testing and fine-tuning the power function you have used to ensure it is a valid parameter by using something known as the *Leave One Out Cross Validation*. -->

<!-- 2) Generating a 95% confidence interval map of our interpolation mode using cross-validation methods. -->

<!-- This is beyond the scope of this module, but if you would like to explore this you can found details on this in Manuel Gimond's tutorial on interpolation available here: [Link](https://mgimond.github.io/Spatial/interpolation-in-r.html) -->

<!-- :::tip -->
<!-- **Tip** <br/> -->
<!-- A raster dataset normally only contains one layer, i.e. one variable. Hence when we want to map a raster, we use the `tm_raster()` and provide the layer name for mapping. In our examples, this has been `layer` and `var1.pred`, for example. However, in some circumstances, such as with satellite imagery we will want to use the `tm_rgb()` function instead. This is because these types of rasters, instead of having a single layer, actually consist of **three** bands: a band with a red value, a band with a green value and a band with a blue value. This is known as **multi-band** imagery. -->

<!-- To visualise multiband images correctly, we need to use the `tm_rgb()` function in order to stack our three bands together to create the appropriate visualisation. We can visualise each band independently of one another, however, you would see that you end up with either a nearly all red, green or blue image. This is also out of scope of this module, but if you are interested to learn more about [using satellite imagery with R](https://andrewmaclachlan.github.io/CASA0005repo/advanced-raster-analysis.html) this CASA tutorial is good place to start. Alternatively, you can also check Esri's help information on [Rasters and Raster Bands here](https://desktop.arcgis.com/en/arcmap/10.3/manage-data/raster-and-images/raster-bands.html). -->

<!-- Learning how to use satellite imagery can be a really useful skill set, particularly as this type of data is being increasingly used human geography applications - as well as, of course, its more traditional applications in physical and environmental geography. -->
<!-- ::: -->

<!-- ## Assignment 2 {#assignment-w09-02} -->
<!-- For your final assignment this week, we want you to redo the IDW interpolation of the London pollution data for the months of **June** and **December** and see to what extent there are differences between these months. In order to do this you will, at least, need to: -->

<!-- 1. Create monthly averages for the pollution data. This will involve quite some data wrangling. Keep in mind that [Google is your friend](https://lmgtfy.app/?q=google+is+your+friend)! -->
<!-- 2. For both the month of **June** and **December** create a spatial dataframe containing the London monitoring sites and their average NO~2~ reading. -->
<!-- 3. Conduct an Inverse Distance Weighting interpolation for both months of data. -->
<!-- 4. Execute some map algebra to identify the areas where the results of the interpolation differ. -->

<!-- ## Before you leave {#byl-w09} -->
<!-- This week, we've looked at raster datasets and how we use the `raster` library to manage and process them. Specifically, we looked at using **map algebra** to apply mathematical operations to rasters, using local, global, focal and zonal approaches and how we use map algebra on either an individual or combination of rasters. -->

<!-- We then looked at how we can use two different interpolation methods to generate raster data from point data. Understanding how to interpolate data correctly is incredibly important. Whilst in most instances you will be working with vector data, especially where government statistics and administrative boundaries are involved, there are also plenty of use cases in which you will need to generate raster data from point data, as we have done today. With that being said: [that is it for our final-to-last week in Geocomputation](https://www.youtube.com/watch?v=8iwBM_YB1sE). -->
