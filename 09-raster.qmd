# Rasters, Zonal Statistics, and Interpolation
The majority of our module has focused on the use of vector data and tabular data. This week, we switch it up by focusing primarily on raster data and its analysis using map algebra and zonal statistics.

## Lecture slides {#slides-w09}
The slides for this week's lecture can be downloaded here: [\[Link\]]({{< var slides.week09 >}})

## Reading list {#reading-w09}
#### Essential readings 
- Gimond, M. 2021. Intro to GIS and spatial analysis. **Chapter 14**: *Spatial Interpolation*. [[Link]](https://mgimond.github.io/Spatial/spatial-interpolation.html)
- Heris, M., Foks, N., Bagstad, K. 2020. A rasterized building footprint dataset for the United States. *Scientific Data* 7: 207. [[Link]](https://doi.org/10.1038/s41597-020-0542-3)
- Thomson, D., Leasure, D., Bird, T. *et al*. 2022. How accurate are WorldPop-Global-Unconstrained gridded population data at the cell-level? A simulation analysis in urban Namibia. *Plos ONE* 17:7: e0271504. [[Link]](https://doi.org/10.1371/journal.pone.0271504)

#### Suggested readings 
- Mellander, C., Lobo, J., Stolarick, K. *et al*. 2015. Night-time light data: a good proxy measure for economic activity? *PLoS ONE* 10(10): e0139779. [[Link]](https://doi.org/10.1371/journal.pone.0139779)

## Raster data
In previous weeks, we have predominantly worked with **vector data** and/or **tabular data** that we then join to vector data for analysis. However, depending on the nature of your research problem, you may also encounter **raster data**. This week's content introduces you to raster data, map algebra and interpolation. After first looking at population change in London using raster data, we will then look at generating pollution maps in London from individual point readings taken from air quality monitoring sites across London. To complete this analysis, we will be using several new datasets:

1. **Population rasters for Great Britain**: [WorldPop](https://hub.worldpop.org/) raster on estimated population counts for Great Britain in 2010 and 2020 at a spatial resolution of 1km.
2. **NO~2~ readings across London**: A dataset contain readings of NO~2~ for individual air quality monitoring sites in London.

```{r}
#| label: fig-raster-vector
#| echo: False
#| fig-cap: "A hypothetical raster and a vector model of landuse. [[Enlarge image]](https://jtvandijk.github.io/GEOG0030/images/w09/raster-vector.png){target='_blank'}"
knitr::include_graphics('images/w09/raster-vector.png')
```

::: {.callout-note}
The main difference between vector and raster models is how they are structured. Our vectors are represented by three different types of geometries: points, lines and polygons. We have used point data in the form of our stations and bike theft, and polygons in the form of our ward and borough boundaries. In comparison, our raster datasets are composed of pixels (or grid cells) --- a bit like an image. This means that a raster dataset represents a geographic phenomenon by dividing the world into a set of rectangular cells that are laid out in a grid. Each cell holds one value that represents the value of that phenomena at the location, e.g. a population density at that grid cell location. In comparison to vector data, we do not have an *attribute table* containing fields to analyse. All analysis conducted on a raster dataset therefore is primarily conducted on the cell values of a raster, rather than on the attribute values of the observations contained within our dataset or the precise geometries of our dataset. Probably one of the most common or well-known types of raster data are those that we can derive from remote sensing, including satellite and RADAR/LIDAR imagery that we see used in many environmental modelling applications, such as land use and pollution monitoring.
::: 

### Getting started {#setup-w08}
Open a new script within your GEOG0030 project and save this script as `wk9-raster-analysis.r`. At the top of your script, add the following metadata:

```{r}
#| label: 09-script-title
#| classes: styled-output
#| echo: True
#| eval: True
#| tidy: False
#| filename: "R code"
# Raster analysis 
# Date: January 2024
```

Now let us add all of the libraries we will be using today to the top of our script:

```{r}
#| label: 09-load-libraries
#| classes: styled-output
#| echo: True
#| eval: False
#| tidy: True
#| filename: "R code"
# load libraries
library(tidyverse)
library(sf)
library(tmap)
library(terra)
library(openair)
library(gstat)
```

```{r}
#| label: 08-load-libraries-bg
#| echo: False
#| eval: True
#| warning: False
# load libraries
library(tidyverse)
library(sf)
library(tmap)
library(terra)
library(openair)
library(gstat)
```

For the first part of this week's practical material we will be using raster datasets from [WorldPop](https://hub.worldpop.org/): 

> "*WorldPop develops peer-reviewed research and methods for the construction of open and high-resolution geospatial data on population distributions, demographics and dynamics, with a focus on low and middle income countries.*"

These population surfaces are estimates of counts of people, displayed within a regular grid raster of a spatial resolution of up to 100m. These surfaces can be used to explore, for example, changes in the demographic profiles of small areas, area deprivation, or country of birth.

1. Navigate to the WorldPop Hub: [[Link]](https://hub.worldpop.org/)
2. Go to **Population Count** -> **Unconstrained individual countries 2000-2020 (1km resolution)**.
3. Type *United Kingdom* in the search bar.
4. Download the [GeoTIFF](https://en.wikipedia.org/wiki/GeoTIFF) files for **2010** and **2020**: `gbr_ppp_2010_1km_Aggregated` and `gbr_ppp_2020_1km_Aggregated`.
5. Save the files in your `population` folder.

## Map algebra
**Map algebra** is a set-based algebra for manipulating geographic data, coined by [Dana Tomlin](https://en.wikipedia.org/wiki/Dana_Tomlin) in the early 1980s. Map algebra uses maths-like operations, including addition, subtraction and multiplication to update raster cell values. The most common type of map algebra is to apply these operations using *a cell-by-cell function*. These operations might include:

- **Arithmetic operations** that use basic mathematical functions like addition, subtraction, multiplication and division.
- **Statistical operations** that use statistical operations such as minimum, maximum, average and median.
- **Relational operations** which compare cells using functions such as greater than, smaller than or equal to.

::: {.callout-note}
The utilisation of these functions can enable many different types of specialised raster analysis, such as recoding or reclassifying individual rasters to reduce complexity in their data values, generating the [Normalised Difference Vegetation Index](https://en.wikipedia.org/wiki/Normalized_difference_vegetation_index) for a satellite imagery dataset or calculating [Least Cost Surfaces](https://pro.arcgis.com/en/pro-app/latest/tool-reference/spatial-analyst/creating-a-cost-surface-raster.htm) to find the most efficient path from one cell in a raster to another. Furthermore, using multiple raster datasets, it is possible to combine these data through mathematical overlays, from the basic mathematical operations mentioned above to more complex modelling..
::: 

We will be using some simple map algebra to look at population change in London between 2010 and 2020. Let's get started and take a look at our data. We can load raster data into R using the `terra` library:

```{r}
#| label: 09-load-raster-data
#| classes: styled-output
#| echo: True
#| eval: True
#| tidy: True
#| filename: "R code"
# load data
pop2010 <- rast('data/raw/population/gbr_ppp_2010_1km_Aggregated.tif')
pop2020 <- rast('data/raw/population/gbr_ppp_2020_1km_Aggregated.tif')

# transform projection
pop2010 <- pop2010 |> project("epsg:27700")
pop2020 <- pop2020 |> project("epsg:27700")
```

```{r}
#| label: fig-09-load-raster-data-2010
#| fig-cap: "WorldPop 2010 population estimates for the UK."
#| classes: styled-output
#| echo: True
#| eval: True
#| tidy: True
#| filename: "R code"
# plot 2010
plot(pop2010)
```

```{r}
#| label: fig-09-load-raster-data-2020
#| fig-cap: "WorldPop 2020 population estimates for the UK."
#| classes: styled-output
#| echo: True
#| eval: True
#| tidy: True
#| filename: "R code"
# plot 2020
plot(pop2020)
```

You should see that whilst your maps look very similar, the legend certainly shows that the values associated with each cell has grown over the 10 years between 2010 and 2021: we see our maximum increase from about 12,000 people per cell to well-over 14,000 people per cell. Now we have our raster data loaded, we want to reduce it to show only the extent of London. 

::: {.callout-note}
The `terra` package does not take in `sf` objects, so once we have loaded the London MSOA file we need to transform the file into a `SpatRaster` or `SpatVector`. The process of turning a vector dataset into a raster dataset is called rasterising. 

```{r}
#| label: fig-rasterise-a-file
#| echo: False
#| fig-cap: "Turning a line and polygon vector into a raster. Source: [Lovelace *et al*. 2023](https://r.geocompx.org/raster-vector#rasterization)."
knitr::include_graphics('images/w09/rtovector.png')
```
:::

```{r}
#| label: 09-confe-them-area
#| classes: styled-output
#| echo: True
#| eval: True
#| tidy: True
#| filename: "R code"
# load data, get outline, rasterise
msoa_london <- st_read('data/raw/boundaries/MSOA2021_London.gpkg') |>
  vect()

# crop
pop2010_london <- crop(pop2010, msoa_london)
pop2020_london <- crop(pop2020, msoa_london)

# mask
pop2010_london <- mask(pop2010_london, msoa_london)
pop2020_london <- mask(pop2020_london, msoa_london)
```

```{r}
#| label: fig-09-load-raster-data-2010-lon
#| fig-cap: "WorldPop 2010 population estimates for London."
#| classes: styled-output
#| echo: True
#| eval: True
#| tidy: True
#| filename: "R code"
# plot 2010
plot(pop2010_london)
```

```{r}
#| label: fig-09-load-raster-data-2020-lon
#| fig-cap: "WorldPop 2020 population estimates for London."
#| classes: styled-output
#| echo: True
#| eval: True
#| tidy: True
#| filename: "R code"
# plot 2020
plot(pop2020_london)
```

Now we have our two London population rasters, we can calculate population change between the two time periods by subtracting our 2010 population raster from our 2020 population raster:

```{r}
#| label: fig-09-subtract-london
#| fig-cap: "Population change in London 2010-2020."
#| classes: styled-output
#| echo: True
#| eval: True
#| tidy: True
#| filename: "R code"
# subtract
lonpop_change <- pop2020_london - pop2010_london

# plot
plot(lonpop_change)
```

## Zonal statistics
To further analyse our population change raster, we can create a smoothed version of our `lonpop_change` raster by using the `focal()` function. Using the `focal()` function, we generate a raster that summarises the average (mean) value of the **9** nearest neighbours for each cell, using a weight matrix defined in our `w` parameter and set to a `matrix`:

```{r}
#| label: fig-09-focus-on-the-hood
#| fig-cap: "Smoothed version of population change in London 2010-2020."
#| classes: styled-output
#| echo: True
#| eval: True
#| tidy: True
#| filename: "R code"
# subtract
lonpop_smooth <- focal(lonpop_change,w=matrix(1,3,3),fun=mean) 

# plot 
plot(lonpop_change)
```

The differences are not very noticeable, but you were to subtract the smoothed raster from the original raster you will see that definitely something happened:

```{r}
#| label: fig-09-focus-on-the-smooth
#| fig-cap: "Difference smoothed population change with original population change raster."
#| classes: styled-output
#| echo: True
#| eval: True
#| tidy: True
#| filename: "R code"
# plot the results
plot(lonpop_change - lonpop_smooth)
```

We can also look to use zonal functions to better represent our population change by aggregating our data to coarser resolutions. For example, we can resize our raster's spatial resolution to contain larger grid cells which will, of course, simplify our data, making larger trends more visible in our data but may similarly end up obfuscating smaller trends. 

::: {.callout-note}
We can resize our `lonpop_change` raster by using the `aggregate()` function and setting the `fact` (factor) parameter to the *order* of rescaling we would like (e.g. increase both the width and height of a cell by a factor of two). We then provide the `fun` (function) by which to aggregate our data, in this case, we will continue to use the `mean` but we could also use the `min` or `max` depending on our application.
:::

```{r}
#| label: fig-09-aggregate-the-raster
#| fig-cap: "Aggregated cell values." 
#| classes: styled-output
#| echo: True
#| eval: True
#| tidy: True
#| filename: "R code"
# aggregate
lonpop_agg <- aggregate(lonpop_change, fact=2, fun=mean) 

# plot 
plot(lonpop_agg)
```

Where we transformed a vector dataset into a raster dataset earlier, in some cases you would want to aggregate move from raster to vector. For example, in our case, we can aggregate the `lonpop_change` raster to our actual London MSOA boundaries, i.e. calculate for each MSOA in our dataset the average (or other function) population change,. We can, of course, use other functions other than the `mean`. What function you use will simply depend on your application. 

```{r tidy="styler"}
#| label: fig-09-aggregate-the-raster-to-vector
#| fig-cap: "Aggregating raster values to a vector geography."
#| classes: styled-output
#| echo: True
#| eval: True
# aggregate 
london_msoa_pop <- extract(lonpop_change, msoa_london, fun=mean)

# add to spatial dataframe
msoa_london <- msoa_london |> 
  st_as_sf() |>
  mutate(pop_change = london_msoa_pop$gbr_ppp_2020_1km_Aggregated)

# plot
tm_shape(msoa_london) +
  tm_fill(
    col = 'pop_change'
  )
```

We now have a vector dataset that we could go ahead and run many of the analyses that we have completed in previous weeks. Furthermore, we can use this data within other analyses we might want to complete. 

::: {.callout-tip}
Trying to calculate population change, particularly across decades as we have done here, can be quite challenging with changing administrative boundaries. Using raster data can be a good workaround to these issues, provided that the different rasters are of same size and extent.
:::

## Interpolation
For the second part of this week's practical material, we will explore several methods of spatial data interpolation by looking at air pollution in London using [Londonair](https://www.londonair.org.uk/) data. Londonair is the website of the London Air Quality Network (LAQN), and shows air pollution in London and southeast England that is provided by the [Environmental Research Group](https://www.imperial.ac.uk/school-public-health/environmental-research-group/) of Imperial College London. The data are publicly available for download and we can use an R package to directly interact with the data without needing to download it. The `openair` [R package](https://davidcarslaw.github.io/openair/) enables us to import data directly from the Londonair website. We will focus on Nitrogen Dioxide (NO~2~) measurements.

::: {.callout-note}
Spatial interpolation is the prediction of a given phenomenon in unmeasured locations. There are many reasons why we may wish to interpolate point data across a map. It could be because we are trying to predict a variable across space, including in areas where there are little to no data.
::: 

```{r tidy="styler"}
#| label: 09-get-air-pollution-data
#| classes: styled-output
#| echo: True
#| eval: False
#| filename: "R code"
# get list of all measurement sites operated by Imperial College
# limit to sites with data for 2022
site_meta <- importMeta(source = "kcl", all=TRUE, year=2022:2022)

# download all data pertaining to these sites
pollution <- importKCL(site=c(site_meta$code), year=2022:2022, pollutant='no2',meta=TRUE)
```

::: {.callout-warning}
This second part of the code might take some time to run as it will try to download the data for all sites for an entire year --- and in many cases data is measured hourly. Despite us limiting our data download, not all measurements sites collect data on NO~2~ so you will get some warnings along the lines of `404 Not Found`. In case you run into too many errors or it is just taking too long, you can download a copy of the data here: [[Download]](https://github.com/jtvandijk/GEOG0030/tree/master/data/zip/london-no2-2022.zip). Once downloaded, copy over the `zip` and put this into a `data/raw/pollution` folder. The file is rather large, so you can leave it unzipped.
:::

Let's inspect the data:

```{r}
#| label: 09-inspect-air-pollution-data
#| classes: styled-output
#| echo: True
#| eval: True
#| tidy: True
#| filename: "R code"
# load from zip if not downloaded through the Open Air library
pollution <- read_csv("data/raw/pollution/london_no2_2022.zip")

# inspect
head(pollution)
```

We can see that in our first five rows we have data for the **same site** and if we look at the **date** field, we can see we have a reading observation for every hour. With 24 hours in the day, 365 days in a year and hundreds of sites, it should therefore be of no surprise that we have such a large `csv`. Let's summarise the values by site to make the dataset a bit more workable:

```{r}
#| label: 09-mean-air-pollution-data
#| classes: styled-output
#| echo: True
#| eval: True
#| tidy: True
#| filename: "R code"
# calculate mean no2 values
pollution_avg <- pollution |>
  filter(!is.na(latitude) & !is.na(no2)) |>
  group_by(code, latitude, longitude) |>
  summarise(no2 = mean(no2))

# inspect
head(pollution_avg)
```
We are now left with only **45** measurement sites --- with only the latitudes, longitudes, and the average NO~2~ values associated with each. When using interpolation, the distribution and density of our data points will impact the accuracy of our final raster and we may end up with a level of uncertainty in the areas where measurements are more sparse. Let's have a look at the spatial distribution of the measurement sites.

```{r tidy="styler"}
#| label: fig-09-air-quality-measurement-sites
#| fig-cap: "NO~2~ measurement sites in London."
#| classes: styled-output
#| echo: True
#| eval: True
# load MSOAs for reference
msoa_london <- st_read('data/raw/boundaries/MSOA2021_London.gpkg') |>
  st_union()

# create a point spatial dataframe
measurement_sites <- pollution_avg |>
  st_as_sf(coords=c('longitude','latitude'), crs=4326) |>
  st_transform(27700)

# ensure all points within London
measurement_sites <- measurement_sites |>
  st_intersection(msoa_london)

# add London outline
tm_shape(msoa_london) +
  tm_polygons(
    col = 'grey'
  ) +
  # add measurement sites
  tm_shape(measurement_sites) +
  tm_dots()
```

Let's also have a look whether measurements differ across London:

```{r tidy="styler"}
#| label: fig-09-air-quality-measurement-sites-proportional-symbol
#| fig-cap: "Proportional symbol map on NO~2~ measurements in London."
#| classes: styled-output
#| echo: True
#| eval: True
# add London outline
tm_shape(msoa_london) +
  tm_polygons(
    cols = 'grey'
  ) +
# add proportional symbol
tm_shape(measurement_sites) +
  tm_bubbles(
    size = "no2", 
    col = "blue", 
    style = "pretty",
    border.col = "white",
    title.size="Average NO2 ug/m3 reading in 2022"
    ) +
  # add legend
  tm_layout(
    legend.position = c("left", "top")
    ) +
  # add north arrow
  tm_compass(
    type = "arrow", 
    position = c("right", "top")
    ) +
  # add scale bar
  tm_scale_bar(
    breaks = c(0, 5, 10, 15, 20), 
    position = c("left", "bottom")
    ) +
  # add credits
  tm_credits("NO~2~ measurements from London Air (2022).")
```

Our proportional symbols shows that there is some heterogeneity in NO~2~ measurements across London --- both in terms of coverage and in terms of NO~2~ levels. This means that if we want to make a reasonable assumptions about NO~2~ levels in an area where no measurement was taking, we need to interpolate the missing values.

#### Thiessen polygons
The first step we can take to interpolate the data across space is to create Thiessen polygons. Thiessen polygons are formed to assign boundaries of the areas closest to each unique point. Therefore, for every point in a dataset, it has a corresponding Thiessen polygon.

```{r}
#| label: fig-thiessen
#| echo: False
#| fig-cap: "Thiessen polygons. [[Enlarge image]](https://jtvandijk.github.io/GEOG0030/images/w09/thiessen.png){target='_blank'}"
knitr::include_graphics('images/w09/thiessen.png')
```

::: {.callout-tip}
Besides Thiessen you may also come across the term Voronoi polygons. Both terms are used interchangeably to describe this type of geometry created from point data. In the field of GIS we tend to refer to them as Thiessen polygons, but in other fields they are often referred to as Voronoi diagrams, in honour of the mathematician Georgy Voronoy.
:::

We can create Thiessen polygons using the `sf` library with a bit of code: we will create a simple function called `st_thiessen_point()` that we can use to generate Thiessen polygons directly from a point dataset. 

::: {.callout-warning}
Do not worry about fully understanding the code behind the function, but simply understand what input (a point spatial dataframe) and output (a Thiessen polygon spatial dataframe) it will provide.
:::

``` {r}
#| label: 09-create-thiessen-polygons
#| classes: styled-output
#| echo: True
#| eval: True
#| tidy: True
#| filename: "R code"
# function 
st_thiessen_point <- function(points){
    
  # input check
    if(!all(st_geometry_type(points) == "POINT")){
        stop("Input not POINT geometries")
    }

    # to multipoint
    g = st_combine(st_geometry(points))

    # to thiessen
    v = st_voronoi(g)
    v = st_collection_extract(v)

    # return
    return(v[unlist(st_intersects(points, v))])
}

# call function
measurement_sites_thiessen <- st_thiessen_point(measurement_sites)

# force thiessen polygons to measurement sites dataframe
measurement_sites_thiessen_df <- measurement_sites |>
  st_set_geometry(measurement_sites_thiessen) |>
  st_intersection(msoa_london)

# inspect
measurement_sites_thiessen_df
```

We can now visualise these Thiessen polygons with their associated NO~2~ value:

```{r tidy="styler"}
#| label: fig-09-air-quality-london-thiessen
#| fig-cap: "Interpolation of NO~2~ measurements in London using Thiessen polygons."
#| classes: styled-output
#| echo: True
#| eval: True
# add thiessen polygons
tm_shape(measurement_sites_thiessen_df) +
  tm_polygons(
    col = 'no2',
    palette = 'Blues'
  ) +
  # add legend
  tm_layout(
    legend.show = TRUE
  )
```

And that's it! We now have our values interpolated using our Thiessen polygon approach. However, as you can see, our approach is quite coarse. Whilst we, of course, can see areas of high and low pollution, it really does not offer us as much spatial detail as we would like, particularly when we know there are better methods out there to use.

#### Inverse Distance Weighting
A second method to interpolate point data is **Inverse Distance Weighting** (IDW). An IDW is a means of converting point data of numerical values into a continuous surface to visualise how the data may be distributed across space. The technique interpolates point data by using a weighted average of a variable from nearby points to predict the value of that variable for each location. The weighting of the points is determined by their inverse distances, essentially drawing on [Toblerâ€™s first law of geography](https://en.wikipedia.org/wiki/Tobler%27s_first_law_of_geography#:~:text=The%20First%20Law%20of%20Geography,specifically%20for%20the%20inverse%20distance).

::: {.callout-tip}
The distance weighting is done by a power function: the larger the power coefficient, the stronger the weight of nearby point. The output is most commonly represented as a raster surface. 
:::

We will start by generating an empty grid to store the predicted values before executing a simple the IDW.

``` {r}
#| label: 09-idw
#| classes: styled-output
#| echo: True
#| eval: True
#| tidy: True
#| filename: "R code"
# create regular output grid from London outline
output_grid <- msoa_london |>
  st_make_grid(cellsize = c(500,500))

# execute IDW
measurement_sites_idw <- idw(formula = no2 ~ 1,
                             locations = measurement_sites,
                             newdata = output_grid,
                             beta = 2)

# clip to London outline
measurement_sites_idw <- measurement_sites_idw |>
  st_intersection(msoa_london)
```

We now have our final predicted raster surface. To map it, we can again use the `tmap` as we have done previously. For our polygon raster, the name of the layer we need to provide is `var1.pred`.
  
```{r tidy="styler"}
#| label: fig-09-air-quality-idw
#| fig-cap: "Interpolation of NO~2~ measurements in London using Inverse Distance Weighting."
#| classes: styled-output
#| echo: True
#| eval: True
#| cache: True
# add idw grid
tm_shape(measurement_sites_idw) +
  tm_fill(
    col = "var1.pred", 
    style = "quantile", 
    n = 100, 
    palette = "Reds",
    legend.show = FALSE
    ) +
  # add measurement sites
  tm_shape(measurement_sites) +
  tm_dots()
```

::: {.callout-note}
We have used an output cell size of 500x500 metres. A smaller cell size will create a smoother IDW output, but it does add uncertainty to these estimates as we do not exactly have a substantial amount of data points to interpolate from. Keep in mind: reducing the cell size will also exponentially increase processing time.
:::

## Assignment {#assignment-w09}
For your final assignment this week, we want you to redo the IDW interpolation of the London pollution data for the months of **June** and **December** and see to what extent there are differences between these months. In order to do this you will, at least, need to:

1. Create monthly averages for the pollution data. This will involve some data wrangling.
2. For both the months of **June** and **December** create a spatial dataframe containing the London monitoring sites and their average NO~2~ reading.
3. Conduct an Inverse Distance Weighting interpolation for both months of data.
4. Combine the results to identify areas that might differ.

## Want more? [Optional] {#wm-w09}
### More raster {.unnumbered}
A raster dataset often only contains one layer, i.e. one variable. Hence when we want to map a raster, we use the `tm_raster()` and provide the layer name for mapping. However, satellite imagery, for instance, consists of **three** bands: a band with a red value, a band with a green value and a band with a blue value. This is known as **multi-band** imagery. We can visualise each band independently of one another, however, you would see that you end up with either a nearly all red, green or blue image. If you are interested to learn more about [using satellite imagery with R](https://andrewmaclachlan.github.io/CASA0005repo/advanced-raster-analysis.html) and raster analysis in general, this is a good place to start. Alternatively, if you are interested in more advanced interpolation methods, [Manual Gimond's](https://mgimond.github.io/Spatial/interpolation-in-r.html) tutorial on spatial data interpolation will get you started.

## Before you leave {#byl-w09}
This week, we've looked at raster datasets and how we use the `terra` library to manage and process them. Specifically, we looked at using **map algebra** to apply mathematical operations to rasters. We further looked at two different interpolation methods to generate raster data from point data. Understanding how to interpolate data correctly is incredibly important. Whilst in most instances you will be working with vector data, especially where government statistics and administrative boundaries are involved, there are also plenty of use cases in which you will need to generate raster data from point data, as we have done today. With that being said: [that is it for our penultimate week](https://www.youtube.com/watch?v=8iwBM_YB1sE). Suppose nothing left to do besides checking out that reading?
