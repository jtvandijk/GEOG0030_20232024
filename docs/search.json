[
  {
    "objectID": "03-cartography.html",
    "href": "03-cartography.html",
    "title": "1 Cartography and Visualisation",
    "section": "",
    "text": "This week’s lecture has given you an introduction on how to create a successful map. We further talked about map projections, cartographic conventions, and issues faced with the analysis of aggregated data at areal units. The practical component of the week puts some of these learnings into practice as we analyse crime rates in London at two different scales.\n\n\nThe slides for this week’s lecture can be downloaded here: [Link]\n\n\n\n\n\n\nLongley, P. et al. 2015. Geographic Information Science & Systems, Chapter 4: Georeferencing, pp. 77-98. [Link]\nLongley, P. et al. 2015. Geographic Information Science & Systems, Chapter 11: Cartography and Map Production, pp. 237-252. [Link]\nWong, D. 2009. Modifiable Areal Unit Problem. International Encyclopedia of Human Geography 169-174. [Link]\n\n\n\n\n\nLongley, P. et al. 2015. Geographic Information Science & systems, Chapter 12: Geovisualization, pp. 266-289. [Link]\nUsery, L. and Seong, J. 2001. All equal-area map projections are created equal, but some are more equal than others. Cartography and Geographic Information Science 28(3): 183-194. [Link]\n\n\n\n\n\nOver the next few weeks, we will explore the spatial patterns of crime across London from a spatial perspective. Reid et al. (2018) suggest:\n\nSpatial analysis can be employed in both an exploratory and well as a more confirmatory manner with the primary purpose of identifying how certain community or ecological factors (such as population characteristics or the built environment) influence the spatial patterns of crime.\n\nAgainst this background, we are actually going to answer a very simple question today: does our perception of crime rates (and its distribution) in London vary at different scales? Here we are looking to test whether we would make the ecological fallacy mistake of assuming patterns at the LSOA level are the same at the Borough level by looking to directly account for the impact of the Modifiable Area Unit Problem within our results. Here we will be looking specifically at a specific type of crime: Theft from a person.\n\n\n\n\n\n\nThe datasets you will create in this practical will be used in other practicals, so make sure to follow every step and export your data to your data folder at the end of the practical.\n\n\n\n\n\nFor our crime data, we will use data directly from the Police Data Portal, which you can find at data.polic.uk. This data portal allows you to access and generate tabular data for crime recorded in the UK across different the different police forces since 2017.\n\n\n\n\n\n\nIn the United Kingdom, there are 45 territorial police forces (TPF) and 3 special police forces (SPF). Each TPF covers a specific area in the UK (e.g. the West Midlands Police Force), whilst the SPFs are cross-jurisdiction and cover specific types of crime, such as the British Transport Police. Therefore, when we want to download data for a specific area, we need to know which police force covers our area interest.\n\n\n\nWhen you look to download crime data for London, there are two territorial police forces working within the city and its greater metropolitan area:\n\nThe Metropolitan Police Force, which covers Greater London.\nThe City of London Police, which covers the City of London.\n\n\n\nNormally, we would now head to the Police Data Portal and download our crime data. However, the manual processing that is required to clean and prepare the data that we need today is too exhaustive and far easier to do using programming, so you can download a pre-filtered spreadsheet below. Unzip the file and copy the csv into a new folder in your raw data folder called: crime.\n\n\n\nFile\nType\nLink\n\n\n\n\nTheft from persons in 2021\ncsv\nDownload\n\n\n\n\n\n\n\n\n\nWhen mapping the data from the provided longitude and latitude coordinates, it is important to know that these locations represent the approximate location of a crime — not the exact place where it happened. This displacement is introduced on purpose to preserve anonymity of the individuals involved. The process by how this displacement occurs is standardised. There is a list of anonymous map points to which the exact location of each crime is compared against this master list to find the nearest map point. The co-ordinates of the actual crime are then replaced with the co-ordinates of the map point. Each map point is specifically chosen to avoid associating that point with an exact household.\n\n\n\nIf we want to study a phenomena like crime and aggregate it to an areal unit, we will need to normalise this by some denominator (e.g. population). Why? When we record events created by humans, there is often a population bias: simply, more people in an area will by probability lead to a higher occurrence of said event. Fortunately, we already prepared a 2021 LSOA population dataset last week.\nIn addition to our LSOA level dataset, we also need a population dataset for our London Boroughs today. We can do this utilising by joining Borough population data to Borough spatial data and creating a Borough2021_London.gpkg file.\n\n\n\nA lot of data about London is collated by the Greater London Authority (GLA) and made available through the London Datastore. Whereas some of the data is relatively old, it is a good place to get some data specific to London:\n\nNavigate to the London Datastore: [Link].\nClick on Data in the navigation menu.\nType London Boroughs into the search field.\nDownload the GeoPackage containing the boundaries of each of London’s 33 Boroughs.\nRename the file to Boroughs_London.gpkg and move it to boundaries folder in your raw data folder.\n\n\n\n\nThe final data that we need is the total population for each of the London Boroughs in 2021. A good source for this is the 2021 Census again.\n\nNavigate to the Nomis portal: [Link]\nClick on Query data in the Data Downloads panel.\nClick on Census 2021 -&gt; Topic Summaries.\nClick on TS007A - Age by five-year age bands.\nSelect Geography and set Local authorities: district / unitary (as of April 2023) to Some.\nUnder List areas within select London. Click on Tick all.\nClick on Download data at the left hand side of the screen.\nOnce the data is ready for download, download the file to your computer and save it as LondonBorough2021_population.xlsx.\nOpen the dataset in your spreadsheet editing software and extract the relevant information: the Borough names and associated population counts. Copy the data that you require into a new csv file and rename the columns as BoroughNames and pop2021.\nNow format the pop2021 column so that it is recognised as being a numeric column.\nSave the file as a new csv in your data folder: LondonBorough2021_population.csv.\n\n\n\n\n\n\n\n\nStart QGIS\nClick on Project -&gt; New. Save your project as w3-crime-analysis. Remember to save your work throughout the practical.\nBefore we get started with adding data, we will first set the Coordinate Reference System of our Project. Click on Project -&gt; Properties – CRS. In the Filter box, type British National Grid. Select OSGB 1936 / British National Grid - EPSG:27700 and click Apply. Click OK.\n\n\n\n\n\nClick on Layer -&gt; Add Layer -&gt; Add Vector Layer.\nWith File select as your source type, click on the small three dots button and navigate to your LSOA2021_London.gpkg file in your boundaries folder. Select the file, then click Add. You may need to close the box after adding the layer.\n\nWe now need to create our Borough population spatial layer. To do so, we need to repeat exactly the same process as last week in terms of joining our table data to our spatial layer. You should now be able to do this, so we will not provide you with detailed instructions. Remember, you need to:\n\nLoad the respective Borough dataset as a Vector Layer found in your raw/boundaries/ data folder: Boroughs_London.gpkg.\nLoad the respective population dataset that you just created as a Delimited Text File Layer. Remember the settings, including no geometry!\nJoin the two datasets together using the Join tool in the Borough dataset Properties box. Remember which fields to use, which to add, and to remove the prefix.\nExport your joined dataset into a new dataset within your data folder as: Boroughs_London_Pop2021.gpkg.\nMake sure this dataset is loaded into your Layers / added to the map.\nRemove the original Borough and population data layers.\n\n\n\n\n\n\n\nBecause the population file that we downloaded does not contain Borough codes, we need to use the Borough names to join the two files together. Whilst it works here and we can easily manually check whether the join was successful because there are only 33 records, codes are preferred as there is much less room for error and incomplete joins. This means it is often worth it to spend a bit more time and find these codes over relying on names.\n\n\n\n\n\n\nWe will now load and map our crime data. We will load this data using the Delimited Text File Layer option you would have used just now to load the Borough population, but this time we will be adding point coordinates to map our crime data as points.\n\nClick on Layer -&gt; Add Layer -&gt; Add Delimited Text File Layer.\n\nClick on the three dots button next to File Name and navigate to your crime-theft-2021-london.csv in your raw/crime folder.\nClick on the .csv file of this dataset and click Open.\nYour file format should be set to csv. In Record and Fields Options tick Decimal separator is comma, First record has field names, Detect field types and Discard empty fields.\nUnder Geometry Definition, select Point coordinates and set the X field to Longitude and the Y field to Latitude. The Geometry CRS should be: EPSG:4326 - WGS84. Click Add.\n\n\n\n\n\n\n\n\nWhen you click Add, you should have gotten a pop-up from QGIS asking about transformations. Transformations are algorithms that convert data from one CRS to another. QGIS knows that the Project CRS is British National Grid but the Layer you are trying to add has a WGS84 CRS. QGIS is asking you what transformation it should use to project the Layer in the Project CRS. This is one of the key strengths of QGIS: it can project data ‘on the fly’. What this means is that QGIS will automatically convert all Layers to the Project CRS once it knows which transformation you would like to use so that they will all be rendered in the correct position with respect to each other. More details on this can be found in QGIS’ user manual section on working with projections.\n\n\n\n\nClick OK to accept QGIS’ suggested on-the-fly projection. You should now see your crime dataset displayed on the map canvas.\n\n\n\n\n\n\nFigure 1: Borough map with crime data. [Enlarge image]\n\n\n\n\n\nWe can test the temporary nature of the projection by looking at the CRS of the crime-theft-2021-london layer. Right-click on the layer then select Properties -&gt; Information and then look at the associated CRS. You should see that the CRS of the layer is still WGS84.\n\nWe want to make sure our analysis is as accurate and efficient as possible, so it is best to reproject our data into the same CRS as our administrative datasets, i.e. British National Grid. This also means we will have the dataset to use in other projects, just in case.\n\nBack in the main QGIS window, click on Vector -&gt; Data Management Tools -&gt; Reproject Layer. Fill in the parameters as follows:\n\nInput Layer: crime-theft-2021-london\nTarget CRS: Project CRS: EPSG: 27700\nReprojected: Click on the three buttons and Save to GeoPackage to create a new data file.\nSave it in your raw/crime folder as crime-theft-2021-london-prj.gpkg, using theft2021 as Layer Name.\nClick Run. You should now see the new data layer added to your project. You can now close the Reproject Layer tool.\n\nYou can now also remove the original crime-theft-2021-london dataset, only keeping the reprojected version.\n\n\n\n\nThe next step of our analysis involves assigning the crime point data to our administrative geographies. We will use the Count Points in Polygons in the Analysis toolset to count how many crimes have occurred in our LSOAs and our Boroughs. We will then have a count statistic that we need to normalise by population data to create a crime rate statistic.\n\nClick on Vector -&gt; Analysis Tools -&gt; Count Points in Polygons.\nWithin the toolbox, select the parameters as follows:\n\nPolygons: Boroughs_London_Pop2021\nPoints: theft2021 [Note how both our data layers state the same CRS.]\nNo weight field or class field\nCount field names: theft2021\nClick on the three dot button and Save to GeoPackage: output -&gt; Borough2021_crime.gpkg, with borough-theft2021 as Layer Name.\n\nClick Run and Close the box. You should now see an Output layer added to your Layers box. Rename the layer to borough-theft2021.\nRight-click on the borough-theft2021 layer and open the Attribute Table. You should now see a theft2021 column next to your pop2021 column. You can look through the column to see the different levels of crime in the each Borough. You can also sort the column, from small to big, big to small, like you would do in a spreadsheet software.\n\nWhilst it is great that we have got our theft2021, what we actually need is a crime rate to account for the different population sizes across the Boroughs. To get our crime rate statistic, we are going to do our first bit of table manipulation in QGIS.\n\nWith the Attribute Table of your borough-theft2021 layer still open, click on the pencil icon at the top left corner. This pencil switches on the Editing mode.\n\n\n\n\n\n\n\nThe Editing mode allows you to edit both the Attribute Table values and the geometry of your data. When it comes to the Attribute Table, it means you can directly edit existing values in the table or create and add new fields to the table. Whilst you can actually do the latter outside of the Editing mode, this Editing mode means you can reverse any edits you make and they are not permanent just in case you make a mistake. Using the Editing mode is the safest approach to editing your table, however, it might not always be the approach you use when generating new fields.\n\n\n\n\nWhilst in the Editing mode, click on New Field button (hotkeys: ctrl + w or cmd + w and fill in the Field Parameters as follows:\n\nName: crime-rate\nComment: leave blank\nType: Decimal number\n\nClick OK. You should now see a new field added to our Attribute Table.\n\nThe empty field has NULL populated for each row, so we need to find a way to give our Boroughs some crime rate data. To do this, we will calculate a simple Crime Rate using the Field Calculator tool provided by QGIS within the Attribute Table. We will create a crime rate that details the number of crimes per 10,000 people in the Borough.\n\nWhilst still in the Editing mode, click on the Abacus button (ctrl + i or cmd + i), which is known as the Field Calculator.\nIn the Field Calculator window:\n\nCheck the Update existing field box.\nUse the drop-down to select the crime-rate field.\nIn the Expression editor, add the following expression: ( \"theft2021\" / \"pop2021\" ) * 10000\nOnce done, click OK.\n\nClick on the Save button to save these edits. Click again on the Pencil button to exit Editing mode.\n\n\n\n\n\n\n\nThe crime-rate is stored as a decimal as this is required for the calculation to succeed, but ultimately you cannot have half a crime. You can transform the decimal number to an integer by forcing the crime-rate column to a new field using the same Field Calculator. Instead of ticking the Update existing field box you would now keep the Create a new field box ticked. Name the new field crime-rate-int, make sure the Output field type is set to Whole number (integer), and use the following expression to create a new field with an integer value: to_int(\"crime-rate\"). Save your changes by clicking on the Save button.\n\n\n\n\nWe now have our Borough crime rate dataset ready for mapping. We just now need to repeat this process to have our LSOA dataset. So, add the LSOA2021_London.gpkg file to your project and repeat the above process to create a crime-rate and crime-rate-int column within the LSOA dataset as well. After reordering some of the layers, your screen should look something like this:\n\n\n\n\n\n\nFigure 2: QGIS view containing crime data layers for both the London Boroughs and LSOAs. [Enlarge image]\n\n\n\n\n\n\n\n\nNow you have both datasets ready, it is time to style the maps. Remember to use the Properties box to symbolise your maps. Think through using an appropriate colour scheme, perhaps have a look at the online colorbrewer 2.0 for inspiration if you do not want to use the default settings. Once you are happy with their symbolisation, we can turn them into proper publishable maps using QGIS’s Print Layout.\n\n\n\n\n\nFigure 3: An example of how your maps could have been symbolised. [Enlarge image]\n\n\n\n\n\n\n\n\n\n\nIf you have used ArcMap before, Print Layout is similar to switch the view of your map canvas to a print layout within the main window but in QGIS’s case, it loads up a new window.\n\n\n\nFrom the main QGIS window, click on Project -&gt; New Print Layout. In the small box that first appears, call your new print layout: crime-map-borough-lsoa. A new window should open up that shows a blank canvas. On the left-hand side of the window, you will find buttons to add print layout items: the current QGIS map canvas, text labels, images, legends, scale bars, basic shapes, arrows, attribute tables and HTML frames. In this toolbar you also find buttons to navigate, zoom in on an area and pan the view on the layout a well as buttons to select any layout item and to move the contents of the map item.\nOn the right-hand side of the window, you will find two set of panels. The upper one holds the panels Items and Undo History and the lower holds the panels Layout, Item properties and Atlas generation. Today, we are most interested in the bottom panel as Layout will control the overall look of our map, whilst Item properties will allow us to customise the elements, such as title or legend, that we may add to our map.\nIn the bottom part of the window, you can find a status bar with mouse position, current page number, a combo box to set the zoom level and the number of selected items if applicable. In the upper part of the window, you can find menus and other toolbars. All print layout tools are available in menus and as icons in a toolbar.\n\n\n\n\n\n\nWorking with maps in the Print Layout is simple but it can be a little fiddly and, to make more complicated maps, requires you to understand how to use certain aspects of Print Layout, such as locking items. If you get stuck, have a look at the training manual or the detailed documentation.\n\n\n\nTo start with creating a map, you use the Add Map tool to draw a box in which a snapshot of the current active map you have displayed in your QGIS main window will be loaded.\n\nClick on the Add Map tool and draw a box in the first half of our map to load our current map. Note, you can move your map around and resize the box simply by clicking on it as you would in Word etc.\n\n\n\n\n\n\nFigure 4: Current active map in the Print Layout. [Enlarge image]\n\n\n\n\n\nWith your map selected, head to the Items Properties panel and look for the Scale parameter.\n\nHere we can manually edit the scale of our map to find the right zoom level.\nHave a go at entering different values and see what level you think suits the size of your map.\nKeep a note of the scale, as we will need this for the second map we will add to our map layout - our LSOA map.\nNext, in the same panel, if you would like, you can add a frame to your map - this will draw a box (of your selected formatting) around the current map.\nIn the same panel, note down the size of your map - we want to make sure the next map we add is of the same size.\nNote, if you need to move the position of the map within the box, look for the Move Item Content tool on the left-hand side toolbar.\nOnce you are done, finally click on the Lock Layers and Lock Style for layers.\n\n\nBy locking the Layers (and their symbology) in our map, it means we can change our data/map in our main QGIS window without changing the map in the Print Layout - as we will see in a minute when adding our Borough crime rate map. If we do not lock our layers, our map would automatically update to whatever is next displayed in the main QGIS window.\n\nClick on the Add Legend tool and again, draw a box on your map in which your legend will appear.\n\nAs you will see, your Legend auto-generates an entry for every layer in our Layers box in the main QGIS application:\nIn Item Properties, uncheck auto-update - this stops QGIS automatically populating your legend and enables you to customise your legend.\nFirst, let us rename our layer in the legend to: Borough Crime Rate (per 10,000 people).\nNext, we want to remove all other Layers, using the - (minus) button\nWe can also customise the Legend further, including type, size and alignment of font. Go ahead and style your legend as you would prefer.\nMove the Legend to an appropriate part of the layout near your Borough crime rate map - resize if necessary.\n\n\n\n\n\n\n\nFigure 5: The automatically generated legend for your map. [Enlarge image]\n\n\n\n\nNow we are finished with the Borough map, we want to make sure we do not change any aspect of its layout. To do so, we need to lock both the map layer and legend in the Items panel. This prevents us accidentally moving items in our layout. Note, this is different to locking your layers in the Items Properties as we did earlier.\n\nIn the Items panel, click the Lock check box for both our map layer and legend.\nIn the main QGIS window, uncheck your borough-theft2021 layer and make sure your lsoa-theft2021 layer is now visible.\nReturn to the Print Layout window and repeat the process above of adding a map to the window. This time you should see your LSOA map loaded in the box (and you should see no changes to your Borough map).\n\nPlace your LSOA map next to your LSOA map - use the snap grids to help.\nSet your LSOA map to the same zoom level as your Borough map.\nMake sure your LSOA map is the same size as your Borough map.\nAdd a frame if you want.\nLock your layer and its symbology in the Items Properties once ready and the lock your layer in the Items panel.\n\n\nWe now just need to add a second legend for our Borough map.\n\n\n\n\n\n\nIf we had standardised our values across our two maps, then we would only need to use one legend. However, in this case, as there are large differences in the value ranges, we need to have two legends.\n\n\n\n\nRepeat the process as above to add a Legend for our LSOA map.\n\nRemember to re-title the legend to make it more legible/informative.\nMatch the same formatting for a clean look.\n\nOnce complete, lock these two items in the Items panel as well.\nNow we have our two maps ready, we can add our main map elements. Using the tools on the left-hand tool bar:\n\nAdd a scale bar: use the Item Properties to adjust the Style, number of segments, font, etc.\nAdd a north arrow: draw a box to generate the arrow and then use the Item Properties to adjust..\nAdd a title at the top of the page, and subtitles above the individual maps.\nFinally add a box detailing our data sources, you can copy and paste the following: Contains National Statistics data © Crown copyright and database right [2021] (Open Government Licence). Contains Ordnance Survey data © Crown copyright and database right [2021]. Crime data obtained from data.police.uk (Open Government Licence).\n\n\nOnce you have added these properties in, you should have something that looks a little like this:\n\n\n\n\n\nFigure 6: Crime rates in London Boroughs and LSOAs. [Enlarge image]\n\n\n\n\nThe only thing outstanding is to export our map to a file. Go to Layout -&gt; Export as Image and then save it as London-2021-crime-rate.png.\n\n\n\n\n\nLooking at the maps you have created, how does your perception of crime (and its distribution) in London vary at different scales?\nAt the moment, we have looked at the crime rate as a relative amount. We therefore use a sequential colour scheme that shows, predominantly, where the crime rate is the highest. Alternatively, we could use a diverging colour scheme that could show us where the crime rate is lower and/or higher than a critical mid-point, such as the average crime rate across the Boroughs and LSOAs. Create a map of crime in London, at both the Borough and LSOA levels that shows for each of these administrative geographies the percentage difference from the overall mean crime rate.\n\n\n\n\n\n\n\n\nYou will need to start by calculating the average crime rate for both datasets and subsequently calculate the difference from these values for each geography.\nAll calculations can be done using the field calculator in QGIS, but you might have to think through writing the right expression.\n\n\n\n\n\n\n\nThat is us all done. Remember to save your project. And remember the reading list."
  },
  {
    "objectID": "03-cartography.html#slides-w03",
    "href": "03-cartography.html#slides-w03",
    "title": "1 Cartography and Visualisation",
    "section": "",
    "text": "The slides for this week’s lecture can be downloaded here: [Link]"
  },
  {
    "objectID": "03-cartography.html#reading-w03",
    "href": "03-cartography.html#reading-w03",
    "title": "1 Cartography and Visualisation",
    "section": "",
    "text": "Longley, P. et al. 2015. Geographic Information Science & Systems, Chapter 4: Georeferencing, pp. 77-98. [Link]\nLongley, P. et al. 2015. Geographic Information Science & Systems, Chapter 11: Cartography and Map Production, pp. 237-252. [Link]\nWong, D. 2009. Modifiable Areal Unit Problem. International Encyclopedia of Human Geography 169-174. [Link]\n\n\n\n\n\nLongley, P. et al. 2015. Geographic Information Science & systems, Chapter 12: Geovisualization, pp. 266-289. [Link]\nUsery, L. and Seong, J. 2001. All equal-area map projections are created equal, but some are more equal than others. Cartography and Geographic Information Science 28(3): 183-194. [Link]"
  },
  {
    "objectID": "03-cartography.html#crime-in-london-i",
    "href": "03-cartography.html#crime-in-london-i",
    "title": "1 Cartography and Visualisation",
    "section": "",
    "text": "Over the next few weeks, we will explore the spatial patterns of crime across London from a spatial perspective. Reid et al. (2018) suggest:\n\nSpatial analysis can be employed in both an exploratory and well as a more confirmatory manner with the primary purpose of identifying how certain community or ecological factors (such as population characteristics or the built environment) influence the spatial patterns of crime.\n\nAgainst this background, we are actually going to answer a very simple question today: does our perception of crime rates (and its distribution) in London vary at different scales? Here we are looking to test whether we would make the ecological fallacy mistake of assuming patterns at the LSOA level are the same at the Borough level by looking to directly account for the impact of the Modifiable Area Unit Problem within our results. Here we will be looking specifically at a specific type of crime: Theft from a person.\n\n\n\n\n\n\nThe datasets you will create in this practical will be used in other practicals, so make sure to follow every step and export your data to your data folder at the end of the practical.\n\n\n\n\n\nFor our crime data, we will use data directly from the Police Data Portal, which you can find at data.polic.uk. This data portal allows you to access and generate tabular data for crime recorded in the UK across different the different police forces since 2017.\n\n\n\n\n\n\nIn the United Kingdom, there are 45 territorial police forces (TPF) and 3 special police forces (SPF). Each TPF covers a specific area in the UK (e.g. the West Midlands Police Force), whilst the SPFs are cross-jurisdiction and cover specific types of crime, such as the British Transport Police. Therefore, when we want to download data for a specific area, we need to know which police force covers our area interest.\n\n\n\nWhen you look to download crime data for London, there are two territorial police forces working within the city and its greater metropolitan area:\n\nThe Metropolitan Police Force, which covers Greater London.\nThe City of London Police, which covers the City of London.\n\n\n\nNormally, we would now head to the Police Data Portal and download our crime data. However, the manual processing that is required to clean and prepare the data that we need today is too exhaustive and far easier to do using programming, so you can download a pre-filtered spreadsheet below. Unzip the file and copy the csv into a new folder in your raw data folder called: crime.\n\n\n\nFile\nType\nLink\n\n\n\n\nTheft from persons in 2021\ncsv\nDownload\n\n\n\n\n\n\n\n\n\nWhen mapping the data from the provided longitude and latitude coordinates, it is important to know that these locations represent the approximate location of a crime — not the exact place where it happened. This displacement is introduced on purpose to preserve anonymity of the individuals involved. The process by how this displacement occurs is standardised. There is a list of anonymous map points to which the exact location of each crime is compared against this master list to find the nearest map point. The co-ordinates of the actual crime are then replaced with the co-ordinates of the map point. Each map point is specifically chosen to avoid associating that point with an exact household.\n\n\n\nIf we want to study a phenomena like crime and aggregate it to an areal unit, we will need to normalise this by some denominator (e.g. population). Why? When we record events created by humans, there is often a population bias: simply, more people in an area will by probability lead to a higher occurrence of said event. Fortunately, we already prepared a 2021 LSOA population dataset last week.\nIn addition to our LSOA level dataset, we also need a population dataset for our London Boroughs today. We can do this utilising by joining Borough population data to Borough spatial data and creating a Borough2021_London.gpkg file.\n\n\n\nA lot of data about London is collated by the Greater London Authority (GLA) and made available through the London Datastore. Whereas some of the data is relatively old, it is a good place to get some data specific to London:\n\nNavigate to the London Datastore: [Link].\nClick on Data in the navigation menu.\nType London Boroughs into the search field.\nDownload the GeoPackage containing the boundaries of each of London’s 33 Boroughs.\nRename the file to Boroughs_London.gpkg and move it to boundaries folder in your raw data folder.\n\n\n\n\nThe final data that we need is the total population for each of the London Boroughs in 2021. A good source for this is the 2021 Census again.\n\nNavigate to the Nomis portal: [Link]\nClick on Query data in the Data Downloads panel.\nClick on Census 2021 -&gt; Topic Summaries.\nClick on TS007A - Age by five-year age bands.\nSelect Geography and set Local authorities: district / unitary (as of April 2023) to Some.\nUnder List areas within select London. Click on Tick all.\nClick on Download data at the left hand side of the screen.\nOnce the data is ready for download, download the file to your computer and save it as LondonBorough2021_population.xlsx.\nOpen the dataset in your spreadsheet editing software and extract the relevant information: the Borough names and associated population counts. Copy the data that you require into a new csv file and rename the columns as BoroughNames and pop2021.\nNow format the pop2021 column so that it is recognised as being a numeric column.\nSave the file as a new csv in your data folder: LondonBorough2021_population.csv.\n\n\n\n\n\n\n\n\nStart QGIS\nClick on Project -&gt; New. Save your project as w3-crime-analysis. Remember to save your work throughout the practical.\nBefore we get started with adding data, we will first set the Coordinate Reference System of our Project. Click on Project -&gt; Properties – CRS. In the Filter box, type British National Grid. Select OSGB 1936 / British National Grid - EPSG:27700 and click Apply. Click OK.\n\n\n\n\n\nClick on Layer -&gt; Add Layer -&gt; Add Vector Layer.\nWith File select as your source type, click on the small three dots button and navigate to your LSOA2021_London.gpkg file in your boundaries folder. Select the file, then click Add. You may need to close the box after adding the layer.\n\nWe now need to create our Borough population spatial layer. To do so, we need to repeat exactly the same process as last week in terms of joining our table data to our spatial layer. You should now be able to do this, so we will not provide you with detailed instructions. Remember, you need to:\n\nLoad the respective Borough dataset as a Vector Layer found in your raw/boundaries/ data folder: Boroughs_London.gpkg.\nLoad the respective population dataset that you just created as a Delimited Text File Layer. Remember the settings, including no geometry!\nJoin the two datasets together using the Join tool in the Borough dataset Properties box. Remember which fields to use, which to add, and to remove the prefix.\nExport your joined dataset into a new dataset within your data folder as: Boroughs_London_Pop2021.gpkg.\nMake sure this dataset is loaded into your Layers / added to the map.\nRemove the original Borough and population data layers.\n\n\n\n\n\n\n\nBecause the population file that we downloaded does not contain Borough codes, we need to use the Borough names to join the two files together. Whilst it works here and we can easily manually check whether the join was successful because there are only 33 records, codes are preferred as there is much less room for error and incomplete joins. This means it is often worth it to spend a bit more time and find these codes over relying on names.\n\n\n\n\n\n\nWe will now load and map our crime data. We will load this data using the Delimited Text File Layer option you would have used just now to load the Borough population, but this time we will be adding point coordinates to map our crime data as points.\n\nClick on Layer -&gt; Add Layer -&gt; Add Delimited Text File Layer.\n\nClick on the three dots button next to File Name and navigate to your crime-theft-2021-london.csv in your raw/crime folder.\nClick on the .csv file of this dataset and click Open.\nYour file format should be set to csv. In Record and Fields Options tick Decimal separator is comma, First record has field names, Detect field types and Discard empty fields.\nUnder Geometry Definition, select Point coordinates and set the X field to Longitude and the Y field to Latitude. The Geometry CRS should be: EPSG:4326 - WGS84. Click Add.\n\n\n\n\n\n\n\n\nWhen you click Add, you should have gotten a pop-up from QGIS asking about transformations. Transformations are algorithms that convert data from one CRS to another. QGIS knows that the Project CRS is British National Grid but the Layer you are trying to add has a WGS84 CRS. QGIS is asking you what transformation it should use to project the Layer in the Project CRS. This is one of the key strengths of QGIS: it can project data ‘on the fly’. What this means is that QGIS will automatically convert all Layers to the Project CRS once it knows which transformation you would like to use so that they will all be rendered in the correct position with respect to each other. More details on this can be found in QGIS’ user manual section on working with projections.\n\n\n\n\nClick OK to accept QGIS’ suggested on-the-fly projection. You should now see your crime dataset displayed on the map canvas.\n\n\n\n\n\n\nFigure 1: Borough map with crime data. [Enlarge image]\n\n\n\n\n\nWe can test the temporary nature of the projection by looking at the CRS of the crime-theft-2021-london layer. Right-click on the layer then select Properties -&gt; Information and then look at the associated CRS. You should see that the CRS of the layer is still WGS84.\n\nWe want to make sure our analysis is as accurate and efficient as possible, so it is best to reproject our data into the same CRS as our administrative datasets, i.e. British National Grid. This also means we will have the dataset to use in other projects, just in case.\n\nBack in the main QGIS window, click on Vector -&gt; Data Management Tools -&gt; Reproject Layer. Fill in the parameters as follows:\n\nInput Layer: crime-theft-2021-london\nTarget CRS: Project CRS: EPSG: 27700\nReprojected: Click on the three buttons and Save to GeoPackage to create a new data file.\nSave it in your raw/crime folder as crime-theft-2021-london-prj.gpkg, using theft2021 as Layer Name.\nClick Run. You should now see the new data layer added to your project. You can now close the Reproject Layer tool.\n\nYou can now also remove the original crime-theft-2021-london dataset, only keeping the reprojected version.\n\n\n\n\nThe next step of our analysis involves assigning the crime point data to our administrative geographies. We will use the Count Points in Polygons in the Analysis toolset to count how many crimes have occurred in our LSOAs and our Boroughs. We will then have a count statistic that we need to normalise by population data to create a crime rate statistic.\n\nClick on Vector -&gt; Analysis Tools -&gt; Count Points in Polygons.\nWithin the toolbox, select the parameters as follows:\n\nPolygons: Boroughs_London_Pop2021\nPoints: theft2021 [Note how both our data layers state the same CRS.]\nNo weight field or class field\nCount field names: theft2021\nClick on the three dot button and Save to GeoPackage: output -&gt; Borough2021_crime.gpkg, with borough-theft2021 as Layer Name.\n\nClick Run and Close the box. You should now see an Output layer added to your Layers box. Rename the layer to borough-theft2021.\nRight-click on the borough-theft2021 layer and open the Attribute Table. You should now see a theft2021 column next to your pop2021 column. You can look through the column to see the different levels of crime in the each Borough. You can also sort the column, from small to big, big to small, like you would do in a spreadsheet software.\n\nWhilst it is great that we have got our theft2021, what we actually need is a crime rate to account for the different population sizes across the Boroughs. To get our crime rate statistic, we are going to do our first bit of table manipulation in QGIS.\n\nWith the Attribute Table of your borough-theft2021 layer still open, click on the pencil icon at the top left corner. This pencil switches on the Editing mode.\n\n\n\n\n\n\n\nThe Editing mode allows you to edit both the Attribute Table values and the geometry of your data. When it comes to the Attribute Table, it means you can directly edit existing values in the table or create and add new fields to the table. Whilst you can actually do the latter outside of the Editing mode, this Editing mode means you can reverse any edits you make and they are not permanent just in case you make a mistake. Using the Editing mode is the safest approach to editing your table, however, it might not always be the approach you use when generating new fields.\n\n\n\n\nWhilst in the Editing mode, click on New Field button (hotkeys: ctrl + w or cmd + w and fill in the Field Parameters as follows:\n\nName: crime-rate\nComment: leave blank\nType: Decimal number\n\nClick OK. You should now see a new field added to our Attribute Table.\n\nThe empty field has NULL populated for each row, so we need to find a way to give our Boroughs some crime rate data. To do this, we will calculate a simple Crime Rate using the Field Calculator tool provided by QGIS within the Attribute Table. We will create a crime rate that details the number of crimes per 10,000 people in the Borough.\n\nWhilst still in the Editing mode, click on the Abacus button (ctrl + i or cmd + i), which is known as the Field Calculator.\nIn the Field Calculator window:\n\nCheck the Update existing field box.\nUse the drop-down to select the crime-rate field.\nIn the Expression editor, add the following expression: ( \"theft2021\" / \"pop2021\" ) * 10000\nOnce done, click OK.\n\nClick on the Save button to save these edits. Click again on the Pencil button to exit Editing mode.\n\n\n\n\n\n\n\nThe crime-rate is stored as a decimal as this is required for the calculation to succeed, but ultimately you cannot have half a crime. You can transform the decimal number to an integer by forcing the crime-rate column to a new field using the same Field Calculator. Instead of ticking the Update existing field box you would now keep the Create a new field box ticked. Name the new field crime-rate-int, make sure the Output field type is set to Whole number (integer), and use the following expression to create a new field with an integer value: to_int(\"crime-rate\"). Save your changes by clicking on the Save button.\n\n\n\n\nWe now have our Borough crime rate dataset ready for mapping. We just now need to repeat this process to have our LSOA dataset. So, add the LSOA2021_London.gpkg file to your project and repeat the above process to create a crime-rate and crime-rate-int column within the LSOA dataset as well. After reordering some of the layers, your screen should look something like this:\n\n\n\n\n\n\nFigure 2: QGIS view containing crime data layers for both the London Boroughs and LSOAs. [Enlarge image]\n\n\n\n\n\n\n\n\nNow you have both datasets ready, it is time to style the maps. Remember to use the Properties box to symbolise your maps. Think through using an appropriate colour scheme, perhaps have a look at the online colorbrewer 2.0 for inspiration if you do not want to use the default settings. Once you are happy with their symbolisation, we can turn them into proper publishable maps using QGIS’s Print Layout.\n\n\n\n\n\nFigure 3: An example of how your maps could have been symbolised. [Enlarge image]\n\n\n\n\n\n\n\n\n\n\nIf you have used ArcMap before, Print Layout is similar to switch the view of your map canvas to a print layout within the main window but in QGIS’s case, it loads up a new window.\n\n\n\nFrom the main QGIS window, click on Project -&gt; New Print Layout. In the small box that first appears, call your new print layout: crime-map-borough-lsoa. A new window should open up that shows a blank canvas. On the left-hand side of the window, you will find buttons to add print layout items: the current QGIS map canvas, text labels, images, legends, scale bars, basic shapes, arrows, attribute tables and HTML frames. In this toolbar you also find buttons to navigate, zoom in on an area and pan the view on the layout a well as buttons to select any layout item and to move the contents of the map item.\nOn the right-hand side of the window, you will find two set of panels. The upper one holds the panels Items and Undo History and the lower holds the panels Layout, Item properties and Atlas generation. Today, we are most interested in the bottom panel as Layout will control the overall look of our map, whilst Item properties will allow us to customise the elements, such as title or legend, that we may add to our map.\nIn the bottom part of the window, you can find a status bar with mouse position, current page number, a combo box to set the zoom level and the number of selected items if applicable. In the upper part of the window, you can find menus and other toolbars. All print layout tools are available in menus and as icons in a toolbar.\n\n\n\n\n\n\nWorking with maps in the Print Layout is simple but it can be a little fiddly and, to make more complicated maps, requires you to understand how to use certain aspects of Print Layout, such as locking items. If you get stuck, have a look at the training manual or the detailed documentation.\n\n\n\nTo start with creating a map, you use the Add Map tool to draw a box in which a snapshot of the current active map you have displayed in your QGIS main window will be loaded.\n\nClick on the Add Map tool and draw a box in the first half of our map to load our current map. Note, you can move your map around and resize the box simply by clicking on it as you would in Word etc.\n\n\n\n\n\n\nFigure 4: Current active map in the Print Layout. [Enlarge image]\n\n\n\n\n\nWith your map selected, head to the Items Properties panel and look for the Scale parameter.\n\nHere we can manually edit the scale of our map to find the right zoom level.\nHave a go at entering different values and see what level you think suits the size of your map.\nKeep a note of the scale, as we will need this for the second map we will add to our map layout - our LSOA map.\nNext, in the same panel, if you would like, you can add a frame to your map - this will draw a box (of your selected formatting) around the current map.\nIn the same panel, note down the size of your map - we want to make sure the next map we add is of the same size.\nNote, if you need to move the position of the map within the box, look for the Move Item Content tool on the left-hand side toolbar.\nOnce you are done, finally click on the Lock Layers and Lock Style for layers.\n\n\nBy locking the Layers (and their symbology) in our map, it means we can change our data/map in our main QGIS window without changing the map in the Print Layout - as we will see in a minute when adding our Borough crime rate map. If we do not lock our layers, our map would automatically update to whatever is next displayed in the main QGIS window.\n\nClick on the Add Legend tool and again, draw a box on your map in which your legend will appear.\n\nAs you will see, your Legend auto-generates an entry for every layer in our Layers box in the main QGIS application:\nIn Item Properties, uncheck auto-update - this stops QGIS automatically populating your legend and enables you to customise your legend.\nFirst, let us rename our layer in the legend to: Borough Crime Rate (per 10,000 people).\nNext, we want to remove all other Layers, using the - (minus) button\nWe can also customise the Legend further, including type, size and alignment of font. Go ahead and style your legend as you would prefer.\nMove the Legend to an appropriate part of the layout near your Borough crime rate map - resize if necessary.\n\n\n\n\n\n\n\nFigure 5: The automatically generated legend for your map. [Enlarge image]\n\n\n\n\nNow we are finished with the Borough map, we want to make sure we do not change any aspect of its layout. To do so, we need to lock both the map layer and legend in the Items panel. This prevents us accidentally moving items in our layout. Note, this is different to locking your layers in the Items Properties as we did earlier.\n\nIn the Items panel, click the Lock check box for both our map layer and legend.\nIn the main QGIS window, uncheck your borough-theft2021 layer and make sure your lsoa-theft2021 layer is now visible.\nReturn to the Print Layout window and repeat the process above of adding a map to the window. This time you should see your LSOA map loaded in the box (and you should see no changes to your Borough map).\n\nPlace your LSOA map next to your LSOA map - use the snap grids to help.\nSet your LSOA map to the same zoom level as your Borough map.\nMake sure your LSOA map is the same size as your Borough map.\nAdd a frame if you want.\nLock your layer and its symbology in the Items Properties once ready and the lock your layer in the Items panel.\n\n\nWe now just need to add a second legend for our Borough map.\n\n\n\n\n\n\nIf we had standardised our values across our two maps, then we would only need to use one legend. However, in this case, as there are large differences in the value ranges, we need to have two legends.\n\n\n\n\nRepeat the process as above to add a Legend for our LSOA map.\n\nRemember to re-title the legend to make it more legible/informative.\nMatch the same formatting for a clean look.\n\nOnce complete, lock these two items in the Items panel as well.\nNow we have our two maps ready, we can add our main map elements. Using the tools on the left-hand tool bar:\n\nAdd a scale bar: use the Item Properties to adjust the Style, number of segments, font, etc.\nAdd a north arrow: draw a box to generate the arrow and then use the Item Properties to adjust..\nAdd a title at the top of the page, and subtitles above the individual maps.\nFinally add a box detailing our data sources, you can copy and paste the following: Contains National Statistics data © Crown copyright and database right [2021] (Open Government Licence). Contains Ordnance Survey data © Crown copyright and database right [2021]. Crime data obtained from data.police.uk (Open Government Licence).\n\n\nOnce you have added these properties in, you should have something that looks a little like this:\n\n\n\n\n\nFigure 6: Crime rates in London Boroughs and LSOAs. [Enlarge image]\n\n\n\n\nThe only thing outstanding is to export our map to a file. Go to Layout -&gt; Export as Image and then save it as London-2021-crime-rate.png."
  },
  {
    "objectID": "03-cartography.html#assignment-w03",
    "href": "03-cartography.html#assignment-w03",
    "title": "1 Cartography and Visualisation",
    "section": "",
    "text": "Looking at the maps you have created, how does your perception of crime (and its distribution) in London vary at different scales?\nAt the moment, we have looked at the crime rate as a relative amount. We therefore use a sequential colour scheme that shows, predominantly, where the crime rate is the highest. Alternatively, we could use a diverging colour scheme that could show us where the crime rate is lower and/or higher than a critical mid-point, such as the average crime rate across the Boroughs and LSOAs. Create a map of crime in London, at both the Borough and LSOA levels that shows for each of these administrative geographies the percentage difference from the overall mean crime rate.\n\n\n\n\n\n\n\n\nYou will need to start by calculating the average crime rate for both datasets and subsequently calculate the difference from these values for each geography.\nAll calculations can be done using the field calculator in QGIS, but you might have to think through writing the right expression."
  },
  {
    "objectID": "03-cartography.html#byl-w03",
    "href": "03-cartography.html#byl-w03",
    "title": "1 Cartography and Visualisation",
    "section": "",
    "text": "That is us all done. Remember to save your project. And remember the reading list."
  },
  {
    "objectID": "09-raster.html",
    "href": "09-raster.html",
    "title": "1 Rasters, Zonal Statistics, and Interpolation",
    "section": "",
    "text": "The majority of our module has focused on the use of vector data and tabular data. This week, we switch it up by focusing primarily on raster data and its analysis using map algebra and zonal statistics.\n\n\nThe slides for this week’s lecture can be downloaded here: [Link]\n\n\n\n\n\n\nGimond, M. 2021. Intro to GIS and spatial analysis. Chapter 14: Spatial Interpolation. [Link]\nHeris, M., Foks, N., Bagstad, K. 2020. A rasterized building footprint dataset for the United States. Scientific Data 7: 207. [Link]\nThomson, D., Leasure, D., Bird, T. et al. 2022. How accurate are WorldPop-Global-Unconstrained gridded population data at the cell-level? A simulation analysis in urban Namibia. Plos ONE 17:7: e0271504. [Link]\n\n\n\n\n\nMellander, C., Lobo, J., Stolarick, K. et al. 2015. Night-time light data: a good proxy measure for economic activity? PLoS ONE 10(10): e0139779. [Link]\n\n\n\n\n\nIn previous weeks, we have predominantly worked with vector data and/or tabular data that we then join to vector data for analysis. However, depending on the nature of your research problem, you may also encounter raster data. This week’s content introduces you to raster data, map algebra and interpolation. After first looking at population change in London using raster data, we will then look at generating pollution maps in London from individual point readings taken from air quality monitoring sites across London. To complete this analysis, we will be using several new datasets:\n\nPopulation rasters for Great Britain: WorldPop raster on estimated population counts for Great Britain in 2010 and 2020 at a spatial resolution of 1km.\nNO2 readings across London: A dataset contain readings of NO2 for individual air quality monitoring sites in London.\n\n\n\n\n\n\nFigure 1: A hypothetical raster and a vector model of landuse. [Enlarge image]\n\n\n\n\n\n\n\n\n\n\nThe main difference between vector and raster models is how they are structured. Our vectors are represented by three different types of geometries: points, lines and polygons. We have used point data in the form of our stations and bike theft, and polygons in the form of our ward and borough boundaries. In comparison, our raster datasets are composed of pixels (or grid cells) — a bit like an image. This means that a raster dataset represents a geographic phenomenon by dividing the world into a set of rectangular cells that are laid out in a grid. Each cell holds one value that represents the value of that phenomena at the location, e.g. a population density at that grid cell location. In comparison to vector data, we do not have an attribute table containing fields to analyse. All analysis conducted on a raster dataset therefore is primarily conducted on the cell values of a raster, rather than on the attribute values of the observations contained within our dataset or the precise geometries of our dataset. Probably one of the most common or well-known types of raster data are those that we can derive from remote sensing, including satellite and RADAR/LIDAR imagery that we see used in many environmental modelling applications, such as land use and pollution monitoring.\n\n\n\n\n\nOpen a new script within your GEOG0030 project and save this script as wk9-raster-analysis.r. At the top of your script, add the following metadata:\n\n\n\nR code\n\n# Raster analysis \n# Date: January 2024\n\n\nNow let us add all of the libraries we will be using today to the top of our script:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\nlibrary(terra)\nlibrary(openair)\nlibrary(gstat)\n\n\nFor the first part of this week’s practical material we will be using raster datasets from WorldPop:\n\n“WorldPop develops peer-reviewed research and methods for the construction of open and high-resolution geospatial data on population distributions, demographics and dynamics, with a focus on low and middle income countries.”\n\nThese population surfaces are estimates of counts of people, displayed within a regular grid raster of a spatial resolution of up to 100m. These surfaces can be used to explore, for example, changes in the demographic profiles of small areas, area deprivation, or country of birth.\n\nNavigate to the WorldPop Hub: [Link]\nGo to Population Count -&gt; Unconstrained individual countries 2000-2020 (1km resolution).\nType United Kingdom in the search bar.\nDownload the GeoTIFF files for 2010 and 2020: gbr_ppp_2010_1km_Aggregated and gbr_ppp_2020_1km_Aggregated.\nSave the files in your population folder.\n\n\n\n\n\nMap algebra is a set-based algebra for manipulating geographic data, coined by Dana Tomlin in the early 1980s. Map algebra uses maths-like operations, including addition, subtraction and multiplication to update raster cell values. The most common type of map algebra is to apply these operations using a cell-by-cell function. These operations might include:\n\nArithmetic operations that use basic mathematical functions like addition, subtraction, multiplication and division.\nStatistical operations that use statistical operations such as minimum, maximum, average and median.\nRelational operations which compare cells using functions such as greater than, smaller than or equal to.\n\n\n\n\n\n\n\nThe utilisation of these functions can enable many different types of specialised raster analysis, such as recoding or reclassifying individual rasters to reduce complexity in their data values, generating the Normalised Difference Vegetation Index for a satellite imagery dataset or calculating Least Cost Surfaces to find the most efficient path from one cell in a raster to another. Furthermore, using multiple raster datasets, it is possible to combine these data through mathematical overlays, from the basic mathematical operations mentioned above to more complex modelling..\n\n\n\nWe will be using some simple map algebra to look at population change in London between 2010 and 2020. Let’s get started and take a look at our data. We can load raster data into R using the terra library:\n\n\n\nR code\n\n# load data\npop2010 &lt;- rast(\"data/raw/population/gbr_ppp_2010_1km_Aggregated.tif\")\npop2020 &lt;- rast(\"data/raw/population/gbr_ppp_2020_1km_Aggregated.tif\")\n\n# transform projection\npop2010 &lt;- pop2010 |&gt;\n    project(\"epsg:27700\")\npop2020 &lt;- pop2020 |&gt;\n    project(\"epsg:27700\")\n\n\n\n\n\nR code\n\n# plot 2010\nplot(pop2010)\n\n\n\n\n\nFigure 2: WorldPop 2010 population estimates for the UK.\n\n\n\n\n\n\n\nR code\n\n# plot 2020\nplot(pop2020)\n\n\n\n\n\nFigure 3: WorldPop 2020 population estimates for the UK.\n\n\n\n\nYou should see that whilst your maps look very similar, the legend certainly shows that the values associated with each cell has grown over the 10 years between 2010 and 2021: we see our maximum increase from about 12,000 people per cell to well-over 14,000 people per cell. Now we have our raster data loaded, we want to reduce it to show only the extent of London.\n\n\n\n\n\n\nThe terra package does not take in sf objects, so once we have loaded the London MSOA file we need to transform the file into a SpatRaster or SpatVector. The process of turning a vector dataset into a raster dataset is called rasterising.\n\n\n\n\n\nFigure 4: Turning a line and polygon vector into a raster. Source: Lovelace et al. 2023.\n\n\n\n\n\n\n\n\n\n\nR code\n\n# load data, get outline, rasterise\nmsoa_london &lt;- st_read(\"data/raw/boundaries/MSOA2021_London.gpkg\") |&gt;\n    vect()\n\n\nReading layer `MSOA2021_London' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/raw/boundaries/MSOA2021_London.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1002 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n\n# crop\npop2010_london &lt;- crop(pop2010, msoa_london)\npop2020_london &lt;- crop(pop2020, msoa_london)\n\n# mask\npop2010_london &lt;- mask(pop2010_london, msoa_london)\npop2020_london &lt;- mask(pop2020_london, msoa_london)\n\n\n\n\nR code\n\n# plot 2010\nplot(pop2010_london)\n\n\n\n\n\nFigure 5: WorldPop 2010 population estimates for London.\n\n\n\n\n\n\n\nR code\n\n# plot 2020\nplot(pop2020_london)\n\n\n\n\n\nFigure 6: WorldPop 2020 population estimates for London.\n\n\n\n\nNow we have our two London population rasters, we can calculate population change between the two time periods by subtracting our 2010 population raster from our 2020 population raster:\n\n\n\nR code\n\n# subtract\nlonpop_change &lt;- pop2020_london - pop2010_london\n\n# plot\nplot(lonpop_change)\n\n\n\n\n\nFigure 7: Population change in London 2010-2020.\n\n\n\n\n\n\n\nTo further analyse our population change raster, we can create a smoothed version of our lonpop_change raster by using the focal() function. Using the focal() function, we generate a raster that summarises the average (mean) value of the 9 nearest neighbours for each cell, using a weight matrix defined in our w parameter and set to a matrix:\n\n\n\nR code\n\n# subtract\nlonpop_smooth &lt;- focal(lonpop_change, w = matrix(1, 3, 3), fun = mean)\n\n# plot\nplot(lonpop_change)\n\n\n\n\n\nFigure 8: Smoothed version of population change in London 2010-2020.\n\n\n\n\nThe differences are not very noticeable, but you were to subtract the smoothed raster from the original raster you will see that definitely something happened:\n\n\n\nR code\n\n# plot the results\nplot(lonpop_change - lonpop_smooth)\n\n\n\n\n\nFigure 9: Difference smoothed population change with original population change raster.\n\n\n\n\nWe can also look to use zonal functions to better represent our population change by aggregating our data to coarser resolutions. For example, we can resize our raster’s spatial resolution to contain larger grid cells which will, of course, simplify our data, making larger trends more visible in our data but may similarly end up obfuscating smaller trends.\n\n\n\n\n\n\nWe can resize our lonpop_change raster by using the aggregate() function and setting the fact (factor) parameter to the order of rescaling we would like (e.g. increase both the width and height of a cell by a factor of two). We then provide the fun (function) by which to aggregate our data, in this case, we will continue to use the mean but we could also use the min or max depending on our application.\n\n\n\n\n\n\nR code\n\n# aggregate\nlonpop_agg &lt;- aggregate(lonpop_change, fact = 2, fun = mean)\n\n# plot\nplot(lonpop_agg)\n\n\n\n\n\nFigure 10: Aggregated cell values.\n\n\n\n\nWhere we transformed a vector dataset into a raster dataset earlier, in some cases you would want to aggregate move from raster to vector. For example, in our case, we can aggregate the lonpop_change raster to our actual London MSOA boundaries, i.e. calculate for each MSOA in our dataset the average (or other function) population change,. We can, of course, use other functions other than the mean. What function you use will simply depend on your application.\n\n# aggregate\nlondon_msoa_pop &lt;- extract(lonpop_change, msoa_london, fun = mean)\n\n# add to spatial dataframe\nmsoa_london &lt;- msoa_london |&gt;\n  st_as_sf() |&gt;\n  mutate(pop_change = london_msoa_pop$gbr_ppp_2020_1km_Aggregated)\n\n# plot\ntm_shape(msoa_london) +\n  tm_fill(\n    col = \"pop_change\"\n  )\n\nVariable(s) \"pop_change\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\nFigure 11: Aggregating raster values to a vector geography.\n\n\n\n\nWe now have a vector dataset that we could go ahead and run many of the analyses that we have completed in previous weeks. Furthermore, we can use this data within other analyses we might want to complete.\n\n\n\n\n\n\nTrying to calculate population change, particularly across decades as we have done here, can be quite challenging with changing administrative boundaries. Using raster data can be a good workaround to these issues, provided that the different rasters are of same size and extent.\n\n\n\n\n\n\nFor the second part of this week’s practical material, we will explore several methods of spatial data interpolation by looking at air pollution in London using Londonair data. Londonair is the website of the London Air Quality Network (LAQN), and shows air pollution in London and southeast England that is provided by the Environmental Research Group of Imperial College London. The data are publicly available for download and we can use an R package to directly interact with the data without needing to download it. The openair R package enables us to import data directly from the Londonair website. We will focus on Nitrogen Dioxide (NO2) measurements.\n\n\n\n\n\n\nSpatial interpolation is the prediction of a given phenomenon in unmeasured locations. There are many reasons why we may wish to interpolate point data across a map. It could be because we are trying to predict a variable across space, including in areas where there are little to no data.\n\n\n\n\n\n\nR code\n\n# get list of all measurement sites operated by Imperial College\n# limit to sites with data for 2022\nsite_meta &lt;- importMeta(source = \"kcl\", all = TRUE, year = 2022:2022)\n\n# download all data pertaining to these sites\npollution &lt;- importKCL(site = c(site_meta$code), year = 2022:2022, pollutant = \"no2\", meta = TRUE)\n\n\n\n\n\n\n\n\nThis second part of the code might take some time to run as it will try to download the data for all sites for an entire year — and in many cases data is measured hourly. Despite us limiting our data download, not all measurements sites collect data on NO2 so you will get some warnings along the lines of 404 Not Found. In case you run into too many errors or it is just taking too long, you can download a copy of the data here: [Download]. Once downloaded, copy over the zip and put this into a data/raw/pollution folder. The file is rather large, so you can leave it unzipped.\n\n\n\nLet’s inspect the data:\n\n\n\nR code\n\n# load from zip if not downloaded through the Open Air library\npollution &lt;- read_csv(\"data/raw/pollution/london_no2_2022.zip\")\n\n\nMultiple files in zip: reading 'london_no2_2022.csv'\nRows: 1624324 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): site, code, source, site_type\ndbl  (3): no2, latitude, longitude\ndttm (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# inspect\nhead(pollution)\n\n# A tibble: 6 × 8\n  date                  no2 site            code  source latit…¹ longi…² site_…³\n  &lt;dttm&gt;              &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;  \n1 2022-01-01 00:00:00    NA City of London… CTA   kcl       51.5 -0.0921 Roadsi…\n2 2022-01-01 01:00:00    NA City of London… CTA   kcl       51.5 -0.0921 Roadsi…\n3 2022-01-01 02:00:00    NA City of London… CTA   kcl       51.5 -0.0921 Roadsi…\n4 2022-01-01 03:00:00    NA City of London… CTA   kcl       51.5 -0.0921 Roadsi…\n5 2022-01-01 04:00:00    NA City of London… CTA   kcl       51.5 -0.0921 Roadsi…\n6 2022-01-01 05:00:00    NA City of London… CTA   kcl       51.5 -0.0921 Roadsi…\n# … with abbreviated variable names ¹​latitude, ²​longitude, ³​site_type\n\n\nWe can see that in our first five rows we have data for the same site and if we look at the date field, we can see we have a reading observation for every hour. With 24 hours in the day, 365 days in a year and hundreds of sites, it should therefore be of no surprise that we have such a large csv. Let’s summarise the values by site to make the dataset a bit more workable:\n\n\n\nR code\n\n# calculate mean no2 values\npollution_avg &lt;- pollution |&gt;\n    filter(!is.na(latitude) & !is.na(no2)) |&gt;\n    group_by(code, latitude, longitude) |&gt;\n    summarise(no2 = mean(no2))\n\n\n`summarise()` has grouped output by 'code', 'latitude'. You can override using\nthe `.groups` argument.\n\n# inspect\nhead(pollution_avg)\n\n# A tibble: 6 × 4\n# Groups:   code, latitude [6]\n  code  latitude longitude   no2\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 AH0       49.8    -7.56   1.82\n2 BE1       54.6    -5.93   5.05\n3 BG1       51.6     0.178 16.8 \n4 BG2       51.5     0.133 20.6 \n5 BH0       50.8    -0.148 11.6 \n6 BK0       53.8    -3.01   6.11\n\n\nWe are now left with only 45 measurement sites — with only the latitudes, longitudes, and the average NO2 values associated with each. When using interpolation, the distribution and density of our data points will impact the accuracy of our final raster and we may end up with a level of uncertainty in the areas where measurements are more sparse. Let’s have a look at the spatial distribution of the measurement sites.\n\n# load MSOAs for reference\nmsoa_london &lt;- st_read(\"data/raw/boundaries/MSOA2021_London.gpkg\") |&gt;\n  st_union()\n\nReading layer `MSOA2021_London' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/raw/boundaries/MSOA2021_London.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1002 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n\n# create a point spatial dataframe\nmeasurement_sites &lt;- pollution_avg |&gt;\n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326) |&gt;\n  st_transform(27700)\n\n# ensure all points within London\nmeasurement_sites &lt;- measurement_sites |&gt;\n  st_intersection(msoa_london)\n\nWarning: attribute variables are assumed to be spatially constant throughout all\ngeometries\n\n# add London outline\ntm_shape(msoa_london) +\n  tm_polygons(\n    col = \"grey\"\n  ) +\n  # add measurement sites\n  tm_shape(measurement_sites) +\n  tm_dots()\n\n\n\n\nFigure 12: NO2 measurement sites in London.\n\n\n\n\nLet’s also have a look whether measurements differ across London:\n\n# add London outline\ntm_shape(msoa_london) +\n  tm_polygons(\n    cols = \"grey\"\n  ) +\n  # add proportional symbol\n  tm_shape(measurement_sites) +\n  tm_bubbles(\n    size = \"no2\",\n    col = \"blue\",\n    style = \"pretty\",\n    border.col = \"white\",\n    title.size = \"Average NO2 ug/m3 reading in 2022\"\n  ) +\n  # add legend\n  tm_layout(\n    legend.position = c(\"left\", \"top\")\n  ) +\n  # add north arrow\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\")\n  ) +\n  # add scale bar\n  tm_scale_bar(\n    breaks = c(0, 5, 10, 15, 20),\n    position = c(\"left\", \"bottom\")\n  ) +\n  # add credits\n  tm_credits(\"NO~2~ measurements from London Air (2022).\")\n\n\n\n\nFigure 13: Proportional symbol map on NO2 measurements in London.\n\n\n\n\nOur proportional symbols shows that there is some heterogeneity in NO2 measurements across London — both in terms of coverage and in terms of NO2 levels. This means that if we want to make a reasonable assumptions about NO2 levels in an area where no measurement was taking, we need to interpolate the missing values.\n\n\nThe first step we can take to interpolate the data across space is to create Thiessen polygons. Thiessen polygons are formed to assign boundaries of the areas closest to each unique point. Therefore, for every point in a dataset, it has a corresponding Thiessen polygon.\n\n\n\n\n\nFigure 14: Thiessen polygons. [Enlarge image]\n\n\n\n\n\n\n\n\n\n\nBesides Thiessen you may also come across the term Voronoi polygons. Both terms are used interchangeably to describe this type of geometry created from point data. In the field of GIS we tend to refer to them as Thiessen polygons, but in other fields they are often referred to as Voronoi diagrams, in honour of the mathematician Georgy Voronoy.\n\n\n\nWe can create Thiessen polygons using the sf library with a bit of code: we will create a simple function called st_thiessen_point() that we can use to generate Thiessen polygons directly from a point dataset.\n\n\n\n\n\n\nDo not worry about fully understanding the code behind the function, but simply understand what input (a point spatial dataframe) and output (a Thiessen polygon spatial dataframe) it will provide.\n\n\n\n\n\n\nR code\n\n# function\nst_thiessen_point &lt;- function(points) {\n\n    # input check\n    if (!all(st_geometry_type(points) == \"POINT\")) {\n        stop(\"Input not POINT geometries\")\n    }\n\n    # to multipoint\n    g = st_combine(st_geometry(points))\n\n    # to thiessen\n    v = st_voronoi(g)\n    v = st_collection_extract(v)\n\n    # return\n    return(v[unlist(st_intersects(points, v))])\n}\n\n# call function\nmeasurement_sites_thiessen &lt;- st_thiessen_point(measurement_sites)\n\n# force thiessen polygons to measurement sites dataframe\nmeasurement_sites_thiessen_df &lt;- measurement_sites |&gt;\n    st_set_geometry(measurement_sites_thiessen) |&gt;\n    st_intersection(msoa_london)\n\n\nWarning: attribute variables are assumed to be spatially constant throughout all\ngeometries\n\n# inspect\nmeasurement_sites_thiessen_df\n\nSimple feature collection with 103 features and 2 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 103 × 3\n   code    no2                                                          geometry\n * &lt;chr&gt; &lt;dbl&gt;                                                    &lt;GEOMETRY [m]&gt;\n 1 BG1    16.8 POLYGON ((547785.5 187913.4, 557897 187404.3, 550800.7 184315.1,…\n 2 BG2    20.6 MULTIPOLYGON (((543729.1 183422.7, 543923.7 183583, 543960.2 183…\n 3 BL0    25.7 POLYGON ((529120.3 182395, 529101.2 184092.5, 531261 183754.7, 5…\n 4 BQ7    16.1 MULTIPOLYGON (((546347 181055.1, 546593.8 181106.4, 546697.1 181…\n 5 BT4    43.1 POLYGON ((516082 187365.6, 520958.9 189405.3, 521236.1 184358.2,…\n 6 BT5    28.0 POLYGON ((523877.3 190896.9, 524273.2 190424, 525686.9 187185.8,…\n 7 BT6    27.7 POLYGON ((522237.8 181852.6, 519969.8 183532.7, 519912.6 183741.…\n 8 BT8    28.5 POLYGON ((522982.8 184472, 525686.9 187185.8, 526408.6 186420, 5…\n 9 BX1    17.6 MULTIPOLYGON (((550063.2 169311.6, 550099.6 169246.6, 550115.1 1…\n10 BX2    15.9 MULTIPOLYGON (((548349.8 175998.9, 549623.4 180763, 550028.2 180…\n# … with 93 more rows\n\n\nWe can now visualise these Thiessen polygons with their associated NO2 value:\n\n# add thiessen polygons\ntm_shape(measurement_sites_thiessen_df) +\n  tm_polygons(\n    col = \"no2\",\n    palette = \"Blues\"\n  ) +\n  # add legend\n  tm_layout(\n    legend.show = TRUE\n  )\n\n\n\n\nFigure 15: Interpolation of NO2 measurements in London using Thiessen polygons.\n\n\n\n\nAnd that’s it! We now have our values interpolated using our Thiessen polygon approach. However, as you can see, our approach is quite coarse. Whilst we, of course, can see areas of high and low pollution, it really does not offer us as much spatial detail as we would like, particularly when we know there are better methods out there to use.\n\n\n\nA second method to interpolate point data is Inverse Distance Weighting (IDW). An IDW is a means of converting point data of numerical values into a continuous surface to visualise how the data may be distributed across space. The technique interpolates point data by using a weighted average of a variable from nearby points to predict the value of that variable for each location. The weighting of the points is determined by their inverse distances, essentially drawing on Tobler’s first law of geography.\n\n\n\n\n\n\nThe distance weighting is done by a power function: the larger the power coefficient, the stronger the weight of nearby point. The output is most commonly represented as a raster surface.\n\n\n\nWe will start by generating an empty grid to store the predicted values before executing a simple the IDW.\n\n\n\nR code\n\n# create regular output grid from London outline\noutput_grid &lt;- msoa_london |&gt;\n    st_make_grid(cellsize = c(500, 500))\n\n# execute IDW\nmeasurement_sites_idw &lt;- idw(formula = no2 ~ 1, locations = measurement_sites, newdata = output_grid,\n    beta = 2)\n\n\n[inverse distance weighted interpolation]\n\n# clip to London outline\nmeasurement_sites_idw &lt;- measurement_sites_idw |&gt;\n    st_intersection(msoa_london)\n\nWarning: attribute variables are assumed to be spatially constant throughout all\ngeometries\n\n\nWe now have our final predicted raster surface. To map it, we can again use the tmap as we have done previously. For our polygon raster, the name of the layer we need to provide is var1.pred.\n\n# add idw grid\ntm_shape(measurement_sites_idw) +\n  tm_fill(\n    col = \"var1.pred\",\n    style = \"quantile\",\n    n = 100,\n    palette = \"Reds\",\n    legend.show = FALSE\n  ) +\n  # add measurement sites\n  tm_shape(measurement_sites) +\n  tm_dots()\n\n\n\n\nFigure 16: Interpolation of NO2 measurements in London using Inverse Distance Weighting.\n\n\n\n\n\n\n\n\n\n\nWe have used an output cell size of 500x500 metres. A smaller cell size will create a smoother IDW output, but it does add uncertainty to these estimates as we do not exactly have a substantial amount of data points to interpolate from. Keep in mind: reducing the cell size will also exponentially increase processing time.\n\n\n\n\n\n\n\nFor your final assignment this week, we want you to redo the IDW interpolation of the London pollution data for the months of June and December and see to what extent there are differences between these months. In order to do this you will, at least, need to:\n\nCreate monthly averages for the pollution data. This will involve some data wrangling.\nFor both the months of June and December create a spatial dataframe containing the London monitoring sites and their average NO2 reading.\nConduct an Inverse Distance Weighting interpolation for both months of data.\nCombine the results to identify areas that might differ.\n\n\n\n\n\n\nA raster dataset often only contains one layer, i.e. one variable. Hence when we want to map a raster, we use the tm_raster() and provide the layer name for mapping. However, satellite imagery, for instance, consists of three bands: a band with a red value, a band with a green value and a band with a blue value. This is known as multi-band imagery. We can visualise each band independently of one another, however, you would see that you end up with either a nearly all red, green or blue image. If you are interested to learn more about using satellite imagery with R and raster analysis in general, this is a good place to start. Alternatively, if you are interested in more advanced interpolation methods, Manual Gimond’s tutorial on spatial data interpolation will get you started.\n\n\n\n\nThis week, we’ve looked at raster datasets and how we use the terra library to manage and process them. Specifically, we looked at using map algebra to apply mathematical operations to rasters. We further looked at two different interpolation methods to generate raster data from point data. Understanding how to interpolate data correctly is incredibly important. Whilst in most instances you will be working with vector data, especially where government statistics and administrative boundaries are involved, there are also plenty of use cases in which you will need to generate raster data from point data, as we have done today. With that being said: that is it for our penultimate week. Suppose nothing left to do besides checking out that reading?"
  },
  {
    "objectID": "09-raster.html#slides-w09",
    "href": "09-raster.html#slides-w09",
    "title": "1 Rasters, Zonal Statistics, and Interpolation",
    "section": "",
    "text": "The slides for this week’s lecture can be downloaded here: [Link]"
  },
  {
    "objectID": "09-raster.html#reading-w09",
    "href": "09-raster.html#reading-w09",
    "title": "1 Rasters, Zonal Statistics, and Interpolation",
    "section": "",
    "text": "Gimond, M. 2021. Intro to GIS and spatial analysis. Chapter 14: Spatial Interpolation. [Link]\nHeris, M., Foks, N., Bagstad, K. 2020. A rasterized building footprint dataset for the United States. Scientific Data 7: 207. [Link]\nThomson, D., Leasure, D., Bird, T. et al. 2022. How accurate are WorldPop-Global-Unconstrained gridded population data at the cell-level? A simulation analysis in urban Namibia. Plos ONE 17:7: e0271504. [Link]\n\n\n\n\n\nMellander, C., Lobo, J., Stolarick, K. et al. 2015. Night-time light data: a good proxy measure for economic activity? PLoS ONE 10(10): e0139779. [Link]"
  },
  {
    "objectID": "09-raster.html#raster-data",
    "href": "09-raster.html#raster-data",
    "title": "1 Rasters, Zonal Statistics, and Interpolation",
    "section": "",
    "text": "In previous weeks, we have predominantly worked with vector data and/or tabular data that we then join to vector data for analysis. However, depending on the nature of your research problem, you may also encounter raster data. This week’s content introduces you to raster data, map algebra and interpolation. After first looking at population change in London using raster data, we will then look at generating pollution maps in London from individual point readings taken from air quality monitoring sites across London. To complete this analysis, we will be using several new datasets:\n\nPopulation rasters for Great Britain: WorldPop raster on estimated population counts for Great Britain in 2010 and 2020 at a spatial resolution of 1km.\nNO2 readings across London: A dataset contain readings of NO2 for individual air quality monitoring sites in London.\n\n\n\n\n\n\nFigure 1: A hypothetical raster and a vector model of landuse. [Enlarge image]\n\n\n\n\n\n\n\n\n\n\nThe main difference between vector and raster models is how they are structured. Our vectors are represented by three different types of geometries: points, lines and polygons. We have used point data in the form of our stations and bike theft, and polygons in the form of our ward and borough boundaries. In comparison, our raster datasets are composed of pixels (or grid cells) — a bit like an image. This means that a raster dataset represents a geographic phenomenon by dividing the world into a set of rectangular cells that are laid out in a grid. Each cell holds one value that represents the value of that phenomena at the location, e.g. a population density at that grid cell location. In comparison to vector data, we do not have an attribute table containing fields to analyse. All analysis conducted on a raster dataset therefore is primarily conducted on the cell values of a raster, rather than on the attribute values of the observations contained within our dataset or the precise geometries of our dataset. Probably one of the most common or well-known types of raster data are those that we can derive from remote sensing, including satellite and RADAR/LIDAR imagery that we see used in many environmental modelling applications, such as land use and pollution monitoring.\n\n\n\n\n\nOpen a new script within your GEOG0030 project and save this script as wk9-raster-analysis.r. At the top of your script, add the following metadata:\n\n\n\nR code\n\n# Raster analysis \n# Date: January 2024\n\n\nNow let us add all of the libraries we will be using today to the top of our script:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\nlibrary(terra)\nlibrary(openair)\nlibrary(gstat)\n\n\nFor the first part of this week’s practical material we will be using raster datasets from WorldPop:\n\n“WorldPop develops peer-reviewed research and methods for the construction of open and high-resolution geospatial data on population distributions, demographics and dynamics, with a focus on low and middle income countries.”\n\nThese population surfaces are estimates of counts of people, displayed within a regular grid raster of a spatial resolution of up to 100m. These surfaces can be used to explore, for example, changes in the demographic profiles of small areas, area deprivation, or country of birth.\n\nNavigate to the WorldPop Hub: [Link]\nGo to Population Count -&gt; Unconstrained individual countries 2000-2020 (1km resolution).\nType United Kingdom in the search bar.\nDownload the GeoTIFF files for 2010 and 2020: gbr_ppp_2010_1km_Aggregated and gbr_ppp_2020_1km_Aggregated.\nSave the files in your population folder."
  },
  {
    "objectID": "09-raster.html#map-algebra",
    "href": "09-raster.html#map-algebra",
    "title": "1 Rasters, Zonal Statistics, and Interpolation",
    "section": "",
    "text": "Map algebra is a set-based algebra for manipulating geographic data, coined by Dana Tomlin in the early 1980s. Map algebra uses maths-like operations, including addition, subtraction and multiplication to update raster cell values. The most common type of map algebra is to apply these operations using a cell-by-cell function. These operations might include:\n\nArithmetic operations that use basic mathematical functions like addition, subtraction, multiplication and division.\nStatistical operations that use statistical operations such as minimum, maximum, average and median.\nRelational operations which compare cells using functions such as greater than, smaller than or equal to.\n\n\n\n\n\n\n\nThe utilisation of these functions can enable many different types of specialised raster analysis, such as recoding or reclassifying individual rasters to reduce complexity in their data values, generating the Normalised Difference Vegetation Index for a satellite imagery dataset or calculating Least Cost Surfaces to find the most efficient path from one cell in a raster to another. Furthermore, using multiple raster datasets, it is possible to combine these data through mathematical overlays, from the basic mathematical operations mentioned above to more complex modelling..\n\n\n\nWe will be using some simple map algebra to look at population change in London between 2010 and 2020. Let’s get started and take a look at our data. We can load raster data into R using the terra library:\n\n\n\nR code\n\n# load data\npop2010 &lt;- rast(\"data/raw/population/gbr_ppp_2010_1km_Aggregated.tif\")\npop2020 &lt;- rast(\"data/raw/population/gbr_ppp_2020_1km_Aggregated.tif\")\n\n# transform projection\npop2010 &lt;- pop2010 |&gt;\n    project(\"epsg:27700\")\npop2020 &lt;- pop2020 |&gt;\n    project(\"epsg:27700\")\n\n\n\n\n\nR code\n\n# plot 2010\nplot(pop2010)\n\n\n\n\n\nFigure 2: WorldPop 2010 population estimates for the UK.\n\n\n\n\n\n\n\nR code\n\n# plot 2020\nplot(pop2020)\n\n\n\n\n\nFigure 3: WorldPop 2020 population estimates for the UK.\n\n\n\n\nYou should see that whilst your maps look very similar, the legend certainly shows that the values associated with each cell has grown over the 10 years between 2010 and 2021: we see our maximum increase from about 12,000 people per cell to well-over 14,000 people per cell. Now we have our raster data loaded, we want to reduce it to show only the extent of London.\n\n\n\n\n\n\nThe terra package does not take in sf objects, so once we have loaded the London MSOA file we need to transform the file into a SpatRaster or SpatVector. The process of turning a vector dataset into a raster dataset is called rasterising.\n\n\n\n\n\nFigure 4: Turning a line and polygon vector into a raster. Source: Lovelace et al. 2023.\n\n\n\n\n\n\n\n\n\n\nR code\n\n# load data, get outline, rasterise\nmsoa_london &lt;- st_read(\"data/raw/boundaries/MSOA2021_London.gpkg\") |&gt;\n    vect()\n\n\nReading layer `MSOA2021_London' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/raw/boundaries/MSOA2021_London.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1002 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n\n# crop\npop2010_london &lt;- crop(pop2010, msoa_london)\npop2020_london &lt;- crop(pop2020, msoa_london)\n\n# mask\npop2010_london &lt;- mask(pop2010_london, msoa_london)\npop2020_london &lt;- mask(pop2020_london, msoa_london)\n\n\n\n\nR code\n\n# plot 2010\nplot(pop2010_london)\n\n\n\n\n\nFigure 5: WorldPop 2010 population estimates for London.\n\n\n\n\n\n\n\nR code\n\n# plot 2020\nplot(pop2020_london)\n\n\n\n\n\nFigure 6: WorldPop 2020 population estimates for London.\n\n\n\n\nNow we have our two London population rasters, we can calculate population change between the two time periods by subtracting our 2010 population raster from our 2020 population raster:\n\n\n\nR code\n\n# subtract\nlonpop_change &lt;- pop2020_london - pop2010_london\n\n# plot\nplot(lonpop_change)\n\n\n\n\n\nFigure 7: Population change in London 2010-2020."
  },
  {
    "objectID": "09-raster.html#zonal-statistics",
    "href": "09-raster.html#zonal-statistics",
    "title": "1 Rasters, Zonal Statistics, and Interpolation",
    "section": "",
    "text": "To further analyse our population change raster, we can create a smoothed version of our lonpop_change raster by using the focal() function. Using the focal() function, we generate a raster that summarises the average (mean) value of the 9 nearest neighbours for each cell, using a weight matrix defined in our w parameter and set to a matrix:\n\n\n\nR code\n\n# subtract\nlonpop_smooth &lt;- focal(lonpop_change, w = matrix(1, 3, 3), fun = mean)\n\n# plot\nplot(lonpop_change)\n\n\n\n\n\nFigure 8: Smoothed version of population change in London 2010-2020.\n\n\n\n\nThe differences are not very noticeable, but you were to subtract the smoothed raster from the original raster you will see that definitely something happened:\n\n\n\nR code\n\n# plot the results\nplot(lonpop_change - lonpop_smooth)\n\n\n\n\n\nFigure 9: Difference smoothed population change with original population change raster.\n\n\n\n\nWe can also look to use zonal functions to better represent our population change by aggregating our data to coarser resolutions. For example, we can resize our raster’s spatial resolution to contain larger grid cells which will, of course, simplify our data, making larger trends more visible in our data but may similarly end up obfuscating smaller trends.\n\n\n\n\n\n\nWe can resize our lonpop_change raster by using the aggregate() function and setting the fact (factor) parameter to the order of rescaling we would like (e.g. increase both the width and height of a cell by a factor of two). We then provide the fun (function) by which to aggregate our data, in this case, we will continue to use the mean but we could also use the min or max depending on our application.\n\n\n\n\n\n\nR code\n\n# aggregate\nlonpop_agg &lt;- aggregate(lonpop_change, fact = 2, fun = mean)\n\n# plot\nplot(lonpop_agg)\n\n\n\n\n\nFigure 10: Aggregated cell values.\n\n\n\n\nWhere we transformed a vector dataset into a raster dataset earlier, in some cases you would want to aggregate move from raster to vector. For example, in our case, we can aggregate the lonpop_change raster to our actual London MSOA boundaries, i.e. calculate for each MSOA in our dataset the average (or other function) population change,. We can, of course, use other functions other than the mean. What function you use will simply depend on your application.\n\n# aggregate\nlondon_msoa_pop &lt;- extract(lonpop_change, msoa_london, fun = mean)\n\n# add to spatial dataframe\nmsoa_london &lt;- msoa_london |&gt;\n  st_as_sf() |&gt;\n  mutate(pop_change = london_msoa_pop$gbr_ppp_2020_1km_Aggregated)\n\n# plot\ntm_shape(msoa_london) +\n  tm_fill(\n    col = \"pop_change\"\n  )\n\nVariable(s) \"pop_change\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\nFigure 11: Aggregating raster values to a vector geography.\n\n\n\n\nWe now have a vector dataset that we could go ahead and run many of the analyses that we have completed in previous weeks. Furthermore, we can use this data within other analyses we might want to complete.\n\n\n\n\n\n\nTrying to calculate population change, particularly across decades as we have done here, can be quite challenging with changing administrative boundaries. Using raster data can be a good workaround to these issues, provided that the different rasters are of same size and extent."
  },
  {
    "objectID": "09-raster.html#interpolation",
    "href": "09-raster.html#interpolation",
    "title": "1 Rasters, Zonal Statistics, and Interpolation",
    "section": "",
    "text": "For the second part of this week’s practical material, we will explore several methods of spatial data interpolation by looking at air pollution in London using Londonair data. Londonair is the website of the London Air Quality Network (LAQN), and shows air pollution in London and southeast England that is provided by the Environmental Research Group of Imperial College London. The data are publicly available for download and we can use an R package to directly interact with the data without needing to download it. The openair R package enables us to import data directly from the Londonair website. We will focus on Nitrogen Dioxide (NO2) measurements.\n\n\n\n\n\n\nSpatial interpolation is the prediction of a given phenomenon in unmeasured locations. There are many reasons why we may wish to interpolate point data across a map. It could be because we are trying to predict a variable across space, including in areas where there are little to no data.\n\n\n\n\n\n\nR code\n\n# get list of all measurement sites operated by Imperial College\n# limit to sites with data for 2022\nsite_meta &lt;- importMeta(source = \"kcl\", all = TRUE, year = 2022:2022)\n\n# download all data pertaining to these sites\npollution &lt;- importKCL(site = c(site_meta$code), year = 2022:2022, pollutant = \"no2\", meta = TRUE)\n\n\n\n\n\n\n\n\nThis second part of the code might take some time to run as it will try to download the data for all sites for an entire year — and in many cases data is measured hourly. Despite us limiting our data download, not all measurements sites collect data on NO2 so you will get some warnings along the lines of 404 Not Found. In case you run into too many errors or it is just taking too long, you can download a copy of the data here: [Download]. Once downloaded, copy over the zip and put this into a data/raw/pollution folder. The file is rather large, so you can leave it unzipped.\n\n\n\nLet’s inspect the data:\n\n\n\nR code\n\n# load from zip if not downloaded through the Open Air library\npollution &lt;- read_csv(\"data/raw/pollution/london_no2_2022.zip\")\n\n\nMultiple files in zip: reading 'london_no2_2022.csv'\nRows: 1624324 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): site, code, source, site_type\ndbl  (3): no2, latitude, longitude\ndttm (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# inspect\nhead(pollution)\n\n# A tibble: 6 × 8\n  date                  no2 site            code  source latit…¹ longi…² site_…³\n  &lt;dttm&gt;              &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;  \n1 2022-01-01 00:00:00    NA City of London… CTA   kcl       51.5 -0.0921 Roadsi…\n2 2022-01-01 01:00:00    NA City of London… CTA   kcl       51.5 -0.0921 Roadsi…\n3 2022-01-01 02:00:00    NA City of London… CTA   kcl       51.5 -0.0921 Roadsi…\n4 2022-01-01 03:00:00    NA City of London… CTA   kcl       51.5 -0.0921 Roadsi…\n5 2022-01-01 04:00:00    NA City of London… CTA   kcl       51.5 -0.0921 Roadsi…\n6 2022-01-01 05:00:00    NA City of London… CTA   kcl       51.5 -0.0921 Roadsi…\n# … with abbreviated variable names ¹​latitude, ²​longitude, ³​site_type\n\n\nWe can see that in our first five rows we have data for the same site and if we look at the date field, we can see we have a reading observation for every hour. With 24 hours in the day, 365 days in a year and hundreds of sites, it should therefore be of no surprise that we have such a large csv. Let’s summarise the values by site to make the dataset a bit more workable:\n\n\n\nR code\n\n# calculate mean no2 values\npollution_avg &lt;- pollution |&gt;\n    filter(!is.na(latitude) & !is.na(no2)) |&gt;\n    group_by(code, latitude, longitude) |&gt;\n    summarise(no2 = mean(no2))\n\n\n`summarise()` has grouped output by 'code', 'latitude'. You can override using\nthe `.groups` argument.\n\n# inspect\nhead(pollution_avg)\n\n# A tibble: 6 × 4\n# Groups:   code, latitude [6]\n  code  latitude longitude   no2\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 AH0       49.8    -7.56   1.82\n2 BE1       54.6    -5.93   5.05\n3 BG1       51.6     0.178 16.8 \n4 BG2       51.5     0.133 20.6 \n5 BH0       50.8    -0.148 11.6 \n6 BK0       53.8    -3.01   6.11\n\n\nWe are now left with only 45 measurement sites — with only the latitudes, longitudes, and the average NO2 values associated with each. When using interpolation, the distribution and density of our data points will impact the accuracy of our final raster and we may end up with a level of uncertainty in the areas where measurements are more sparse. Let’s have a look at the spatial distribution of the measurement sites.\n\n# load MSOAs for reference\nmsoa_london &lt;- st_read(\"data/raw/boundaries/MSOA2021_London.gpkg\") |&gt;\n  st_union()\n\nReading layer `MSOA2021_London' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/raw/boundaries/MSOA2021_London.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1002 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n\n# create a point spatial dataframe\nmeasurement_sites &lt;- pollution_avg |&gt;\n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326) |&gt;\n  st_transform(27700)\n\n# ensure all points within London\nmeasurement_sites &lt;- measurement_sites |&gt;\n  st_intersection(msoa_london)\n\nWarning: attribute variables are assumed to be spatially constant throughout all\ngeometries\n\n# add London outline\ntm_shape(msoa_london) +\n  tm_polygons(\n    col = \"grey\"\n  ) +\n  # add measurement sites\n  tm_shape(measurement_sites) +\n  tm_dots()\n\n\n\n\nFigure 12: NO2 measurement sites in London.\n\n\n\n\nLet’s also have a look whether measurements differ across London:\n\n# add London outline\ntm_shape(msoa_london) +\n  tm_polygons(\n    cols = \"grey\"\n  ) +\n  # add proportional symbol\n  tm_shape(measurement_sites) +\n  tm_bubbles(\n    size = \"no2\",\n    col = \"blue\",\n    style = \"pretty\",\n    border.col = \"white\",\n    title.size = \"Average NO2 ug/m3 reading in 2022\"\n  ) +\n  # add legend\n  tm_layout(\n    legend.position = c(\"left\", \"top\")\n  ) +\n  # add north arrow\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\")\n  ) +\n  # add scale bar\n  tm_scale_bar(\n    breaks = c(0, 5, 10, 15, 20),\n    position = c(\"left\", \"bottom\")\n  ) +\n  # add credits\n  tm_credits(\"NO~2~ measurements from London Air (2022).\")\n\n\n\n\nFigure 13: Proportional symbol map on NO2 measurements in London.\n\n\n\n\nOur proportional symbols shows that there is some heterogeneity in NO2 measurements across London — both in terms of coverage and in terms of NO2 levels. This means that if we want to make a reasonable assumptions about NO2 levels in an area where no measurement was taking, we need to interpolate the missing values.\n\n\nThe first step we can take to interpolate the data across space is to create Thiessen polygons. Thiessen polygons are formed to assign boundaries of the areas closest to each unique point. Therefore, for every point in a dataset, it has a corresponding Thiessen polygon.\n\n\n\n\n\nFigure 14: Thiessen polygons. [Enlarge image]\n\n\n\n\n\n\n\n\n\n\nBesides Thiessen you may also come across the term Voronoi polygons. Both terms are used interchangeably to describe this type of geometry created from point data. In the field of GIS we tend to refer to them as Thiessen polygons, but in other fields they are often referred to as Voronoi diagrams, in honour of the mathematician Georgy Voronoy.\n\n\n\nWe can create Thiessen polygons using the sf library with a bit of code: we will create a simple function called st_thiessen_point() that we can use to generate Thiessen polygons directly from a point dataset.\n\n\n\n\n\n\nDo not worry about fully understanding the code behind the function, but simply understand what input (a point spatial dataframe) and output (a Thiessen polygon spatial dataframe) it will provide.\n\n\n\n\n\n\nR code\n\n# function\nst_thiessen_point &lt;- function(points) {\n\n    # input check\n    if (!all(st_geometry_type(points) == \"POINT\")) {\n        stop(\"Input not POINT geometries\")\n    }\n\n    # to multipoint\n    g = st_combine(st_geometry(points))\n\n    # to thiessen\n    v = st_voronoi(g)\n    v = st_collection_extract(v)\n\n    # return\n    return(v[unlist(st_intersects(points, v))])\n}\n\n# call function\nmeasurement_sites_thiessen &lt;- st_thiessen_point(measurement_sites)\n\n# force thiessen polygons to measurement sites dataframe\nmeasurement_sites_thiessen_df &lt;- measurement_sites |&gt;\n    st_set_geometry(measurement_sites_thiessen) |&gt;\n    st_intersection(msoa_london)\n\n\nWarning: attribute variables are assumed to be spatially constant throughout all\ngeometries\n\n# inspect\nmeasurement_sites_thiessen_df\n\nSimple feature collection with 103 features and 2 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 103 × 3\n   code    no2                                                          geometry\n * &lt;chr&gt; &lt;dbl&gt;                                                    &lt;GEOMETRY [m]&gt;\n 1 BG1    16.8 POLYGON ((547785.5 187913.4, 557897 187404.3, 550800.7 184315.1,…\n 2 BG2    20.6 MULTIPOLYGON (((543729.1 183422.7, 543923.7 183583, 543960.2 183…\n 3 BL0    25.7 POLYGON ((529120.3 182395, 529101.2 184092.5, 531261 183754.7, 5…\n 4 BQ7    16.1 MULTIPOLYGON (((546347 181055.1, 546593.8 181106.4, 546697.1 181…\n 5 BT4    43.1 POLYGON ((516082 187365.6, 520958.9 189405.3, 521236.1 184358.2,…\n 6 BT5    28.0 POLYGON ((523877.3 190896.9, 524273.2 190424, 525686.9 187185.8,…\n 7 BT6    27.7 POLYGON ((522237.8 181852.6, 519969.8 183532.7, 519912.6 183741.…\n 8 BT8    28.5 POLYGON ((522982.8 184472, 525686.9 187185.8, 526408.6 186420, 5…\n 9 BX1    17.6 MULTIPOLYGON (((550063.2 169311.6, 550099.6 169246.6, 550115.1 1…\n10 BX2    15.9 MULTIPOLYGON (((548349.8 175998.9, 549623.4 180763, 550028.2 180…\n# … with 93 more rows\n\n\nWe can now visualise these Thiessen polygons with their associated NO2 value:\n\n# add thiessen polygons\ntm_shape(measurement_sites_thiessen_df) +\n  tm_polygons(\n    col = \"no2\",\n    palette = \"Blues\"\n  ) +\n  # add legend\n  tm_layout(\n    legend.show = TRUE\n  )\n\n\n\n\nFigure 15: Interpolation of NO2 measurements in London using Thiessen polygons.\n\n\n\n\nAnd that’s it! We now have our values interpolated using our Thiessen polygon approach. However, as you can see, our approach is quite coarse. Whilst we, of course, can see areas of high and low pollution, it really does not offer us as much spatial detail as we would like, particularly when we know there are better methods out there to use.\n\n\n\nA second method to interpolate point data is Inverse Distance Weighting (IDW). An IDW is a means of converting point data of numerical values into a continuous surface to visualise how the data may be distributed across space. The technique interpolates point data by using a weighted average of a variable from nearby points to predict the value of that variable for each location. The weighting of the points is determined by their inverse distances, essentially drawing on Tobler’s first law of geography.\n\n\n\n\n\n\nThe distance weighting is done by a power function: the larger the power coefficient, the stronger the weight of nearby point. The output is most commonly represented as a raster surface.\n\n\n\nWe will start by generating an empty grid to store the predicted values before executing a simple the IDW.\n\n\n\nR code\n\n# create regular output grid from London outline\noutput_grid &lt;- msoa_london |&gt;\n    st_make_grid(cellsize = c(500, 500))\n\n# execute IDW\nmeasurement_sites_idw &lt;- idw(formula = no2 ~ 1, locations = measurement_sites, newdata = output_grid,\n    beta = 2)\n\n\n[inverse distance weighted interpolation]\n\n# clip to London outline\nmeasurement_sites_idw &lt;- measurement_sites_idw |&gt;\n    st_intersection(msoa_london)\n\nWarning: attribute variables are assumed to be spatially constant throughout all\ngeometries\n\n\nWe now have our final predicted raster surface. To map it, we can again use the tmap as we have done previously. For our polygon raster, the name of the layer we need to provide is var1.pred.\n\n# add idw grid\ntm_shape(measurement_sites_idw) +\n  tm_fill(\n    col = \"var1.pred\",\n    style = \"quantile\",\n    n = 100,\n    palette = \"Reds\",\n    legend.show = FALSE\n  ) +\n  # add measurement sites\n  tm_shape(measurement_sites) +\n  tm_dots()\n\n\n\n\nFigure 16: Interpolation of NO2 measurements in London using Inverse Distance Weighting.\n\n\n\n\n\n\n\n\n\n\nWe have used an output cell size of 500x500 metres. A smaller cell size will create a smoother IDW output, but it does add uncertainty to these estimates as we do not exactly have a substantial amount of data points to interpolate from. Keep in mind: reducing the cell size will also exponentially increase processing time."
  },
  {
    "objectID": "09-raster.html#assignment-w09",
    "href": "09-raster.html#assignment-w09",
    "title": "1 Rasters, Zonal Statistics, and Interpolation",
    "section": "",
    "text": "For your final assignment this week, we want you to redo the IDW interpolation of the London pollution data for the months of June and December and see to what extent there are differences between these months. In order to do this you will, at least, need to:\n\nCreate monthly averages for the pollution data. This will involve some data wrangling.\nFor both the months of June and December create a spatial dataframe containing the London monitoring sites and their average NO2 reading.\nConduct an Inverse Distance Weighting interpolation for both months of data.\nCombine the results to identify areas that might differ."
  },
  {
    "objectID": "09-raster.html#wm-w09",
    "href": "09-raster.html#wm-w09",
    "title": "1 Rasters, Zonal Statistics, and Interpolation",
    "section": "",
    "text": "A raster dataset often only contains one layer, i.e. one variable. Hence when we want to map a raster, we use the tm_raster() and provide the layer name for mapping. However, satellite imagery, for instance, consists of three bands: a band with a red value, a band with a green value and a band with a blue value. This is known as multi-band imagery. We can visualise each band independently of one another, however, you would see that you end up with either a nearly all red, green or blue image. If you are interested to learn more about using satellite imagery with R and raster analysis in general, this is a good place to start. Alternatively, if you are interested in more advanced interpolation methods, Manual Gimond’s tutorial on spatial data interpolation will get you started."
  },
  {
    "objectID": "09-raster.html#byl-w09",
    "href": "09-raster.html#byl-w09",
    "title": "1 Rasters, Zonal Statistics, and Interpolation",
    "section": "",
    "text": "This week, we’ve looked at raster datasets and how we use the terra library to manage and process them. Specifically, we looked at using map algebra to apply mathematical operations to rasters. We further looked at two different interpolation methods to generate raster data from point data. Understanding how to interpolate data correctly is incredibly important. Whilst in most instances you will be working with vector data, especially where government statistics and administrative boundaries are involved, there are also plenty of use cases in which you will need to generate raster data from point data, as we have done today. With that being said: that is it for our penultimate week. Suppose nothing left to do besides checking out that reading?"
  },
  {
    "objectID": "10-network.html",
    "href": "10-network.html",
    "title": "1 Transport Network Analysis",
    "section": "",
    "text": "This week we will cover a different type of data: network data. We will take a look at how we can use network data to measure accessibility using the dodgr R library. We will calculate the network distances between combinations of locations (i.e. a set of origins and a set of destinations). These distances can then, for instance, be used to calculate the number of a resource (e.g. fast-food outlets) within a certain distance of a Point of Interest (e.g. a school or population-weighted centroid).\n\n\nThe slides for this week’s lecture can be downloaded here: [Link]\n\n\n\n\n\n\nGeurs, K., Van Wee, B. 2004. Accessibility evaluation of land-use and transport strategies: review and research directions. Journal of Transport Geography 12(2): 127-140. [Link]\nHiggins, C., Palm, M. DeJohn, A. et al. 2022. Calculating place-based transit accessibility: Methods, tools and algorithmic dependence. Journal of Transport and Land Use 15(1): 95-116. [Link]\n\n\n\n\n\nSchwanen, T. and De Jong, T. 2008. Exploring the juggling of responsibilities with space-time accessibility analysis. Urban Geography 29(6): 556-580. [Link]\nVan Dijk, J., Krygsman, S. and De Jong, T. 2015. Toward spatial justice: The spatial equity effects of a toll road in Cape Town, South Africa. Journal of Transport and Land Use 8(3): 95-114. [Link]\nVan Dijk, J. and De Jong, T. 2017. Post-processing GPS-tracks in reconstructing travelled routes in a GIS-environment: network subset selection and attribute adjustment. Annals of GIS 23(3): 203-217. [Link]\n\n\n\n\n\nThe term network analysis covers a wide range of analysis techniques ranging from complex network analysis to social network analysis, and from link analysis to transport network analysis. What the techniques have in common is that they are based on the concept of a network. A network or network graph is constituted by a collection of vertices that are connected to one another by edges. Note, vertices may also be called nodes or points, whilst edges may be called links or lines. Within social network analysis, you may find the terms actors (the vertices) and ties or relations (the edges) also used.\n\n\n\n\n\nFigure 1: Visualising networks with vertices and edges.\n\n\n\n\n\n\nFor this week’s practical, we will be using Portsmouth in the UK as our area of interest for our analysis. One prominent topic within the city is the issue of public health and childhood obesity. According to figures released in March 2020 by Public Health England, more than one in three school pupils are overweight or obese by the time they finish primary school within the city; this is much higher than the national average of one in four. One potential contributor to the health crisis is the ease and availability of fast-food outlets in the city. In the following, we will measure the accessibility of fast-food outlets within specific walking distances of all school in Portsmouth starting at 400m, then 800m and finally a 1km walking distance. We will then aggregate these results to Lower Super Output Areas (LSOA) and overlay these results with some socio-economic variables.\nTo execute this analysis, we will need to first calculate the distances between our schools and fast-food outlets. This involves calculating the shortest distance a child would walk between a school and a fast-food outlet, using roads or streets. We will use the dodgr R package to conduct this transport network analysis.\n\n\n\n\n\n\nAll calculations within the dodgr library currently need to be run in WGS84/4236. This is why we will not transform the CRS of our data in this practical.\n\n\n\n\n\n\nOpen a new script within your GEOG0030 project and save this script as wk10-accessibility-analysis.r. At the top of your script, add the following metadata:\n\n\n\nR code\n\n# Accessibility of fast-food outlets in Portsmouth\n# Date: January 2024\n\n\nNow let us add all of the libraries we will be using today to the top of our script:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(osmdata)\nlibrary(dodgr)\n\n\nTo create our network and Origin-Destination dataset, we will need data on schools, fast-food outlets, and a street network. One data sources that contains probably information on all three topics is OpenStreetMap.\n\n\n\n\n\n\nPlease refer back to Week 6 if you have forgotten about using the Overpass API and key and value pairs.\n\n\n\n\n\n\nTo download our road network dataset, we first define a variable to store our bounding box coordinates, p_bbox(). We then use this within our OSM query to extract specific types of road segments within that bounding box - the results of our query are then stored in an osmdata object. We will select all OSM features with the highway tag that are likely to be used by pedestrians (e.g. not motorways).\n\n\n\nR code\n\n# define our bbox coordinates for Portsmouth\np_bbox &lt;- c(-1.113197, 50.775781, -1.026508, 50.859941)\n\n# pass bounding box coordinates into the OverPassQuery (opq) function only\n# download features that are not classified as motorway\nosmdata &lt;- opq(bbox = p_bbox) |&gt;\n    add_osm_feature(key = \"highway\", value = c(\"primary\", \"secondary\", \"tertiary\",\n        \"residential\", \"path\", \"footway\", \"unclassified\", \"living_street\", \"pedestrian\")) |&gt;\n    osmdata_sf()\n\n\n\n\n\n\n\n\nIn some instances the OSM query will return an error, especially when several people from the same location are executing the exact same query. If this happens, you can just read through the instructions and download a prepared copy of the data that contains all required OSM Portsmouth data instead: [Link].\nYou can load these downloaded data as follows into R:\n\n\n\nR code\n\nload(\"../path/to/file/ports_ff.RData\")\nload(\"../path/to/file/ports_roads_edges.RData\")\nload(\"../path/to/file/ports_schools.RData\")\n\n\nAfter loading your data, you can continue with the analysis in the Measuring Accessiblity section below, starting with the creation of a network graph with the ‘foot weighting’ profile.\n\n\n\nThe osmdata object contains the bounding box of your query, a time-stamp of the query, and then the spatial data as osm_points, osm_lines, osm_multilines and osm_polgyons (which are listed with their respective fields also detailed). Some of the spatial features maybe empty, depending on what you asked your query to return. Our next step therefore is to extract our spatial data from our osmdata object to create our road network data set. This is in fact incredibly easy, using the traditional $ R approach to access these spatial features from our object.\nDeciding what to extract is probably the more complicated aspect of this - mainly as you need to understand how to represent your road network, and this will usually be determined by the library/functions you will be using it within. Today, we want to extract the edges of the network, i.e. the lines that represent the roads, as well as the nodes of the network, i.e. the points that represent the locations at which the roads start, end, or intersect. For our points, we will only keep the osm_id data field, just in case we need to refer to this later. For our lines, we will keep a little more information that we might want to use within our transport network analysis, including the type of road, the maximum speed, and whether the road is one-way or not.\n\n\n\nR code\n\n# extract the points, with their osm_id.\nports_roads_nodes &lt;- osmdata$osm_points[, \"osm_id\"]\n\n# extract the lines, with their osm_id, name, type of highway, max speed and\n# one-way attributes\nports_roads_edges &lt;- osmdata$osm_lines[, c(\"osm_id\", \"name\", \"highway\", \"maxspeed\",\n    \"oneway\")]\n\n\nTo check our data set, we can quickly plot the edges of our road network using the plot() function:\n\n\n\nR code\n\n# inspect\nplot(ports_roads_edges, max.plot = 1)\n\n\n\n\n\nFigure 2: OSM road network\n\n\n\n\nBecause we are focusing on walking, we will overwrite the oneway variable by suggesting that none of the road segments are restricted to one-way traffic which may affect our analysis as well as the general connectivity of the network.\n\n\n\nR code\n\n# overwrite one-way default\nports_roads_edges$oneway &lt;- \"no\"\n\n\nNow we have the network edges, we can turn this into a graph-representation that allows for the calculation of network-based accessibility statistics.\n\n\n\nBefore we can construct our full network graph for the purpose of accessibility analysis, we need to also provide our Origin and Destination points, i.e. the data points we wish to calculate the distances between. According to the dodgr documentation, these points need to be in either a vector or matrix format, containing the two coordinates for each point for the origins and for the destinations.\nAs for our Portsmouth scenario we are interested in calculating the shortest distances between schools and fast-food outlets, we need to try and download these datasets from OpenStreetMap as well. Following a similar structure to our query above, we will use our knowledge of OpenStreetMap keys and values to extract the points of Origins (schools) and Destinations (fast-food outlets) we are interested in:\n\n\n\nR code\n\n# download schools\nschools &lt;- opq(bbox = p_bbox) |&gt;\n    add_osm_feature(key = \"amenity\", value = \"school\") |&gt;\n    osmdata_sf()\n\n# download fast-food outlets\nff_outlets &lt;- opq(bbox = p_bbox) |&gt;\n    add_osm_feature(key = \"amenity\", value = \"fast_food\") |&gt;\n    osmdata_sf()\n\n\nWe also need to then extract the relevant data from the osmdata object:\n\n\n\nR code\n\n# extract school points\nports_schools &lt;- schools$osm_points[, c(\"osm_id\", \"name\")]\n\n# extract fast-food outlet points\nports_ff &lt;- ff_outlets$osm_points[, c(\"osm_id\", \"name\")]\n\n\nWe now have our road network data and our Origin-Destination (OD) points in place and we can now move to construct our network graph and run our transport network analysis.\n\n\n\n\n\n\nIn this analysis, we are highly reliant on the use of OpenStreetMap to provide data for both our Origins and Destinations. Whilst in the UK OSM provides substantial coverage, its quality is not always guaranteed. As a result, to improve on our current methodology in future analysis, we should investigate into a more official school data set or at least validate the number of schools against City Council records. The same applies to our fast-food outlets.\n\n\n\nWith any network analysis, the main data structure is a graph, constructed by our nodes and edges. To create a graph for use within dodgr, we pass our ports_roads_edges() into the weight_streetnet() function. The dodgr library also contains weighting profiles, that you can customise, for use within your network analysis. These weighting profiles contain weights based on the type of road, determined by the type of transportation the profile aims to model. Here we will use the weighting profile foot, as we are looking to model walking accessibility.\n\n\n\nR code\n\n# create network graph with the foot weighting profile\ngraph &lt;- weight_streetnet(ports_roads_edges, wt_profile = \"foot\")\n\n\nOnce we have our graph, we can then use this to calculate our network distances between our OD points. One thing to keep in mind is that potentially not all individual components in the network that we extracted are connected, for instance, because the bounding box cut off the access road of a cul-de-sac. To make sure that our entire extracted network is connected, we now extract the largest connected component of the graph. You can use table(graph$component) to examine the sizes of all individual subgraphs. You will notice that most subgraphs consist of a very small number of edges.\n\n\n\n\n\n\nThe dodgr package documentation explains that components are numbered in order of decreasing size, with $component = 1 always denoting the largest component. Always inspect the resulting subgraph to make sure that its coverage is adequate for analysis.\n\n\n\n\n\n\nR code\n\n# extract the largest connected graph component\ngraph_connected &lt;- graph[graph$component == 1, ]\n\n# inspect number of remaining road segments\nnrow(graph_connected)\n\n\n[1] 57384\n\n# inspect\nplot(dodgr_to_sf(graph_connected), max.plot = 1)\n\n\n\n\nFigure 3: Largest graph component\n\n\n\n\n\n\n\n\n\n\nOpenStreetMap is a living dataset, meaning that changes are made on a continuous basis; as such it may very well possible that the number of remaining road segments as shown above may be slightly different when you run this analysis.\n\n\n\nNow we have our connected subgraph, will can use the dodgr_distances() function to calculate the network distances between every possible Origin and Destination. In the dodgr_distances() function, we first pass our graph, then our Origin points (schools), in the from argument, and then our Destination points (fast-food outlets), in the to argument. One thing to note is our addition of the st_coordinates() function as we pass our two point data sets within the from and to functions as we need to supplement our Origins and Destinations in a matrix format. For all Origins and Destinations, dodgr_distances() will map the points to the closest network points, and return corresponding shortest-path distances.\n\n\n\nR code\n\n# create a distance matrix between schools and fast-food stores\nsch_to_ff_calc &lt;- dodgr_distances(graph_connected, from = st_coordinates(ports_schools),\n    to = st_coordinates(ports_ff), shortest = TRUE, pairwise = FALSE, quiet = FALSE)\n\n\nThe result of this computation is a distance-matrix that contains the network distances between all Origins (i.e. schools) and all Destinations (i.e. fast-food outlets). Let’s inspect the first row of our output. Do you understand what the values mean?\n\n\n\nR code\n\n# inspect\nhead(sch_to_ff_calc, n = 1)\n\n\n         1        2        3        4        5        6        7        8\n1 4000.016 2090.485 6549.779 9130.106 10677.47 2231.919 11642.83 2292.263\n         9       10       11       12       13       14       15       16\n1 1676.914 1680.151 1697.525 1676.914 2090.485 2090.485 2090.485 4000.016\n        17       18       19      20       21       22       23       24\n1 2324.454 4000.016 4000.016 3338.83 1700.179 581.0582 7277.651 3370.976\n        25       26       27       28       29       30       31       32\n1 1359.146 3321.374 340.8823 1102.226 10570.65 2644.278 2720.491 3260.791\n        33       34       35       36       37       38       39       40\n1 2330.386 2580.465 2920.565 2920.565 736.0556 3399.756 844.6083 5996.449\n        41      42       43       44       45       46       47       48\n1 3394.397 8945.04 9146.476 9012.112 9255.798 1629.029 3277.902 2400.918\n        49       50       51       52       53       54       55       56\n1 6937.961 8750.917 9176.882 9103.774 6483.465 6483.465 6470.802 6470.802\n        57       58       59       60       61       62       63       64\n1 804.5849 763.0434 2526.807 1067.907 1010.408 1010.408 1967.697 1418.767\n        65       66      67       68       69       70       71       72\n1 2580.465 1245.823 2157.02 688.8672 2162.922 2205.285 2119.323 11552.65\n        73       74       75       76       77       78       79       80\n1 4603.517 2738.124 844.6083 615.1102 788.7212 2586.476 2667.125 728.4882\n        81       82       83       84       85       86       87       88\n1 11642.83 11642.83 11642.83 11731.98 11731.98 11731.98 11642.83 11642.83\n        89       90       91       92       93       94       95       96\n1 11642.83 11642.83 11642.83 11642.83 11642.83 11113.76 6310.216 6354.465\n        97       98       99      100      101      102      103      104\n1 6354.465 6216.593 4000.016 4000.016 4234.058 5703.495 3841.384 581.0582\n       105      106      107      108      109      110      111      112\n1 597.3575 2654.314 4884.169 5081.596 4823.045 375.2084 926.0127 4823.045\n       113     114      115     116      117      118     119      120      121\n1 3808.049 753.752 556.5173 713.536 622.6949 619.2467 1225.66 3452.874 3363.827\n       122      123      124      125      126      127      128      129\n1 5233.586 6435.029 6399.453 6399.453 6391.258 6399.453 6399.453 6391.258\n       130      131      132      133      134      135      136      137\n1 6435.029 6435.029 6391.258 6435.029 6435.029 6435.029 6435.029 6435.029\n       138      139      140      141      142      143     144      145\n1 12122.56 9302.449 9302.449 5296.254 9113.485 9113.485 9065.06 8337.971\n      146      147     148     149      150     151      152      153      154\n1 9065.06 9113.485 11823.9 11823.9 11831.19 11823.5 9322.319 3349.922 2300.121\n       155      156      157      158      159      160      161      162\n1 815.5434 3219.572 5472.702 905.8238 905.8238 905.8238 4950.036 4968.988\n       163      164      165      166      167      168      169      170\n1 858.9023 3216.085 3159.418 11410.34 4919.699 1813.109 8934.149 1666.407\n       171      172      173      174      175      176      177      178\n1 9373.088 9433.043 4462.093 3177.171 3219.572 3118.594 5747.221 9312.943\n       179      180      181      182      183      184      185      186\n1 9433.043 5462.571 4046.518 8993.699 6190.672 6126.632 8679.466 6672.568\n       187      188      189      190     191      192     193      194\n1 6049.243 10546.09 1505.914 4995.748 11286.5 1574.826 5242.73 1804.091\n       195      196      197      198      199     200     201      202\n1 3084.572 8876.496 4512.959 4968.988 3803.913 12956.7 5242.73 3363.827\n       203      204      205     206      207      208      209      210\n1 3363.827 556.5173 788.7212 9886.96 4866.125 9312.943 9312.943 9312.943\n       211      212      213      214      215      216      217      218\n1 9294.621 9294.621 9294.621 9294.621 9312.943 9312.943 5765.619 5370.568\n      219      220      221      222      223      224      225      226\n1 4492.53 3803.913 4661.927 4913.756 1543.604 1510.665 1559.537 5081.936\n       227      228      229      230      231      232      233      234\n1 11444.06 4719.045 4723.757 4271.398 3277.902 5811.015 5216.759 5081.596\n       235      236      237      238      239      240      241      242\n1 5081.596 5081.596 5081.596 5081.596 5081.596 5081.596 4963.868 6036.662\n      243      244      245      246      247      248      249      250\n1 5140.89 4512.959 5592.362 6399.453 4625.997 11356.79 11381.41 4823.045\n       251      252      253      254      255      256      257      258\n1 4823.045 4823.045 4823.045 4866.125 4895.861 4866.125 4895.861 4895.861\n       259      260      261      262      263      264      265      266\n1 4890.115 11642.83 6549.779 6549.779 6529.533 6549.779 6549.779 6529.533\n       267      268      269      270      271      272      273      274\n1 4582.555 4968.988 11642.83 4548.575 4136.321 1129.672 1302.289 1302.289\n       275      276      277      278      279      280      281     282\n1 1320.727 3344.664 4464.824 1332.334 1122.165 5061.751 11731.98 11823.9\n      283      284      285      286      287      288      289     290\n1 3002.79 6483.465 5549.376 3135.612 3166.965 11381.41 2920.565 1677.06\n       291      292      293      294      295      296      297      298\n1 2312.611 5611.098 5509.371 5511.236 5611.098 5611.098 4733.129 1128.534\n       299     300      301      302      303      304      305      306\n1 9007.239 2060.98 7466.163 1114.958 1092.273 1092.273 1122.165 1092.273\n      307      308      309      310      311      312      313      314\n1 4661.76 2205.285 3361.814 2738.453 3026.324 3260.599 3275.482 3275.482\n       315      316      317      318      319     320      321     322     323\n1 3370.976 3370.976 3400.026 3139.566 3275.482 2487.06 597.3575 3294.96 7376.95\n       324      325     326\n1 7338.052 7338.052 7376.95\n\n\nOur output shows the calculations for the first school - and the distances between the school and every fast-food outlet. Because we manually overwrote the values for all one-way streets as well as that we extracted the larges connected graph only, we currently should not have any NA values.\n\n\n\n\n\n\nThe dodgr vignette notes that a distance matrix obtained from running dodgr_distances on graph_connected should generally contain no NA values, although some points may still be effectively unreachable due to one-way connections (or streets). Thus, routing on the largest connected component of a directed graph ought to be expected to yield the minimal number of NA values, which may sometimes be more than zero. Note further that spatial routing points (expressed as from and/or to arguments) will in this case be mapped to the nearest vertices of graph_connected, rather than the potentially closer nearest points of the full graph.\n\n\n\nThe next step of processing all depends on what you are trying to assess. Today we want to understand which schools have a closer proximity to fast-food outlets and which do not, quantified by how many outlets are within walking distance. We will therefore look to count how many outlets are with walking distance from each school and store this as a new column within our ports_school data frame.\n\n\n\nR code\n\n# fastfood outlets within 400m\nports_schools$ff_within_400m &lt;- rowSums(sch_to_ff_calc &lt;= 400)\n\n# fastfood outlets within 800m\nports_schools$ff_within_800m &lt;- rowSums(sch_to_ff_calc &lt;= 800)\n\n# fastfood outlets within 1000m\nports_schools$ff_within_1km &lt;- rowSums(sch_to_ff_calc &lt;= 1000)\n\n\nYou can inspect the ports_schools object to see the results of this analysis.\n\n\n\n\nNow you have calculated the number of fast-food outlets within specific distances from every school in Portsmouth and should get the idea behind a basic accessibility analysis, your task is to estimate the accessibility of fast-food outlets at the LSOA scale and compare this to the 2019 Index of Multiple Deprivation.\n\n\n\n\n\n\nThis skills and steps required for this analysis are not just based on this week’s practical, but you will have to combine all your knowledge of coding and spatial analysis you have gained over the past weeks.\n\n\n\nOne way of doing this, is by taking some of the following steps:\n\nDownload the 2011 LSOA boundaries and extract only those that relate to Portsmouth.\nDownload the 2019 Index of Multiple Deprivation scores.\nDecide on an accessibility measure, such as:\n\nThe average number of fast-food restaurants within x meters of a school within each LSOA.\nThe average distance a fast-food restaurant is from a school within each LSOA.\nThe (average) shortest distance a fast-food restaurant is from a school within each LSOA.\nThe minimum shortest distance a fast-food outlet is from a school within each LSOA.\n\nAggregate accessibility scores to the LSOA level.\nJoin the 2019 Index of Multiple Deprivation data to your LSOA dataset.\nFor each IMD decile, calculate the average for your chosen aggregate measure and produce a table.\n\nUsing your approach what do you think: are fast-food restaurants, on average, more accessible for students at schools that are located within LSOAs with a lower IMD decile (more deprived) when compared to students at schools that are located within LSOAs with a higher IMD decile (less deprived)?\n\n\n\nWe have now conducted some basic accessibility analysis, however, there is some additional fundamental challenges to consider in the context of transport network and accessibility analysis:\n\nHow do the different weight profiles of the dodgr package work? How would one go about creating your own weight profile? How would using a different weight profiles affect the results of your analysis?\nWhy do we have unconnected segments in the extracted transport network? How would you inspect these unconnected segments? Would they need to be connected? If so, how would one do this?\nWhy you think all Origins and Destinations are mapped onto the closest network points? Is this always the best option? What alternative methods could you think of and how would you implement these?\n\n\n\n\n\n\n\nIf you really want to take a deep dive into accessibility analysis, there is a great resource that got published recently: Introduction to urban accessibility: a practical guide in R.\n\n\n\n\n\n\nHaving finished this tutorial on transport network analysis and, hopefully, having been able to independently conduct some further area-profiling using IMD deciles, you have now reached the end of this week’s content and the end of Geocomputation! It is now inevitable: time for that reading list."
  },
  {
    "objectID": "10-network.html#lecture-w10",
    "href": "10-network.html#lecture-w10",
    "title": "1 Transport Network Analysis",
    "section": "",
    "text": "The slides for this week’s lecture can be downloaded here: [Link]"
  },
  {
    "objectID": "10-network.html#reading-w10",
    "href": "10-network.html#reading-w10",
    "title": "1 Transport Network Analysis",
    "section": "",
    "text": "Geurs, K., Van Wee, B. 2004. Accessibility evaluation of land-use and transport strategies: review and research directions. Journal of Transport Geography 12(2): 127-140. [Link]\nHiggins, C., Palm, M. DeJohn, A. et al. 2022. Calculating place-based transit accessibility: Methods, tools and algorithmic dependence. Journal of Transport and Land Use 15(1): 95-116. [Link]\n\n\n\n\n\nSchwanen, T. and De Jong, T. 2008. Exploring the juggling of responsibilities with space-time accessibility analysis. Urban Geography 29(6): 556-580. [Link]\nVan Dijk, J., Krygsman, S. and De Jong, T. 2015. Toward spatial justice: The spatial equity effects of a toll road in Cape Town, South Africa. Journal of Transport and Land Use 8(3): 95-114. [Link]\nVan Dijk, J. and De Jong, T. 2017. Post-processing GPS-tracks in reconstructing travelled routes in a GIS-environment: network subset selection and attribute adjustment. Annals of GIS 23(3): 203-217. [Link]"
  },
  {
    "objectID": "10-network.html#transport-network-analysis-1",
    "href": "10-network.html#transport-network-analysis-1",
    "title": "1 Transport Network Analysis",
    "section": "",
    "text": "The term network analysis covers a wide range of analysis techniques ranging from complex network analysis to social network analysis, and from link analysis to transport network analysis. What the techniques have in common is that they are based on the concept of a network. A network or network graph is constituted by a collection of vertices that are connected to one another by edges. Note, vertices may also be called nodes or points, whilst edges may be called links or lines. Within social network analysis, you may find the terms actors (the vertices) and ties or relations (the edges) also used.\n\n\n\n\n\nFigure 1: Visualising networks with vertices and edges.\n\n\n\n\n\n\nFor this week’s practical, we will be using Portsmouth in the UK as our area of interest for our analysis. One prominent topic within the city is the issue of public health and childhood obesity. According to figures released in March 2020 by Public Health England, more than one in three school pupils are overweight or obese by the time they finish primary school within the city; this is much higher than the national average of one in four. One potential contributor to the health crisis is the ease and availability of fast-food outlets in the city. In the following, we will measure the accessibility of fast-food outlets within specific walking distances of all school in Portsmouth starting at 400m, then 800m and finally a 1km walking distance. We will then aggregate these results to Lower Super Output Areas (LSOA) and overlay these results with some socio-economic variables.\nTo execute this analysis, we will need to first calculate the distances between our schools and fast-food outlets. This involves calculating the shortest distance a child would walk between a school and a fast-food outlet, using roads or streets. We will use the dodgr R package to conduct this transport network analysis.\n\n\n\n\n\n\nAll calculations within the dodgr library currently need to be run in WGS84/4236. This is why we will not transform the CRS of our data in this practical.\n\n\n\n\n\n\nOpen a new script within your GEOG0030 project and save this script as wk10-accessibility-analysis.r. At the top of your script, add the following metadata:\n\n\n\nR code\n\n# Accessibility of fast-food outlets in Portsmouth\n# Date: January 2024\n\n\nNow let us add all of the libraries we will be using today to the top of our script:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(osmdata)\nlibrary(dodgr)\n\n\nTo create our network and Origin-Destination dataset, we will need data on schools, fast-food outlets, and a street network. One data sources that contains probably information on all three topics is OpenStreetMap.\n\n\n\n\n\n\nPlease refer back to Week 6 if you have forgotten about using the Overpass API and key and value pairs.\n\n\n\n\n\n\nTo download our road network dataset, we first define a variable to store our bounding box coordinates, p_bbox(). We then use this within our OSM query to extract specific types of road segments within that bounding box - the results of our query are then stored in an osmdata object. We will select all OSM features with the highway tag that are likely to be used by pedestrians (e.g. not motorways).\n\n\n\nR code\n\n# define our bbox coordinates for Portsmouth\np_bbox &lt;- c(-1.113197, 50.775781, -1.026508, 50.859941)\n\n# pass bounding box coordinates into the OverPassQuery (opq) function only\n# download features that are not classified as motorway\nosmdata &lt;- opq(bbox = p_bbox) |&gt;\n    add_osm_feature(key = \"highway\", value = c(\"primary\", \"secondary\", \"tertiary\",\n        \"residential\", \"path\", \"footway\", \"unclassified\", \"living_street\", \"pedestrian\")) |&gt;\n    osmdata_sf()\n\n\n\n\n\n\n\n\nIn some instances the OSM query will return an error, especially when several people from the same location are executing the exact same query. If this happens, you can just read through the instructions and download a prepared copy of the data that contains all required OSM Portsmouth data instead: [Link].\nYou can load these downloaded data as follows into R:\n\n\n\nR code\n\nload(\"../path/to/file/ports_ff.RData\")\nload(\"../path/to/file/ports_roads_edges.RData\")\nload(\"../path/to/file/ports_schools.RData\")\n\n\nAfter loading your data, you can continue with the analysis in the Measuring Accessiblity section below, starting with the creation of a network graph with the ‘foot weighting’ profile.\n\n\n\nThe osmdata object contains the bounding box of your query, a time-stamp of the query, and then the spatial data as osm_points, osm_lines, osm_multilines and osm_polgyons (which are listed with their respective fields also detailed). Some of the spatial features maybe empty, depending on what you asked your query to return. Our next step therefore is to extract our spatial data from our osmdata object to create our road network data set. This is in fact incredibly easy, using the traditional $ R approach to access these spatial features from our object.\nDeciding what to extract is probably the more complicated aspect of this - mainly as you need to understand how to represent your road network, and this will usually be determined by the library/functions you will be using it within. Today, we want to extract the edges of the network, i.e. the lines that represent the roads, as well as the nodes of the network, i.e. the points that represent the locations at which the roads start, end, or intersect. For our points, we will only keep the osm_id data field, just in case we need to refer to this later. For our lines, we will keep a little more information that we might want to use within our transport network analysis, including the type of road, the maximum speed, and whether the road is one-way or not.\n\n\n\nR code\n\n# extract the points, with their osm_id.\nports_roads_nodes &lt;- osmdata$osm_points[, \"osm_id\"]\n\n# extract the lines, with their osm_id, name, type of highway, max speed and\n# one-way attributes\nports_roads_edges &lt;- osmdata$osm_lines[, c(\"osm_id\", \"name\", \"highway\", \"maxspeed\",\n    \"oneway\")]\n\n\nTo check our data set, we can quickly plot the edges of our road network using the plot() function:\n\n\n\nR code\n\n# inspect\nplot(ports_roads_edges, max.plot = 1)\n\n\n\n\n\nFigure 2: OSM road network\n\n\n\n\nBecause we are focusing on walking, we will overwrite the oneway variable by suggesting that none of the road segments are restricted to one-way traffic which may affect our analysis as well as the general connectivity of the network.\n\n\n\nR code\n\n# overwrite one-way default\nports_roads_edges$oneway &lt;- \"no\"\n\n\nNow we have the network edges, we can turn this into a graph-representation that allows for the calculation of network-based accessibility statistics.\n\n\n\nBefore we can construct our full network graph for the purpose of accessibility analysis, we need to also provide our Origin and Destination points, i.e. the data points we wish to calculate the distances between. According to the dodgr documentation, these points need to be in either a vector or matrix format, containing the two coordinates for each point for the origins and for the destinations.\nAs for our Portsmouth scenario we are interested in calculating the shortest distances between schools and fast-food outlets, we need to try and download these datasets from OpenStreetMap as well. Following a similar structure to our query above, we will use our knowledge of OpenStreetMap keys and values to extract the points of Origins (schools) and Destinations (fast-food outlets) we are interested in:\n\n\n\nR code\n\n# download schools\nschools &lt;- opq(bbox = p_bbox) |&gt;\n    add_osm_feature(key = \"amenity\", value = \"school\") |&gt;\n    osmdata_sf()\n\n# download fast-food outlets\nff_outlets &lt;- opq(bbox = p_bbox) |&gt;\n    add_osm_feature(key = \"amenity\", value = \"fast_food\") |&gt;\n    osmdata_sf()\n\n\nWe also need to then extract the relevant data from the osmdata object:\n\n\n\nR code\n\n# extract school points\nports_schools &lt;- schools$osm_points[, c(\"osm_id\", \"name\")]\n\n# extract fast-food outlet points\nports_ff &lt;- ff_outlets$osm_points[, c(\"osm_id\", \"name\")]\n\n\nWe now have our road network data and our Origin-Destination (OD) points in place and we can now move to construct our network graph and run our transport network analysis.\n\n\n\n\n\n\nIn this analysis, we are highly reliant on the use of OpenStreetMap to provide data for both our Origins and Destinations. Whilst in the UK OSM provides substantial coverage, its quality is not always guaranteed. As a result, to improve on our current methodology in future analysis, we should investigate into a more official school data set or at least validate the number of schools against City Council records. The same applies to our fast-food outlets.\n\n\n\nWith any network analysis, the main data structure is a graph, constructed by our nodes and edges. To create a graph for use within dodgr, we pass our ports_roads_edges() into the weight_streetnet() function. The dodgr library also contains weighting profiles, that you can customise, for use within your network analysis. These weighting profiles contain weights based on the type of road, determined by the type of transportation the profile aims to model. Here we will use the weighting profile foot, as we are looking to model walking accessibility.\n\n\n\nR code\n\n# create network graph with the foot weighting profile\ngraph &lt;- weight_streetnet(ports_roads_edges, wt_profile = \"foot\")\n\n\nOnce we have our graph, we can then use this to calculate our network distances between our OD points. One thing to keep in mind is that potentially not all individual components in the network that we extracted are connected, for instance, because the bounding box cut off the access road of a cul-de-sac. To make sure that our entire extracted network is connected, we now extract the largest connected component of the graph. You can use table(graph$component) to examine the sizes of all individual subgraphs. You will notice that most subgraphs consist of a very small number of edges.\n\n\n\n\n\n\nThe dodgr package documentation explains that components are numbered in order of decreasing size, with $component = 1 always denoting the largest component. Always inspect the resulting subgraph to make sure that its coverage is adequate for analysis.\n\n\n\n\n\n\nR code\n\n# extract the largest connected graph component\ngraph_connected &lt;- graph[graph$component == 1, ]\n\n# inspect number of remaining road segments\nnrow(graph_connected)\n\n\n[1] 57384\n\n# inspect\nplot(dodgr_to_sf(graph_connected), max.plot = 1)\n\n\n\n\nFigure 3: Largest graph component\n\n\n\n\n\n\n\n\n\n\nOpenStreetMap is a living dataset, meaning that changes are made on a continuous basis; as such it may very well possible that the number of remaining road segments as shown above may be slightly different when you run this analysis.\n\n\n\nNow we have our connected subgraph, will can use the dodgr_distances() function to calculate the network distances between every possible Origin and Destination. In the dodgr_distances() function, we first pass our graph, then our Origin points (schools), in the from argument, and then our Destination points (fast-food outlets), in the to argument. One thing to note is our addition of the st_coordinates() function as we pass our two point data sets within the from and to functions as we need to supplement our Origins and Destinations in a matrix format. For all Origins and Destinations, dodgr_distances() will map the points to the closest network points, and return corresponding shortest-path distances.\n\n\n\nR code\n\n# create a distance matrix between schools and fast-food stores\nsch_to_ff_calc &lt;- dodgr_distances(graph_connected, from = st_coordinates(ports_schools),\n    to = st_coordinates(ports_ff), shortest = TRUE, pairwise = FALSE, quiet = FALSE)\n\n\nThe result of this computation is a distance-matrix that contains the network distances between all Origins (i.e. schools) and all Destinations (i.e. fast-food outlets). Let’s inspect the first row of our output. Do you understand what the values mean?\n\n\n\nR code\n\n# inspect\nhead(sch_to_ff_calc, n = 1)\n\n\n         1        2        3        4        5        6        7        8\n1 4000.016 2090.485 6549.779 9130.106 10677.47 2231.919 11642.83 2292.263\n         9       10       11       12       13       14       15       16\n1 1676.914 1680.151 1697.525 1676.914 2090.485 2090.485 2090.485 4000.016\n        17       18       19      20       21       22       23       24\n1 2324.454 4000.016 4000.016 3338.83 1700.179 581.0582 7277.651 3370.976\n        25       26       27       28       29       30       31       32\n1 1359.146 3321.374 340.8823 1102.226 10570.65 2644.278 2720.491 3260.791\n        33       34       35       36       37       38       39       40\n1 2330.386 2580.465 2920.565 2920.565 736.0556 3399.756 844.6083 5996.449\n        41      42       43       44       45       46       47       48\n1 3394.397 8945.04 9146.476 9012.112 9255.798 1629.029 3277.902 2400.918\n        49       50       51       52       53       54       55       56\n1 6937.961 8750.917 9176.882 9103.774 6483.465 6483.465 6470.802 6470.802\n        57       58       59       60       61       62       63       64\n1 804.5849 763.0434 2526.807 1067.907 1010.408 1010.408 1967.697 1418.767\n        65       66      67       68       69       70       71       72\n1 2580.465 1245.823 2157.02 688.8672 2162.922 2205.285 2119.323 11552.65\n        73       74       75       76       77       78       79       80\n1 4603.517 2738.124 844.6083 615.1102 788.7212 2586.476 2667.125 728.4882\n        81       82       83       84       85       86       87       88\n1 11642.83 11642.83 11642.83 11731.98 11731.98 11731.98 11642.83 11642.83\n        89       90       91       92       93       94       95       96\n1 11642.83 11642.83 11642.83 11642.83 11642.83 11113.76 6310.216 6354.465\n        97       98       99      100      101      102      103      104\n1 6354.465 6216.593 4000.016 4000.016 4234.058 5703.495 3841.384 581.0582\n       105      106      107      108      109      110      111      112\n1 597.3575 2654.314 4884.169 5081.596 4823.045 375.2084 926.0127 4823.045\n       113     114      115     116      117      118     119      120      121\n1 3808.049 753.752 556.5173 713.536 622.6949 619.2467 1225.66 3452.874 3363.827\n       122      123      124      125      126      127      128      129\n1 5233.586 6435.029 6399.453 6399.453 6391.258 6399.453 6399.453 6391.258\n       130      131      132      133      134      135      136      137\n1 6435.029 6435.029 6391.258 6435.029 6435.029 6435.029 6435.029 6435.029\n       138      139      140      141      142      143     144      145\n1 12122.56 9302.449 9302.449 5296.254 9113.485 9113.485 9065.06 8337.971\n      146      147     148     149      150     151      152      153      154\n1 9065.06 9113.485 11823.9 11823.9 11831.19 11823.5 9322.319 3349.922 2300.121\n       155      156      157      158      159      160      161      162\n1 815.5434 3219.572 5472.702 905.8238 905.8238 905.8238 4950.036 4968.988\n       163      164      165      166      167      168      169      170\n1 858.9023 3216.085 3159.418 11410.34 4919.699 1813.109 8934.149 1666.407\n       171      172      173      174      175      176      177      178\n1 9373.088 9433.043 4462.093 3177.171 3219.572 3118.594 5747.221 9312.943\n       179      180      181      182      183      184      185      186\n1 9433.043 5462.571 4046.518 8993.699 6190.672 6126.632 8679.466 6672.568\n       187      188      189      190     191      192     193      194\n1 6049.243 10546.09 1505.914 4995.748 11286.5 1574.826 5242.73 1804.091\n       195      196      197      198      199     200     201      202\n1 3084.572 8876.496 4512.959 4968.988 3803.913 12956.7 5242.73 3363.827\n       203      204      205     206      207      208      209      210\n1 3363.827 556.5173 788.7212 9886.96 4866.125 9312.943 9312.943 9312.943\n       211      212      213      214      215      216      217      218\n1 9294.621 9294.621 9294.621 9294.621 9312.943 9312.943 5765.619 5370.568\n      219      220      221      222      223      224      225      226\n1 4492.53 3803.913 4661.927 4913.756 1543.604 1510.665 1559.537 5081.936\n       227      228      229      230      231      232      233      234\n1 11444.06 4719.045 4723.757 4271.398 3277.902 5811.015 5216.759 5081.596\n       235      236      237      238      239      240      241      242\n1 5081.596 5081.596 5081.596 5081.596 5081.596 5081.596 4963.868 6036.662\n      243      244      245      246      247      248      249      250\n1 5140.89 4512.959 5592.362 6399.453 4625.997 11356.79 11381.41 4823.045\n       251      252      253      254      255      256      257      258\n1 4823.045 4823.045 4823.045 4866.125 4895.861 4866.125 4895.861 4895.861\n       259      260      261      262      263      264      265      266\n1 4890.115 11642.83 6549.779 6549.779 6529.533 6549.779 6549.779 6529.533\n       267      268      269      270      271      272      273      274\n1 4582.555 4968.988 11642.83 4548.575 4136.321 1129.672 1302.289 1302.289\n       275      276      277      278      279      280      281     282\n1 1320.727 3344.664 4464.824 1332.334 1122.165 5061.751 11731.98 11823.9\n      283      284      285      286      287      288      289     290\n1 3002.79 6483.465 5549.376 3135.612 3166.965 11381.41 2920.565 1677.06\n       291      292      293      294      295      296      297      298\n1 2312.611 5611.098 5509.371 5511.236 5611.098 5611.098 4733.129 1128.534\n       299     300      301      302      303      304      305      306\n1 9007.239 2060.98 7466.163 1114.958 1092.273 1092.273 1122.165 1092.273\n      307      308      309      310      311      312      313      314\n1 4661.76 2205.285 3361.814 2738.453 3026.324 3260.599 3275.482 3275.482\n       315      316      317      318      319     320      321     322     323\n1 3370.976 3370.976 3400.026 3139.566 3275.482 2487.06 597.3575 3294.96 7376.95\n       324      325     326\n1 7338.052 7338.052 7376.95\n\n\nOur output shows the calculations for the first school - and the distances between the school and every fast-food outlet. Because we manually overwrote the values for all one-way streets as well as that we extracted the larges connected graph only, we currently should not have any NA values.\n\n\n\n\n\n\nThe dodgr vignette notes that a distance matrix obtained from running dodgr_distances on graph_connected should generally contain no NA values, although some points may still be effectively unreachable due to one-way connections (or streets). Thus, routing on the largest connected component of a directed graph ought to be expected to yield the minimal number of NA values, which may sometimes be more than zero. Note further that spatial routing points (expressed as from and/or to arguments) will in this case be mapped to the nearest vertices of graph_connected, rather than the potentially closer nearest points of the full graph.\n\n\n\nThe next step of processing all depends on what you are trying to assess. Today we want to understand which schools have a closer proximity to fast-food outlets and which do not, quantified by how many outlets are within walking distance. We will therefore look to count how many outlets are with walking distance from each school and store this as a new column within our ports_school data frame.\n\n\n\nR code\n\n# fastfood outlets within 400m\nports_schools$ff_within_400m &lt;- rowSums(sch_to_ff_calc &lt;= 400)\n\n# fastfood outlets within 800m\nports_schools$ff_within_800m &lt;- rowSums(sch_to_ff_calc &lt;= 800)\n\n# fastfood outlets within 1000m\nports_schools$ff_within_1km &lt;- rowSums(sch_to_ff_calc &lt;= 1000)\n\n\nYou can inspect the ports_schools object to see the results of this analysis."
  },
  {
    "objectID": "10-network.html#assignment-w10",
    "href": "10-network.html#assignment-w10",
    "title": "1 Transport Network Analysis",
    "section": "",
    "text": "Now you have calculated the number of fast-food outlets within specific distances from every school in Portsmouth and should get the idea behind a basic accessibility analysis, your task is to estimate the accessibility of fast-food outlets at the LSOA scale and compare this to the 2019 Index of Multiple Deprivation.\n\n\n\n\n\n\nThis skills and steps required for this analysis are not just based on this week’s practical, but you will have to combine all your knowledge of coding and spatial analysis you have gained over the past weeks.\n\n\n\nOne way of doing this, is by taking some of the following steps:\n\nDownload the 2011 LSOA boundaries and extract only those that relate to Portsmouth.\nDownload the 2019 Index of Multiple Deprivation scores.\nDecide on an accessibility measure, such as:\n\nThe average number of fast-food restaurants within x meters of a school within each LSOA.\nThe average distance a fast-food restaurant is from a school within each LSOA.\nThe (average) shortest distance a fast-food restaurant is from a school within each LSOA.\nThe minimum shortest distance a fast-food outlet is from a school within each LSOA.\n\nAggregate accessibility scores to the LSOA level.\nJoin the 2019 Index of Multiple Deprivation data to your LSOA dataset.\nFor each IMD decile, calculate the average for your chosen aggregate measure and produce a table.\n\nUsing your approach what do you think: are fast-food restaurants, on average, more accessible for students at schools that are located within LSOAs with a lower IMD decile (more deprived) when compared to students at schools that are located within LSOAs with a higher IMD decile (less deprived)?"
  },
  {
    "objectID": "10-network.html#wm-w10",
    "href": "10-network.html#wm-w10",
    "title": "1 Transport Network Analysis",
    "section": "",
    "text": "We have now conducted some basic accessibility analysis, however, there is some additional fundamental challenges to consider in the context of transport network and accessibility analysis:\n\nHow do the different weight profiles of the dodgr package work? How would one go about creating your own weight profile? How would using a different weight profiles affect the results of your analysis?\nWhy do we have unconnected segments in the extracted transport network? How would you inspect these unconnected segments? Would they need to be connected? If so, how would one do this?\nWhy you think all Origins and Destinations are mapped onto the closest network points? Is this always the best option? What alternative methods could you think of and how would you implement these?\n\n\n\n\n\n\n\nIf you really want to take a deep dive into accessibility analysis, there is a great resource that got published recently: Introduction to urban accessibility: a practical guide in R."
  },
  {
    "objectID": "10-network.html#byl-ntx",
    "href": "10-network.html#byl-ntx",
    "title": "1 Transport Network Analysis",
    "section": "",
    "text": "Having finished this tutorial on transport network analysis and, hopefully, having been able to independently conduct some further area-profiling using IMD deciles, you have now reached the end of this week’s content and the end of Geocomputation! It is now inevitable: time for that reading list."
  },
  {
    "objectID": "04-statistics.html",
    "href": "04-statistics.html",
    "title": "1 Programming for Data Analysis",
    "section": "",
    "text": "This week’s content introduces you to the foundational concepts associated with Programming for Data Analysis. We will cover some general principles of programming as well how we can use R and RStudio effectively for data analysis by continuing to look at crime in London.\n\n\nThe slides for this week’s lecture can be downloaded here: [Link]\n\n\n\n\n\n\nHadley, W. 2017. R for Data Science. Chapter 3: Workflow: basics. [Link]\nHadley, W. 2017. R for Data Science. Chapter 4: Data transformation. [Link]\nHadley, W. 2017. R for Data Science. Chapter 7: Workflow: scripts and projects. [Link]\nLovelace, R., Nowosad, J. and Muenchow, J. 2021. Geocomputation with R, Chapter 1: Introduction. [Link]\n\n\n\n\n\nArribas-Bel, D. et al. 2021. Open data products - A framework for creating valuable analysis ready data. Journal of Geographical Systems 23: 497-514. [Link]\n\n\n\n\n\nProgramming is our most fundamental way of interacting with a computer, it was how computers were first built and operated and for a long time, the Command Line Interface (CLI) was our primary way of using computers before our Graphical User Interface (GUI) Operating Systems (OS) and software became mainstream. Nowadays, the majority of us use our computers through clicking instead of typing. However, programming and computer code underpin every single application that we use on our computers.\n\n\n\n\n\n\nProgramming is used for endless purposes and applications, ranging from software engineering and application development, to creating websites and managing databases at substantial scales. To help with this diversity of applications, multiple types of programming languages have developed. Wikipedia, for example, has a list of hundreds of different languages, although there is some overlap between many of these, some are used for incredibly niche activities, and some are not used any more at all.\n\n\n\n\n\nWe will be using R in this module as the main tool to complete specific tasks we need to do for our data analysis. There are a lot of alternative tools out there that you can use to achieve the same outcomes (as you have seen with QGIS, and no doubt had experience of using some statistics/spreadsheet software) but we choose to use this tool because it provides us with many advantages over these other tools.\nWhat is important to understand is that R and RStudio are two different things:\n\nR is our programming language, which we need to understand in terms of general principles, syntax and structure.\nRStudio is our Integrated Development Environment (IDE), which we need to understand in terms of functionality and workflow. An IDE is simply a software programme that makes creating and organising your code easier.\n\nAs you may know already, R is a free and open-source programming language, that originally was created to focus on statistical analysis. In conjunction with the development of R as a language, the same community created the RStudio IDE to execute this statistical programming. Together, R and RStudio have grown into an incredibly success partnership of analytical programming language and analysis software. As a result, it has a huge and active contributor community which constantly adds functionality to the language and software, making it an incredibly useful tool for many purposes and applications beyond statistical analysis.\nUnlike traditional statistical analysis programmes you may have used such as Microsoft Excel or even ArcGIS Online, within the RStudio IDE, the user has to type commands to get it to execute tasks such as loading in a dataset or performing a calculation. We primarily do this by building up a script, that provides a record of what you have done, whilst also enabling the straightforward repetition of tasks.\nWe can also use the R Console to execute simple instructions that do not need repeating such as installing libraries or quickly viewing data. In addition, R, its various graphic-oriented packages and RStudio are capable of making graphs, charts and maps through just a few lines of code which can then be easily modified and tweaked by making slight changes to the script if mistakes are spotted. Unfortunately, command-line computing can also be off-putting at first. It is easy to make mistakes that are not always obvious to detect and thus debug. Nevertheless, there are good reasons to stick with R and RStudio. These include:\n\nIt is broadly intuitive with a strong focus on publishable-quality graphics.\nIt is ‘intelligent’ and offers in-built good practice; it tends to stick to statistical conventions and present data in sensible ways.\nIt is free, cross-platform, customisable and extendable with a whole swathe of packages (libraries) including those for discrete choice, multilevel and longitudinal regression, mapping, spatial statistics, spatial regression, geostatistics, network analysis, etc.\nIt is well respected and used at the world’s largest technology companies including Google, Microsoft and Facebook.\nIt offers a transferable skill that shows to potential employers experience both of statistics and of computing.\n\n\n\n\n\n\n\nThe intention of the practical elements of this week is to provide a thorough introduction to RStudio to get you started:\n\nThe basic programming principles behind R.\nLoading in data from csv files, filtering and subsetting it into smaller chunks and joining them together.\nCalculating a number of statistics for data exploration and checking.\nCreating basic and more complex plots in order to visualise the distributions values within a dataset.\n\n\n\n\nWhat you should remember is that R has a steep learning curve, but the benefits of using it are well worth the effort. The best way to really learn R is to take the basic code provided in tutorials and experiment with changing parameters such as the colour of points in a graph to really get ‘under the hood’ of the software.\n\n\n\nYou should all have access to some form of R on your personal computer, or through Desktop@UCL Anywhere or the RStudio Server. If not, please refer to the Geocomputation: An Introduction section. Go ahead and open RStudio and we will first take a quick tour of the various components of the RStudio environment interface and how and when to use them. When you first open RStudio, it should look a little something like this:\n\n\n\n\n\nFigure 1: RStudio on RStudio Server. [Enlarge image]\n\n\n\n\nThe main windows (panel/pane) to keep focused on for now are:\n\nConsole: where we write “one-off” code, such as installing libraries/packages, as well as running quick views or plots of our data.\nFiles: where our files are stored on our computer system, also helpful for general file management.\nEnvironment: where our variables are recorded; we can find out a lot about our variables by looking at the environment window, including data structure, data type(s) and the fields and ‘attributes’ of our variables.\nPlots: where the outputs of our graphs, charts and maps are shown\nHelp: where you can search for help, e.g. by typing in a function to find out its parameters.\n\nYou may also have your Script Window open, which is where we build up and write code. This not only helps us to keep a record of our work, but also enables us to repeat and re-run code again. We will not use this window until we get to the final practical instructions.\nWe will see how we use these windows as we progress through this tutorial and understand in more detail what we mean by words such as attributes (do not get confused here with the Attribute Table for QGIS) and data structures.\n\n\n\n\nWe will first start off with using RStudio’s console to test out some of R’s in-built functionality by creating a few variables as well as a dummy dataset that we will be able to analyse.\n\n\n\n\n\n\nYou might need to click on the console window to get it to expand; you can then drag it to take up a larger space in your RStudio window.\n\n\n\nIn your console, let us go ahead and conduct some quick maths. At their most basic, all programming languages can be used like calculators.\n\n\n\n\n\n\nIn your RStudio console, you should see a prompt sign &gt; on the left hand side. This is where we can directly interact with R. Anything that appears as red in the command line means it is an error (or a warning) so you will likely need to correct your code. If you just see a &gt; it means you can type in your next line, whilst a + means that you have not finished the previous line of code. As will become clear, + signs often appear if you do not close brackets or you did not properly finish your command in a way that R expected.\n\n\n\n\n\nType in 10 * 12 into the console.\n\n\n\nR code\n\n# conduct some maths\n10 * 12\n\n\n[1] 120\n\n\nOnce you press return, you should see the answer of 120 returned below.\n\n\n\nRather than use raw or standalone numbers and values, we primarily want to use variables that store these values (or groups of them) under a memorable name for easy reference later. In R terminology this is called creating an object and this object becomes stored as a variable. The &lt;- symbol is used to assign the value to the variable name you have given. Let us create two variables to experiment with.\nType in ten &lt;- 10 into the console and execute.\n\n\n\nR code\n\n# store a variable\nten &lt;- 10\n\n\nYou have just created your first variable. You will see nothing is returned in the console, but if you check your environment window it has now appeared as a new variable that contains the associated value.\nType in twelve &lt;- 12 into the console and execute.\n\n\n\nR code\n\n# store a variable\ntwelve &lt;- 12\n\n\nOnce again, you will see nothing returned to the console but do check your environment window for your variable. We have now stored two numbers into our environment and given them variable names for easy reference. R stores these objects as variables in your computer’s RAM memory so they can be processed quickly. Without saving your environment, these variables would be lost if you close R. Now we have our variables, we can go ahead and execute the same simple multiplication:\nType in ten * twelve into the console and execute.\n\n\n\nR code\n\n# using variables\nten * twelve\n\n\n[1] 120\n\n\nYou should see the output in the console of 120. Whilst this maths may look trivial, it is, in fact, extremely powerful as it shows how these variables can be treated in the same way as the values they contain.\nNext, type in ten * twelve * 8 into the console and execute.\n\n\n\nR code\n\n# using variables and values\nten * twelve * 8\n\n\n[1] 960\n\n\nYou should get an answer of 960. As you can see, we can mix variables with raw values without any problems. We can also store the output of variable calculations as a new variable.\nType output &lt;- ten * twelve * 8 into the console and execute.\n\n\n\nR code\n\n# store output\noutput &lt;- ten * twelve * 8\n\n\nBecause we are storing the output of our maths to a new variable, the answer is not returned to the screen.\n\n\n\nWe can ask our computer to return this output by simply typing it into the console. You should see we get the same value as the earlier equation.\n\n\n\nR code\n\n# return value\noutput\n\n\n[1] 960\n\n\n\n\n\nWe can also store variables of different data types, not just numbers but text as well.\nType in str_variable &lt;- \"This is our 1st string variable\" into the console and execute.\n\n\n\nR code\n\n# store a variable\nstr_variable &lt;- \"This is our 1st string variable\"\n\n\nWe have just stored our sentence made from a combination of characters, including letters and numbers. A variable that stores “words” (that may be sentences, or codes, or file names), is known as a string. A string is always denoted by the use of quotation marks (\"\" or '').\nType in str_variable into the console and execute.\n\n\n\nR code\n\n# return variable\nstr_variable\n\n\n[1] \"This is our 1st string variable\"\n\n\nYou should see our entire sentence returned,enclosed in quotation marks (\"\"). Again, by simply entering our variable into the console, we have asked R to return our variable to us.\n\n\n\nWe can also call a function on our variable. This use of call is a very specific programming term and generally what you use to say ‘use’ a function. What it simply means is that we will use a specific function to do something to our variable. For example, we can also ask R to print our variable, which will give us the same output as accessing it directly via the console.\nType in print(str_variable) into the console and execute.\n\n\n\nR code\n\n# printing a variable\nprint(str_variable)\n\n\n[1] \"This is our 1st string variable\"\n\n\nWe have just used our first function: print(). This function actively finds the variable and then returns this to our screen.\nYou can type ?print into the console to find out more about the print() function.\n\n\n\nR code\n\n# open documentation of the print function\n?print\n\n\nThis can be used with any function to get access to their documentation which is essential to know how to use the function correctly and understand its output.\n\n\n\n\n\n\nIn many cases, a function will take more than one argument or parameter, so it is important to know what you need to provide the function with in order for it to work. For now, we are using functions that only need one required argument although most functions will also have several optional or default parameters.\n\n\n\n\n\n\nWhen a function provides an output, such as this, it is known as returning. Not all functions will return an output to your screen, so often we require a print() statement or another type of returning function to check whether the function was successful or not. More on this later.\n\n\n\nWithin the base R language, there are various functions that have been written to help us examine and find out information about our variables. For example, we can use the typeof() function to check what data type our variable is.\nType in typeof(str_variable) into the console and execute.\n\n\n\nR code\n\n# call the typeof() function\ntypeof(str_variable)\n\n\n[1] \"character\"\n\n\nYou should see the answer: character. As evident, our str_variable is a character data type. We can try testing this out on one of our earlier variables too.\nType in typeof(ten) into the console and execute.\n\n\n\nR code\n\n# call the typeof() function\ntypeof(ten)\n\n\n[1] \"double\"\n\n\nYou should see the answer: double. As evident, our ten is a double data type. For high-level objects that involve (more complicated) data structures, such as when we load a csv into R as a dataframe, we are also able to check what class our object is. Type in class(str_variable) into the console and execute.\n\n\n\nR code\n\n# call the class() function\nclass(str_variable)\n\n\n[1] \"character\"\n\n\nIn this case, you will get the same answer because in R both its class and type are the same: a character. In other programming languages, you might have had string returned instead, but this effectively means the same thing.\nType in class(ten) into the console and execute.\n\n\n\nR code\n\n# call the class() function\nclass(ten)\n\n\n[1] \"numeric\"\n\n\nIn this case, you will get a different answer because the class of this variable is numeric. This is because the class of numeric objects can contain either doubles (decimals) or integers (whole numbers). We can test this by asking whether our ten variable is an integer or not.\nType in is.integer(ten) into the console and execute.\n\n\n\nR code\n\nis.integer(ten)\n\n\n[1] FALSE\n\n\nYou should see we get the answer FALSE: as we know from our earlier typeof() function our variable ten is stored as a double and therefore cannot be an integer.\n\n\n\n\n\n\nWhilst knowing how to distinguish between different data types might not seem important now, the difference of a double versus an integer can quite easily lead to unexpected errors.\n\n\n\nWe can also ask how long our variable is. in this case, we will find out how many different sets of characters (strings) are stored in our variable, str_variable.\nType in length(str_variable) into the console and execute.\n\n\n\nR code\n\n# call the length() function\nlength(str_variable)\n\n\n[1] 1\n\n\nYou should get the answer 1 because we only have one set of characters. We can also ask how long each set of characters is within our variable, i.e. ask how long the string contained by our variable is.\nType in nchar(str_variable) into the console and execute.\n\n\n\nR code\n\n# call the nchar() function\nnchar(str_variable)\n\n\n[1] 31\n\n\nYou should get an answer of 31.\n\n\n\nLet us go ahead and test these two ‘length’ functions a little further by creating a new variable to store two string sets within our object, i.e. our variable will hold two elements.\nType in two_str_variable &lt;- c(\"This is our second variable\", \"It has two parts to it\") into the console and execute.\n\n\n\nR code\n\n# store a new variable\ntwo_str_variable &lt;- c(\"This is our second string variable\", \"It has two parts to it\")\n\n\nIn this piece of code, we have created a new variable using the c() function in R, that stands for combine values into a vector or list. We have provided that function with two sets of strings, using a comma to separate our two strings - all contained within the function’s brackets (()). You should now see a new variable in your environment window which tells us it is a) chr: characters, b) contains two items, and c) lists those items.\nLet us now try both our length() and nchar() on our new variable and see what the results are:\n\n\n\nR code\n\n# call the length() function\nlength(two_str_variable)\n\n\n[1] 2\n\n# call the nchar() function\nnchar(two_str_variable)\n\n[1] 34 22\n\n\nYou should notice that the length() function now returned a 2 and the nchar() function returned two values of 34 and 22.\nThere is one final function that we often want to use with our variables when we are first exploring them, which is attributes(). Because our current variables are very simple, they do not have any attributes but it is a really useful function, which we will come across later on.\n\n\n\nR code\n\n# call the attributes() function\nattributes(two_str_variable)\n\n\nNULL\n\n\n\n\n\n\n\n\nIn addition to make notes about the functions you are coming across in the workshop, you should notice that with each line of code in the examples, an additional comment is used to explain what the code does. Comments are denoted using the hash symbol #. This comments out that particular line so that R ignores it when the code is run. These comments will help you in future when you return to scripts a week or so after writing the code as well as help others understand what is going on when sharing your code. It is good practice to get into writing comments as you code and not leave it to do retrospectively. Whilst we are using the console, using comments is not necessary but as we start to build up a script later on, you will find them essential to help understand your workflow in the future.\n\n\n\n\n\n\n\nThe objects we created and played with above are very simple but the real power of R comes when we can begin to execute functions on more complex objects. R accepts four main types of data structures: vectors, matrices, dataframes, and lists. These data structures are essential because they allow us to apply common statistical functions.\nWe are going to explore these data structures with some of dummy data on the total number of pages and publication dates of the various editions of Geographic Information Systems and Science (GISS) book by Longley et al. and use these for a brief analysis:\n\n\n\nBook Edition\nYear of Publication\nTotal Number of Pages\n\n\n\n\n1st\n2001\n454\n\n\n2nd\n2005\n517\n\n\n3rd\n2011\n560\n\n\n4th\n2015\n477\n\n\n\n\n\nFirst, let us clear up our workspace and remove our current variables. Type rm(ten, twelve, output, str_variable, two_str_variable) into the console and execute.\n\n\n\nR code\n\n# clear our workspace\nrm(ten, twelve, output, str_variable, two_str_variable)\n\n\nYou should now see we no longer have any variables in our window. We just used the rm() function to remove these variables from our environment and free up some RAM. Keeping a clear workspace is another recommendation of good practice moving forward. Of course, we do not want to get rid of any variables we might need to use later but removing any variables we no longer need (such as test variables) will help you understand and manage your code and your working environment.\n\n\n\nThe first complex data object we will create is a vector. A vector is the most common and basic data structure in R. Vectors are a collection of elements that are mostly of either character, logical integer or numeric data types. Technically, vectors can be one of two types:\n\nAtomic vectors (all elements are of the same data type)\nLists (elements can be of different data types)\n\nLet us create our first official vector, detailing the different total page numbers for GISS. Type gisspage_no &lt;- c(454, 517, 560, 477) into the console and execute.\n\n\n\nR code\n\n# store the page numbers as a variable\ngiss_page_no &lt;- c(454, 517, 560, 477)\n\n\nType print(giss_page_no) into the console and execute to check the results.\n\n\n\nR code\n\n# print our giss_page_no variable\nprint(giss_page_no)\n\n\n[1] 454 517 560 477\n\n\nWe can see we have our total number of pages collected together in a single vector. We could if we want, execute some statistical functions on our vector object:\n\n\n\nR code\n\n# calculate the arithmetic mean on our variable\nmean(giss_page_no)\n\n\n[1] 502\n\n# calculate the median on our variable\nmedian(giss_page_no)\n\n[1] 497\n\n# calculate the range numbers of our variable\nrange(giss_page_no)\n\n[1] 454 560\n\n\nWe have now completed our first set of descriptive statistics in R. Let us see how we can build on our vector object by adding in a second vector object that details the relevant years of our book. Note that the total number of pages are entered in a specific order to correspond to these publishing dates (i.e. chronological) and therefore we will need to enter the publication year in the same order.\nType giss_year &lt;- c(2001, 2005, 2011, 2015) into the console and execute.\n\n\n\nR code\n\n# store the publication years as a variable\ngiss_year &lt;- c(2001, 2005, 2011, 2015)\n\n\nType print(giss_year) into the console and execute.\n\n\n\nR code\n\nprint(giss_year)\n\n\n[1] 2001 2005 2011 2015\n\n\nOf course, on their own, the two vectors do not mean much but we can use the same c() function that we used earlier to combine the two together to create a matrix.\n\n\n\nIn R, a matrix is simply an extension of the numeric or character vectors. They are not a separate type of object per se but simply a vector that has two dimensions. That is they contain both rows and columns. As with atomic vectors, the elements of a matrix must be of the same data type. As both our page numbers and our years are numeric, we can add them together to create a matrix using the matrix() function.\nType giss_year_nos &lt;- matrix(c(giss_year, giss_page_no), ncol=2) into the console and execute.\n\n\n\nR code\n\n# create a new matrix from our two vectors with two columns\ngiss_year_nos &lt;- matrix(c(giss_year, giss_page_no), ncol = 2)\n\n\nType print(giss_year_nos) into the console and execute to check the result.\n\n\n\nR code\n\n# inspect\nprint(giss_year_nos)\n\n\n     [,1] [,2]\n[1,] 2001  454\n[2,] 2005  517\n[3,] 2011  560\n[4,] 2015  477\n\n\nThe thing about matrices is that, for us, they do not have a huge amount of use. If we were to look at this matrix in isolation from what we know it represents, we would not really know what to do with it. As a result, we tend to primarily use dataframes in R as they offer the opportunity to add field names to our columns to help with their interpretation.\n\n\n\n\n\n\nThe function we just used above, matrix(), was the first function that we used that took more than one argument. In this case, the arguments the matrix needed to run were:\n\nWhat data or dataset should be stored in the matrix.\nHow many columns (ncol=) do we need to store our data in.\n\nFor any function, there will be mandatory arguments (i.e. it will not run without these) or optional arguments (i.e. it will run without these, as the default to this argument has been set usually to FALSE, 0 or NULL). These are normally documented in the documentation, including details on the format the function expects these arguments to be in.\nUnderstanding how to find out what object and data type a variable is essential therefore to knowing whether it can be used within a function or whether we will need to transform our variable into a different data structure to be used for that specific function.\n\n\n\n\n\n\nA dataframe is an extremely important data type in R. It is pretty much the de-facto data structure for most tabular data and the data structure we use for statistics. It also is the underlying structure to the table data (what we would call the attribute table in Q-GIS) that we associate with spatial data, more on this next week.\nA dataframe is a special type of list where every element of the list will have the same length (i.e. dataframe is a ‘rectangular’ list), Essentially, a dataframe is constructed from columns (which represent a list) and rows (which represents a corresponding element on each list). Each column will have the same amount of entries - even if, for that row, for example, the entry is simply NULL.\ndataframes can have additional attributes such as rownames(), which can be useful for annotating data, like subject_id or sample_id or UID. In statistics, they are often not used but in spatial analysis, these IDs can be essential to join data together. Some additional information on dataframes:\n\nThey are usually created by read.csv() and read.table(), i.e. when importing the data into R.\nYou can also create a new dataframe with data.frame() function, e.g. a matrix can be converted to a dataframe.\nYou can find out the number of rows and columns with nrow() and ncol(), respectively.\nRownames are often automatically generated and look likeX1, X2, … , Xn. Consistency in numbering of rownames may not be honoured when rows are reshuffled or subset.\n\nType giss_df &lt;- data.frame(giss_year_nos) into the console and execute.\n\n\n\nR code\n\n# create a new dataframe from our matrix\ngiss_df &lt;- data.frame(giss_year_nos)\n\n\nWe now have a dataframe, we can use the View() function in R. Still in your console, type: View(giss_df)\n\n\n\nR code\n\n# view our dataframe\nView(giss_df)\n\n\nYou should now see a table pop-up as a new tab on your script window. It is now starting to look like the table we are trying to create, but we need to do something about the fieldnames. X1 and X2 are not very informative.\n\n\n\nWe can rename our dataframe column field names by using the names() function. Before we do this, have a read of what the names() function does. Still in your console, type: ?names\n\n\n\nR code\n\n# open documentation of the names function\n?names\n\n\nAs you can see, the function will get or set the names of an object, with renaming occurring by using the following syntax: names(x) &lt;- value\nThe value itself needs to be a character vector of up to the same length as x, or NULL. We have two columns in our dataframe, so we need to parse our names() function with a character vector with two elements. In the console, we shall enter two lines of code, one after another. First our character vector with our new names, new_names &lt;- c(\"year\", \"page_nos\"), and then the names() function containing this vector for renaming, names(giss_df) &lt;- new_names:\n\n\n\nR code\n\n# create a vector with our new column names\nnew_names &lt;- c(\"year\", \"page_nos\")\n\n# rename our columns with our next names\nnames(giss_df) &lt;- new_names\n\n\nYou can go and check your dataframe again and see the new names using either View() function or by clicking on the tab at the top.\n\n\n\nWe are still missing one final column from our dataframe: our edition of the textbook column. As this is a character data type, we would not have been able to add this directly to our matrix. This is because dataframes can take different data types, unlike matrices - so let us go ahead and add the edition as a new column.\nTo do so, we follow a similar process of creating a vector with our editions listed in chronological order, but then add this to our dataframe by storing this vector as a new column in our dataframe. We use the $ sign with our code that gives us “access” to the dataframe’s column - we then specify the column edition, which whilst it does not exist at the moment, will be created from our code that assigns our edition variable to this column.\nType and execute edition &lt;- c(\"1st\", \"2nd\", \"3rd\", \"4th\"). Then store this vector as a new column in our dataframe under the column name edition by typing and executing giss_df$edition &lt;- edition:\n\n\n\nR code\n\n# create a vector with editions\nedition &lt;- c(\"1st\", \"2nd\", \"3rd\", \"4th\")\n\n# add this vector as a new column to our dataframe\ngiss_df$edition &lt;- edition\n\n\nAgain, you can go and check your dataframe and see the new column using either View() function, by clicking on the tab at the top or by typing giss_df in your console window, or by typing the name of the object into the console:\n\n\n\nR code\n\n# inspect\ngiss_df\n\n\n  year page_nos edition\n1 2001      454     1st\n2 2005      517     2nd\n3 2011      560     3rd\n4 2015      477     4th\n\n\nNow we have our dataframe, let us find out a little about it. We can first return the dimensions (the size) of our dataframe by using the dim() function. In your console, type dim(giss_df) and execute.\n\n\n\nR code\n\n# check our dataframe dimensions\ndim(giss_df)\n\n\n[1] 4 3\n\n\nWe can see we have four rows and three columns. We can also finally use our attributes() function to get the attributes of our dataframe. In your console, type attributes(giss_df) and execute:\n\n\n\nR code\n\n# check our dataframe attributes\nattributes(giss_df)\n\n\n$names\n[1] \"year\"     \"page_nos\" \"edition\" \n\n$row.names\n[1] 1 2 3 4\n\n$class\n[1] \"data.frame\"\n\n\n\n\n\n\n\n\nSome important notes to keep in mind:\n\nR is case-sensitive so you need to make sure that you capitalise everything correctly if required.\nThe spaces between the words do not matter but the positions of the commas and brackets do. Remember, if you find the prompt, &gt;, is replaced with a + it is because the command is incomplete. If necessary, hit the escape (esc) key and try again.\nIt is important to come up with good names for your objects. In the case of the majority of our variables, we used an underscore (_) to separate the words. It is good practice to keep the object names as short as possible but they still need to be easy to read and clear what they are referring to. Be aware: you cannot start an object name with a number!\nIf you press the up arrow in the console you will be able to edit the previous lines of code you have inputted.\n\n\n\n\n\n\n\n\nDuring Week 1’s computer tutorial, we already installed several R libraries. One of these libraries was called the tidyverse. The tidyverse is a collection of packages that are specifically designed for data wrangling, management, cleaning, analysis and visualisation within RStudio. Whilst in many cases different packages work all slightly differently, all packages of the tidyverse share the underlying design philosophy, grammar, and data structures.\nThe tidyverse itself is treated and loaded as a single package, but this means if you load the tidyverse package within your script (through library(tidyverse)), you will directly have access to all the functions that are part of each of the packages that are within the overall tidyverse. This means you do not have to load each package separately.\n\n\n\n\n\n\nFor more information on the tidyverse have a look at www.tidyverse.org.\n\n\n\nThere are some specific functions in the tidyverse suite of packages that will help us cleaning and preparing our datasets now and in the future, which is one of the main reasons for using this library. Some of the most important and useful functions, from the tidyr and dplyr packages, are:\n\n\n\n\n\n\n\n\nPackage\nFunction\nUse to\n\n\n\n\ndplyr\nselect()\nselect columns\n\n\ndplyr\nfilter()\nselect rows\n\n\ndplyr\nmutate()\ntransform or recode variables\n\n\ndplyr\nsummarise()\nsummarise data\n\n\ndplyr\ngroup_by()\ngroup data into subgroups for further processing\n\n\ntidyr\npivot_longer()\nconvert data from wide format to long format\n\n\ntidyr\npivot_wider()\nconvert long format dataset to wide format\n\n\n\nThese functions all complete very fundamental tasks that we need to manipulate and wrangle our data.\n\n\n\n\n\n\nThe code you just ran asked R to load all functions of the tidyverse. However: these functions are only available for the duration of your R sessions. When you restart your R session, you will have to load these functions again if you want to use them. Another thing to be aware of when loading libraries is that sometimes these functions share a name with another function from one of the base R packages. For instance, there is a select() function in the stats package that conducts linear filtering on a time series. However, after we load the tidyverse package and we would type select() this function will select columns from a dataframe. We therefore sometimes need to specify which function exactly we want. This can be done with a simple command (library::function): stats::select to filter on a time series and dplyr::select to select columns in a dataframe.\n\n\n\n\n\nIn the previous section, R may have seemed fairly labour-intensive. We had to enter all our data manually and each line of code had to be written into the command line. Fortunately this is not routinely the case. In RStudio, we can use scripts to build up our code that we can run repeatedly and save for future use. Before we start a new script, we first want to set up ourselves ready for the rest of our practicals by creating a new project.\nTo put it succinctly, projects in RStudio keep all the files associated with a project together: input data, R scripts, analytical results, figures, etc. This means we can easily keep track of all data, input and output, whilst still creating standalone scripts for each bit of processing analysis we do. It also makes dealing with directories and filepaths a whole lot easier; particularly if you have followed the folder structure that was advised at the start of the module.\nClick on File -&gt; New Project -&gt; Existing Directory and browse to your GEOG0030 folder. Click on Create Project. You should now see your main window switch to this new project and if you check your Files window, you should now see a new R Project called GEOG0030.\n\n\n\n\n\n\nPlease ensure that folder names and file names do not contain spaces or special characters such as * . \" / \\ [ ] : ; | = , &lt; ? &gt; & $ # ! ' { } ( ). Different operating systems and programming languages deal differently with spaces and special characters and as such including these in your folder names and file names can cause many problems and unexpected errors. As an alternative to using white space you can use an underscore _ or hyphen - if you like.\n\n\n\n\n\n\nFor the majority of our analysis work, we will type our code within a script and not the console. Let us create our first script. Click on File -&gt; New File -&gt; R Script. This should give you a blank document that looks a bit like the command line. The difference is that anything you type here can be saved as a script and re-run at a later date.\n\n\n\n\n\nFigure 2: Creating a new script in RStudio. [Enlarge image]\n\n\n\n\nSave your script as: wk4-csv-processing.r. Through our name, we know now that our script was created in Week 4 of Geocomputation and the code it will contain is something to do with csv processing. This will help us a lot in the future when we come to find code that we need for other projects.\nThe first bit of code you will want to add to any script is to add a title. This title should give any reader a quick understanding of what your code achieves. When writing a script it is important to keep notes about what each step is doing. To do this, the hash (#) symbol is put before any code. This comments out that particular line so that R ignores it when the script is run.\nLet us go ahead and give our script a title and include some metadata:\n\n\n\nR code\n\n# Analysing theft in London by month\n# Date: January 2024\n\n\nNow we have our title, the second bit of code we want to include in our script is to load our libraries (i.e. the installed packages we want to use in our script):\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   1.0.2 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\n\n\n\n\n\n\nBy loading simply the tidyverse we gain access to several useful functions. However, when developing a script you might realise that you need to load other libraries as well. When you do this, always add your library to the top of your script. If you ever share your script, it helps the person you are sharing with to recognise quickly if they need to install any additional packages prior to running the code. It also means your libraries do not get lost in the multiple lines of code you are writing.\n\n\n\n\n\n\nThere are two main ways to run a script in RStudio: all at once or by line/chunk. It can be advantageous to pursue with the second option as you first start out to build your script as it allows you to test your code interactively.\n\n\n\nBy clicking: select the line or chunk of code you want to run, then click on Code and choose Run selected lines.\nBy key commands: select the line or chunk of code you want to run and then hold Ctl or Cmd and press Return.\n\n\n\n\n\nBy clicking: select Run on the top-right of the scripting window and choose Run All.\nBy key commands: hold Option plus Ctl or Cmd and R.\n\nIf you are running a script that seems for whatever reason to be stuck or you notice some of your code is wrong, you will need to interrupt R. To do so, click on Session -&gt; Interrupt R. If this does not work, you may have to terminate and restart R.\n\n\n\n\nWhere last week we provided you with a crime dataset, this week you will download and prepare the dataset yourselves.\n\nStart by navigating to data.police.uk. And click on Downloads.\nUnder the data range select January 2021 to December 2021.\nUnder the Custom download tab select Metropolitan Police Service and City of London Police. Leave all other settings and click on Generate file.\n\n\n\n\n\n\nFigure 3: Downloading London’s crime data. [Enlarge image]\n\n\n\n\n\nIt may take a few minutes for the download to be generated, so be patient. Once the Download now button appears, you can download the 2021 crime dataset.\nOnce downloaded, unzip the file. You will notice that the zip file contains 12 individual folders, one for each month in 2021. Each folder contains two files: one containing the data for the Metropolitan Police Service and one for the City of London Police.\nCreate a new folder named all-crime in your data/raw/crime directory and copy all 12 folders containing our data to this new folder.\n\n\n\n\n\n\nFigure 4: Your data folder should now look something like this.. [Enlarge image]\n\n\n\n\n\n\nWe are now ready to get started with using the crime data csv's currently sat in our all-crime folder. To do so, we need to first figure out how to import the csv and understand the data structure it will be in after importing. To read in a csv into R requires the use of a very simple function from the tidyverse library: read_csv().\nWe can look at the help documentation to understand what we need to provide the function (or rather the optional arguments), but as we just want to load single csv, we will go ahead and just use the function with a simple parameter.\n\n\n\nR code\n\n# read in a single csv from our crime data\ncrime_csv &lt;- read_csv(\"data/raw/crime/all-crime/2021-01/2021-01-metropolitan-street.csv\")\n\n\nRows: 84848 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (9): Crime ID, Month, Reported by, Falls within, Location, LSOA code, LS...\ndbl (2): Longitude, Latitude\nlgl (1): Context\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n\n\nIf using a Windows machine, you will need to substitute your forward-slashes (/) with two backslashes (\\\\) whenever you are dealing with file paths.\n\n\n\nWe can explore the csv we have just loaded as our new crime_csv variable and understand the class, attributes and dimensions of our variable:\n\n\n\nR code\n\n# inspect class\nclass(crime_csv)\n\n\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \n\n# inspect dimensions\ndim(crime_csv)\n\n[1] 84848    12\n\n\nWe have found out our variable is a dataframe, containing 84,848 rows and 12 columns. We however do not want just the single csv and instead what to combine all our csv's in our all-crime folder into a single dataframe. How do we do this?\nThis will be the most complicated section of code you will come across today, and we will use some functions that you have not seen before. Copy the following code below into your script, then execute.\n\n\n\nR code\n\n# create a list of all csv files in the crime folder\nall_crime_df &lt;- list.files(path = \"data/raw/crime/all-crime/\", full.names = TRUE, recursive = TRUE) |&gt;\n  # apply the read_csv() function on each of these files\n  lapply(read_csv) |&gt;\n  # combine ('bind') them all together into one\n  bind_rows()\n\n\nDepending on your computer, this might take a little time to process because we have a lot of data to get through. You should see a new dataframe appear in your global environment called all_crime_df, for which we now have 1,079,267 observations.\n\n\n\n\n\n\nIt is a little difficult to explain the code above without going into too much detail and at this stage you are not expected to fully understand what is happening here, but essentially what the code does is:\n\nList all the files found in the filepath: data/raw/crime/all-crime/\nRead each of these as a csv by applying the read_csv() function to all files.\nSticking all rows of all individual dataframes together in a single dataframe.\n\nThese three different actions are combined by using something called a pipe (|&gt;), which we will explain in a bit more detail next week.\n\n\n\n\n\n\nWe can now have a look at our large dataframe in more detail.\n\n\n\nR code\n\n# full inspection of the dataframe\nncol(all_crime_df)\n\n\n[1] 12\n\nnrow(all_crime_df)\n\n[1] 1079267\n\nhead(all_crime_df)\n\n# A tibble: 6 × 12\n  Crime …¹ Month Repor…² Falls…³ Longi…⁴ Latit…⁵ Locat…⁶ LSOA …⁷ LSOA …⁸ Crime…⁹\n  &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  \n1 &lt;NA&gt;     2021… City o… City o… -0.0976    51.5 On or … E01000… City o… Anti-s…\n2 &lt;NA&gt;     2021… City o… City o… -0.0986    51.5 On or … E01000… City o… Anti-s…\n3 455f0a5… 2021… City o… City o… -0.0973    51.5 On or … E01000… City o… Other …\n4 19f0605… 2021… City o… City o… -0.0986    51.5 On or … E01000… City o… Other …\n5 c1554ce… 2021… City o… City o… -0.0976    51.5 On or … E01000… City o… Shopli…\n6 fe0819e… 2021… City o… City o… -0.0976    51.5 On or … E01000… City o… Shopli…\n# … with 2 more variables: `Last outcome category` &lt;chr&gt;, Context &lt;lgl&gt;, and\n#   abbreviated variable names ¹​`Crime ID`, ²​`Reported by`, ³​`Falls within`,\n#   ⁴​Longitude, ⁵​Latitude, ⁶​Location, ⁷​`LSOA code`, ⁸​`LSOA name`, ⁹​`Crime type`\n\n\nYou should now see with have the same number of columns as our previous single csv, but with many more rows. You can also see that the head() function provides us with the first five rows of our dataframe. You can conversely use tail() to provide the last five rows.\nFor now in our analysis, we only want to extract the theft crime in our dataframe, so we need to filter our data based on the Crime type column. However, as we can see, we have a space in our field name for Crime type and, in fact, many of the other fields. As we want to avoid having spaces in our field names when coding, we need to rename our fields. Rename the field names, just as we did with our GIS table earlier:\n\n\n\nR code\n\n# create a new vector containing updated no space / no capital field names\nno_space_names &lt;- c(\"crime_id\", \"month\", \"reported_by\", \"falls_within\", \"longitude\",\n    \"latitude\", \"location\", \"lsoa_code\", \"lsoa_name\", \"crime_type\", \"last_outcome_category\",\n    \"context\")\n\n# rename our df field names using these new names\nnames(all_crime_df) &lt;- no_space_names\n\n\nWe now have our dataframe ready for filtering. To do so, we will use the filter() function from the dplyr package:\n\n\n\nR code\n\n# filter all_crime_df to contain only theft\nall_theft_df &lt;- dplyr::filter(all_crime_df, crime_type == \"Theft from the person\")\n\n\n\n\n\n\n\n\nIf two functions from different packages have the same name, R default to use the function from the package that got loaded last. To avoid confusion, however, it can be a good idea at times to specify the library from which a function should be taken. In this case, we can make it clear that we want to use filter() from the dplyr package instead of the default stats library.\n\n\n\nYou should now see the new variable appear in your environment with 38,229 observations. Now save the dataframe as a csv file.\n\n\n\nR code\n\n# save all_crime_df as csv\nwrite_csv(all_theft_df, \"data/raw/crime/crime-theft-2021-london.csv\")\n\n\n\n\n\n\n\n\nRemember that if using a Windows machine, you might need to substitute your forward-slashes (/) with two backslashes (\\\\) whenever you are dealing with file paths!\n\n\n\nWe now want to do some further housekeeping and create on final dataframe that will allow us to analyse crime in London by month. To do so, we want to count how many thefts occur each month in London. Fortunately, dplyr has another function that will do this for us, known simply as count().\nWhen you go ahead and search the documentation to understand the count() function, you will see that there is only one function called count() at the moment, i.e. the one in the dplyr library, so we do not need to use the additional syntax we used above. Let us go ahead and count the number of thefts in London by month:\n\n\n\nR code\n\n# count in the all_theft_df the number of crimes by month\nmonth_theft_df &lt;- count(all_theft_df, month)\n\n\nWe have stored the output of our count() function to a new dataframe: month_theft_df. Go ahead and look at the dataframe to see the output: it is a very simple table containing simply the month and n, i.e. the number of thefts occurring per month. We can and should go ahead and rename this column to help with our interpretation of the dataframe. We will use a quick approach to do this, that uses selection of the precise column to rename only the second column:\n\n\n\nR code\n\n# rename the second column of our new dataframe to crime_totals\nnames(month_theft_df)[2] &lt;- \"crime_totals\"\n\n\nThis selection is made through the [2] element of code added after the names() function we have used earlier. We will look more at selection, slicing and indexing in next week’s tutorial.\n\n\n\n\n\nNow we have prepared our dataset, we can conduct some analysis:\n\nWhat was the average number of crimes per month in London in 2021?\nWhat was the median number of crimes per month in London in 2021?\nWhat were the minimum and maximum values of crime in London in 2021?\nBesides descriptive statistics, it would be really useful to generate a simple chart. Use the documentation of the barplot() function to create the barplot below:\n\n\n\n\n\n\n\n\n\n\n\n\nDo not forget to save your script so you can go back to it at a later time. When you close R and are asked if you want to save your workspace: this is not per se necessary. Saving the workspace will keep any variables generated during your current session saved and available in a future session, but so will re-running your script.\n\n\n\n\n\n\n\n\nThe barplot we have made above is very basic and there are better ways to make nice visualisations. For this we can turn to other R packages that have been developed. In fact, there are many hundreds of packages in R each designed for a specific purpose, some of which you can use to create plots in R. One of those packages is called ggplot2. The ggplot2 package is an implementation of the Grammar of Graphics (Wilkinson 2005): a general scheme for data visualisation that breaks up graphs into semantic components such as scales and layers. ggplot2 can serve as a replacement for the base graphics in R and contains a number of default options that match good visualisation practice. You provide the data, tell ggplot2 how to map variables to aesthetics, what graphical primitives to use, and it takes care of the details. An excellent introduction to ggplot2 can be found in the online, freely available book R for Data Science; written by Hadley Wickham, core developer of ggplot2 and the tidyverse. Have a particularly close look at Chapter 2: Data vizualisation.\n\n\n\n\nWe have managed to take a dataset of over one million records and clean and filter it to create a barplot on theft crime in London. Of course, there is a lot more research and exploratory data analysis that can be done, but this first chart is certainly a step in the right direction. Next week, we will be doing a lot more with our dataset, but hopefully this week has shown you want you can achieve with just a few lines of code. That concludes the tutorial for this week. Back to the reading list you go!"
  },
  {
    "objectID": "04-statistics.html#slides-w04",
    "href": "04-statistics.html#slides-w04",
    "title": "1 Programming for Data Analysis",
    "section": "",
    "text": "The slides for this week’s lecture can be downloaded here: [Link]"
  },
  {
    "objectID": "04-statistics.html#reading-w04",
    "href": "04-statistics.html#reading-w04",
    "title": "1 Programming for Data Analysis",
    "section": "",
    "text": "Hadley, W. 2017. R for Data Science. Chapter 3: Workflow: basics. [Link]\nHadley, W. 2017. R for Data Science. Chapter 4: Data transformation. [Link]\nHadley, W. 2017. R for Data Science. Chapter 7: Workflow: scripts and projects. [Link]\nLovelace, R., Nowosad, J. and Muenchow, J. 2021. Geocomputation with R, Chapter 1: Introduction. [Link]\n\n\n\n\n\nArribas-Bel, D. et al. 2021. Open data products - A framework for creating valuable analysis ready data. Journal of Geographical Systems 23: 497-514. [Link]"
  },
  {
    "objectID": "04-statistics.html#programming-in-r",
    "href": "04-statistics.html#programming-in-r",
    "title": "1 Programming for Data Analysis",
    "section": "",
    "text": "Programming is our most fundamental way of interacting with a computer, it was how computers were first built and operated and for a long time, the Command Line Interface (CLI) was our primary way of using computers before our Graphical User Interface (GUI) Operating Systems (OS) and software became mainstream. Nowadays, the majority of us use our computers through clicking instead of typing. However, programming and computer code underpin every single application that we use on our computers.\n\n\n\n\n\n\nProgramming is used for endless purposes and applications, ranging from software engineering and application development, to creating websites and managing databases at substantial scales. To help with this diversity of applications, multiple types of programming languages have developed. Wikipedia, for example, has a list of hundreds of different languages, although there is some overlap between many of these, some are used for incredibly niche activities, and some are not used any more at all.\n\n\n\n\n\nWe will be using R in this module as the main tool to complete specific tasks we need to do for our data analysis. There are a lot of alternative tools out there that you can use to achieve the same outcomes (as you have seen with QGIS, and no doubt had experience of using some statistics/spreadsheet software) but we choose to use this tool because it provides us with many advantages over these other tools.\nWhat is important to understand is that R and RStudio are two different things:\n\nR is our programming language, which we need to understand in terms of general principles, syntax and structure.\nRStudio is our Integrated Development Environment (IDE), which we need to understand in terms of functionality and workflow. An IDE is simply a software programme that makes creating and organising your code easier.\n\nAs you may know already, R is a free and open-source programming language, that originally was created to focus on statistical analysis. In conjunction with the development of R as a language, the same community created the RStudio IDE to execute this statistical programming. Together, R and RStudio have grown into an incredibly success partnership of analytical programming language and analysis software. As a result, it has a huge and active contributor community which constantly adds functionality to the language and software, making it an incredibly useful tool for many purposes and applications beyond statistical analysis.\nUnlike traditional statistical analysis programmes you may have used such as Microsoft Excel or even ArcGIS Online, within the RStudio IDE, the user has to type commands to get it to execute tasks such as loading in a dataset or performing a calculation. We primarily do this by building up a script, that provides a record of what you have done, whilst also enabling the straightforward repetition of tasks.\nWe can also use the R Console to execute simple instructions that do not need repeating such as installing libraries or quickly viewing data. In addition, R, its various graphic-oriented packages and RStudio are capable of making graphs, charts and maps through just a few lines of code which can then be easily modified and tweaked by making slight changes to the script if mistakes are spotted. Unfortunately, command-line computing can also be off-putting at first. It is easy to make mistakes that are not always obvious to detect and thus debug. Nevertheless, there are good reasons to stick with R and RStudio. These include:\n\nIt is broadly intuitive with a strong focus on publishable-quality graphics.\nIt is ‘intelligent’ and offers in-built good practice; it tends to stick to statistical conventions and present data in sensible ways.\nIt is free, cross-platform, customisable and extendable with a whole swathe of packages (libraries) including those for discrete choice, multilevel and longitudinal regression, mapping, spatial statistics, spatial regression, geostatistics, network analysis, etc.\nIt is well respected and used at the world’s largest technology companies including Google, Microsoft and Facebook.\nIt offers a transferable skill that shows to potential employers experience both of statistics and of computing.\n\n\n\n\n\n\n\nThe intention of the practical elements of this week is to provide a thorough introduction to RStudio to get you started:\n\nThe basic programming principles behind R.\nLoading in data from csv files, filtering and subsetting it into smaller chunks and joining them together.\nCalculating a number of statistics for data exploration and checking.\nCreating basic and more complex plots in order to visualise the distributions values within a dataset.\n\n\n\n\nWhat you should remember is that R has a steep learning curve, but the benefits of using it are well worth the effort. The best way to really learn R is to take the basic code provided in tutorials and experiment with changing parameters such as the colour of points in a graph to really get ‘under the hood’ of the software.\n\n\n\nYou should all have access to some form of R on your personal computer, or through Desktop@UCL Anywhere or the RStudio Server. If not, please refer to the Geocomputation: An Introduction section. Go ahead and open RStudio and we will first take a quick tour of the various components of the RStudio environment interface and how and when to use them. When you first open RStudio, it should look a little something like this:\n\n\n\n\n\nFigure 1: RStudio on RStudio Server. [Enlarge image]\n\n\n\n\nThe main windows (panel/pane) to keep focused on for now are:\n\nConsole: where we write “one-off” code, such as installing libraries/packages, as well as running quick views or plots of our data.\nFiles: where our files are stored on our computer system, also helpful for general file management.\nEnvironment: where our variables are recorded; we can find out a lot about our variables by looking at the environment window, including data structure, data type(s) and the fields and ‘attributes’ of our variables.\nPlots: where the outputs of our graphs, charts and maps are shown\nHelp: where you can search for help, e.g. by typing in a function to find out its parameters.\n\nYou may also have your Script Window open, which is where we build up and write code. This not only helps us to keep a record of our work, but also enables us to repeat and re-run code again. We will not use this window until we get to the final practical instructions.\nWe will see how we use these windows as we progress through this tutorial and understand in more detail what we mean by words such as attributes (do not get confused here with the Attribute Table for QGIS) and data structures."
  },
  {
    "objectID": "04-statistics.html#rstudio-console",
    "href": "04-statistics.html#rstudio-console",
    "title": "1 Programming for Data Analysis",
    "section": "",
    "text": "We will first start off with using RStudio’s console to test out some of R’s in-built functionality by creating a few variables as well as a dummy dataset that we will be able to analyse.\n\n\n\n\n\n\nYou might need to click on the console window to get it to expand; you can then drag it to take up a larger space in your RStudio window.\n\n\n\nIn your console, let us go ahead and conduct some quick maths. At their most basic, all programming languages can be used like calculators.\n\n\n\n\n\n\nIn your RStudio console, you should see a prompt sign &gt; on the left hand side. This is where we can directly interact with R. Anything that appears as red in the command line means it is an error (or a warning) so you will likely need to correct your code. If you just see a &gt; it means you can type in your next line, whilst a + means that you have not finished the previous line of code. As will become clear, + signs often appear if you do not close brackets or you did not properly finish your command in a way that R expected.\n\n\n\n\n\nType in 10 * 12 into the console.\n\n\n\nR code\n\n# conduct some maths\n10 * 12\n\n\n[1] 120\n\n\nOnce you press return, you should see the answer of 120 returned below.\n\n\n\nRather than use raw or standalone numbers and values, we primarily want to use variables that store these values (or groups of them) under a memorable name for easy reference later. In R terminology this is called creating an object and this object becomes stored as a variable. The &lt;- symbol is used to assign the value to the variable name you have given. Let us create two variables to experiment with.\nType in ten &lt;- 10 into the console and execute.\n\n\n\nR code\n\n# store a variable\nten &lt;- 10\n\n\nYou have just created your first variable. You will see nothing is returned in the console, but if you check your environment window it has now appeared as a new variable that contains the associated value.\nType in twelve &lt;- 12 into the console and execute.\n\n\n\nR code\n\n# store a variable\ntwelve &lt;- 12\n\n\nOnce again, you will see nothing returned to the console but do check your environment window for your variable. We have now stored two numbers into our environment and given them variable names for easy reference. R stores these objects as variables in your computer’s RAM memory so they can be processed quickly. Without saving your environment, these variables would be lost if you close R. Now we have our variables, we can go ahead and execute the same simple multiplication:\nType in ten * twelve into the console and execute.\n\n\n\nR code\n\n# using variables\nten * twelve\n\n\n[1] 120\n\n\nYou should see the output in the console of 120. Whilst this maths may look trivial, it is, in fact, extremely powerful as it shows how these variables can be treated in the same way as the values they contain.\nNext, type in ten * twelve * 8 into the console and execute.\n\n\n\nR code\n\n# using variables and values\nten * twelve * 8\n\n\n[1] 960\n\n\nYou should get an answer of 960. As you can see, we can mix variables with raw values without any problems. We can also store the output of variable calculations as a new variable.\nType output &lt;- ten * twelve * 8 into the console and execute.\n\n\n\nR code\n\n# store output\noutput &lt;- ten * twelve * 8\n\n\nBecause we are storing the output of our maths to a new variable, the answer is not returned to the screen.\n\n\n\nWe can ask our computer to return this output by simply typing it into the console. You should see we get the same value as the earlier equation.\n\n\n\nR code\n\n# return value\noutput\n\n\n[1] 960\n\n\n\n\n\nWe can also store variables of different data types, not just numbers but text as well.\nType in str_variable &lt;- \"This is our 1st string variable\" into the console and execute.\n\n\n\nR code\n\n# store a variable\nstr_variable &lt;- \"This is our 1st string variable\"\n\n\nWe have just stored our sentence made from a combination of characters, including letters and numbers. A variable that stores “words” (that may be sentences, or codes, or file names), is known as a string. A string is always denoted by the use of quotation marks (\"\" or '').\nType in str_variable into the console and execute.\n\n\n\nR code\n\n# return variable\nstr_variable\n\n\n[1] \"This is our 1st string variable\"\n\n\nYou should see our entire sentence returned,enclosed in quotation marks (\"\"). Again, by simply entering our variable into the console, we have asked R to return our variable to us.\n\n\n\nWe can also call a function on our variable. This use of call is a very specific programming term and generally what you use to say ‘use’ a function. What it simply means is that we will use a specific function to do something to our variable. For example, we can also ask R to print our variable, which will give us the same output as accessing it directly via the console.\nType in print(str_variable) into the console and execute.\n\n\n\nR code\n\n# printing a variable\nprint(str_variable)\n\n\n[1] \"This is our 1st string variable\"\n\n\nWe have just used our first function: print(). This function actively finds the variable and then returns this to our screen.\nYou can type ?print into the console to find out more about the print() function.\n\n\n\nR code\n\n# open documentation of the print function\n?print\n\n\nThis can be used with any function to get access to their documentation which is essential to know how to use the function correctly and understand its output.\n\n\n\n\n\n\nIn many cases, a function will take more than one argument or parameter, so it is important to know what you need to provide the function with in order for it to work. For now, we are using functions that only need one required argument although most functions will also have several optional or default parameters.\n\n\n\n\n\n\nWhen a function provides an output, such as this, it is known as returning. Not all functions will return an output to your screen, so often we require a print() statement or another type of returning function to check whether the function was successful or not. More on this later.\n\n\n\nWithin the base R language, there are various functions that have been written to help us examine and find out information about our variables. For example, we can use the typeof() function to check what data type our variable is.\nType in typeof(str_variable) into the console and execute.\n\n\n\nR code\n\n# call the typeof() function\ntypeof(str_variable)\n\n\n[1] \"character\"\n\n\nYou should see the answer: character. As evident, our str_variable is a character data type. We can try testing this out on one of our earlier variables too.\nType in typeof(ten) into the console and execute.\n\n\n\nR code\n\n# call the typeof() function\ntypeof(ten)\n\n\n[1] \"double\"\n\n\nYou should see the answer: double. As evident, our ten is a double data type. For high-level objects that involve (more complicated) data structures, such as when we load a csv into R as a dataframe, we are also able to check what class our object is. Type in class(str_variable) into the console and execute.\n\n\n\nR code\n\n# call the class() function\nclass(str_variable)\n\n\n[1] \"character\"\n\n\nIn this case, you will get the same answer because in R both its class and type are the same: a character. In other programming languages, you might have had string returned instead, but this effectively means the same thing.\nType in class(ten) into the console and execute.\n\n\n\nR code\n\n# call the class() function\nclass(ten)\n\n\n[1] \"numeric\"\n\n\nIn this case, you will get a different answer because the class of this variable is numeric. This is because the class of numeric objects can contain either doubles (decimals) or integers (whole numbers). We can test this by asking whether our ten variable is an integer or not.\nType in is.integer(ten) into the console and execute.\n\n\n\nR code\n\nis.integer(ten)\n\n\n[1] FALSE\n\n\nYou should see we get the answer FALSE: as we know from our earlier typeof() function our variable ten is stored as a double and therefore cannot be an integer.\n\n\n\n\n\n\nWhilst knowing how to distinguish between different data types might not seem important now, the difference of a double versus an integer can quite easily lead to unexpected errors.\n\n\n\nWe can also ask how long our variable is. in this case, we will find out how many different sets of characters (strings) are stored in our variable, str_variable.\nType in length(str_variable) into the console and execute.\n\n\n\nR code\n\n# call the length() function\nlength(str_variable)\n\n\n[1] 1\n\n\nYou should get the answer 1 because we only have one set of characters. We can also ask how long each set of characters is within our variable, i.e. ask how long the string contained by our variable is.\nType in nchar(str_variable) into the console and execute.\n\n\n\nR code\n\n# call the nchar() function\nnchar(str_variable)\n\n\n[1] 31\n\n\nYou should get an answer of 31.\n\n\n\nLet us go ahead and test these two ‘length’ functions a little further by creating a new variable to store two string sets within our object, i.e. our variable will hold two elements.\nType in two_str_variable &lt;- c(\"This is our second variable\", \"It has two parts to it\") into the console and execute.\n\n\n\nR code\n\n# store a new variable\ntwo_str_variable &lt;- c(\"This is our second string variable\", \"It has two parts to it\")\n\n\nIn this piece of code, we have created a new variable using the c() function in R, that stands for combine values into a vector or list. We have provided that function with two sets of strings, using a comma to separate our two strings - all contained within the function’s brackets (()). You should now see a new variable in your environment window which tells us it is a) chr: characters, b) contains two items, and c) lists those items.\nLet us now try both our length() and nchar() on our new variable and see what the results are:\n\n\n\nR code\n\n# call the length() function\nlength(two_str_variable)\n\n\n[1] 2\n\n# call the nchar() function\nnchar(two_str_variable)\n\n[1] 34 22\n\n\nYou should notice that the length() function now returned a 2 and the nchar() function returned two values of 34 and 22.\nThere is one final function that we often want to use with our variables when we are first exploring them, which is attributes(). Because our current variables are very simple, they do not have any attributes but it is a really useful function, which we will come across later on.\n\n\n\nR code\n\n# call the attributes() function\nattributes(two_str_variable)\n\n\nNULL\n\n\n\n\n\n\n\n\nIn addition to make notes about the functions you are coming across in the workshop, you should notice that with each line of code in the examples, an additional comment is used to explain what the code does. Comments are denoted using the hash symbol #. This comments out that particular line so that R ignores it when the code is run. These comments will help you in future when you return to scripts a week or so after writing the code as well as help others understand what is going on when sharing your code. It is good practice to get into writing comments as you code and not leave it to do retrospectively. Whilst we are using the console, using comments is not necessary but as we start to build up a script later on, you will find them essential to help understand your workflow in the future."
  },
  {
    "objectID": "04-statistics.html#getting-started",
    "href": "04-statistics.html#getting-started",
    "title": "1 Programming for Data Analysis",
    "section": "",
    "text": "The objects we created and played with above are very simple but the real power of R comes when we can begin to execute functions on more complex objects. R accepts four main types of data structures: vectors, matrices, dataframes, and lists. These data structures are essential because they allow us to apply common statistical functions.\nWe are going to explore these data structures with some of dummy data on the total number of pages and publication dates of the various editions of Geographic Information Systems and Science (GISS) book by Longley et al. and use these for a brief analysis:\n\n\n\nBook Edition\nYear of Publication\nTotal Number of Pages\n\n\n\n\n1st\n2001\n454\n\n\n2nd\n2005\n517\n\n\n3rd\n2011\n560\n\n\n4th\n2015\n477\n\n\n\n\n\nFirst, let us clear up our workspace and remove our current variables. Type rm(ten, twelve, output, str_variable, two_str_variable) into the console and execute.\n\n\n\nR code\n\n# clear our workspace\nrm(ten, twelve, output, str_variable, two_str_variable)\n\n\nYou should now see we no longer have any variables in our window. We just used the rm() function to remove these variables from our environment and free up some RAM. Keeping a clear workspace is another recommendation of good practice moving forward. Of course, we do not want to get rid of any variables we might need to use later but removing any variables we no longer need (such as test variables) will help you understand and manage your code and your working environment.\n\n\n\nThe first complex data object we will create is a vector. A vector is the most common and basic data structure in R. Vectors are a collection of elements that are mostly of either character, logical integer or numeric data types. Technically, vectors can be one of two types:\n\nAtomic vectors (all elements are of the same data type)\nLists (elements can be of different data types)\n\nLet us create our first official vector, detailing the different total page numbers for GISS. Type gisspage_no &lt;- c(454, 517, 560, 477) into the console and execute.\n\n\n\nR code\n\n# store the page numbers as a variable\ngiss_page_no &lt;- c(454, 517, 560, 477)\n\n\nType print(giss_page_no) into the console and execute to check the results.\n\n\n\nR code\n\n# print our giss_page_no variable\nprint(giss_page_no)\n\n\n[1] 454 517 560 477\n\n\nWe can see we have our total number of pages collected together in a single vector. We could if we want, execute some statistical functions on our vector object:\n\n\n\nR code\n\n# calculate the arithmetic mean on our variable\nmean(giss_page_no)\n\n\n[1] 502\n\n# calculate the median on our variable\nmedian(giss_page_no)\n\n[1] 497\n\n# calculate the range numbers of our variable\nrange(giss_page_no)\n\n[1] 454 560\n\n\nWe have now completed our first set of descriptive statistics in R. Let us see how we can build on our vector object by adding in a second vector object that details the relevant years of our book. Note that the total number of pages are entered in a specific order to correspond to these publishing dates (i.e. chronological) and therefore we will need to enter the publication year in the same order.\nType giss_year &lt;- c(2001, 2005, 2011, 2015) into the console and execute.\n\n\n\nR code\n\n# store the publication years as a variable\ngiss_year &lt;- c(2001, 2005, 2011, 2015)\n\n\nType print(giss_year) into the console and execute.\n\n\n\nR code\n\nprint(giss_year)\n\n\n[1] 2001 2005 2011 2015\n\n\nOf course, on their own, the two vectors do not mean much but we can use the same c() function that we used earlier to combine the two together to create a matrix.\n\n\n\nIn R, a matrix is simply an extension of the numeric or character vectors. They are not a separate type of object per se but simply a vector that has two dimensions. That is they contain both rows and columns. As with atomic vectors, the elements of a matrix must be of the same data type. As both our page numbers and our years are numeric, we can add them together to create a matrix using the matrix() function.\nType giss_year_nos &lt;- matrix(c(giss_year, giss_page_no), ncol=2) into the console and execute.\n\n\n\nR code\n\n# create a new matrix from our two vectors with two columns\ngiss_year_nos &lt;- matrix(c(giss_year, giss_page_no), ncol = 2)\n\n\nType print(giss_year_nos) into the console and execute to check the result.\n\n\n\nR code\n\n# inspect\nprint(giss_year_nos)\n\n\n     [,1] [,2]\n[1,] 2001  454\n[2,] 2005  517\n[3,] 2011  560\n[4,] 2015  477\n\n\nThe thing about matrices is that, for us, they do not have a huge amount of use. If we were to look at this matrix in isolation from what we know it represents, we would not really know what to do with it. As a result, we tend to primarily use dataframes in R as they offer the opportunity to add field names to our columns to help with their interpretation.\n\n\n\n\n\n\nThe function we just used above, matrix(), was the first function that we used that took more than one argument. In this case, the arguments the matrix needed to run were:\n\nWhat data or dataset should be stored in the matrix.\nHow many columns (ncol=) do we need to store our data in.\n\nFor any function, there will be mandatory arguments (i.e. it will not run without these) or optional arguments (i.e. it will run without these, as the default to this argument has been set usually to FALSE, 0 or NULL). These are normally documented in the documentation, including details on the format the function expects these arguments to be in.\nUnderstanding how to find out what object and data type a variable is essential therefore to knowing whether it can be used within a function or whether we will need to transform our variable into a different data structure to be used for that specific function.\n\n\n\n\n\n\nA dataframe is an extremely important data type in R. It is pretty much the de-facto data structure for most tabular data and the data structure we use for statistics. It also is the underlying structure to the table data (what we would call the attribute table in Q-GIS) that we associate with spatial data, more on this next week.\nA dataframe is a special type of list where every element of the list will have the same length (i.e. dataframe is a ‘rectangular’ list), Essentially, a dataframe is constructed from columns (which represent a list) and rows (which represents a corresponding element on each list). Each column will have the same amount of entries - even if, for that row, for example, the entry is simply NULL.\ndataframes can have additional attributes such as rownames(), which can be useful for annotating data, like subject_id or sample_id or UID. In statistics, they are often not used but in spatial analysis, these IDs can be essential to join data together. Some additional information on dataframes:\n\nThey are usually created by read.csv() and read.table(), i.e. when importing the data into R.\nYou can also create a new dataframe with data.frame() function, e.g. a matrix can be converted to a dataframe.\nYou can find out the number of rows and columns with nrow() and ncol(), respectively.\nRownames are often automatically generated and look likeX1, X2, … , Xn. Consistency in numbering of rownames may not be honoured when rows are reshuffled or subset.\n\nType giss_df &lt;- data.frame(giss_year_nos) into the console and execute.\n\n\n\nR code\n\n# create a new dataframe from our matrix\ngiss_df &lt;- data.frame(giss_year_nos)\n\n\nWe now have a dataframe, we can use the View() function in R. Still in your console, type: View(giss_df)\n\n\n\nR code\n\n# view our dataframe\nView(giss_df)\n\n\nYou should now see a table pop-up as a new tab on your script window. It is now starting to look like the table we are trying to create, but we need to do something about the fieldnames. X1 and X2 are not very informative.\n\n\n\nWe can rename our dataframe column field names by using the names() function. Before we do this, have a read of what the names() function does. Still in your console, type: ?names\n\n\n\nR code\n\n# open documentation of the names function\n?names\n\n\nAs you can see, the function will get or set the names of an object, with renaming occurring by using the following syntax: names(x) &lt;- value\nThe value itself needs to be a character vector of up to the same length as x, or NULL. We have two columns in our dataframe, so we need to parse our names() function with a character vector with two elements. In the console, we shall enter two lines of code, one after another. First our character vector with our new names, new_names &lt;- c(\"year\", \"page_nos\"), and then the names() function containing this vector for renaming, names(giss_df) &lt;- new_names:\n\n\n\nR code\n\n# create a vector with our new column names\nnew_names &lt;- c(\"year\", \"page_nos\")\n\n# rename our columns with our next names\nnames(giss_df) &lt;- new_names\n\n\nYou can go and check your dataframe again and see the new names using either View() function or by clicking on the tab at the top.\n\n\n\nWe are still missing one final column from our dataframe: our edition of the textbook column. As this is a character data type, we would not have been able to add this directly to our matrix. This is because dataframes can take different data types, unlike matrices - so let us go ahead and add the edition as a new column.\nTo do so, we follow a similar process of creating a vector with our editions listed in chronological order, but then add this to our dataframe by storing this vector as a new column in our dataframe. We use the $ sign with our code that gives us “access” to the dataframe’s column - we then specify the column edition, which whilst it does not exist at the moment, will be created from our code that assigns our edition variable to this column.\nType and execute edition &lt;- c(\"1st\", \"2nd\", \"3rd\", \"4th\"). Then store this vector as a new column in our dataframe under the column name edition by typing and executing giss_df$edition &lt;- edition:\n\n\n\nR code\n\n# create a vector with editions\nedition &lt;- c(\"1st\", \"2nd\", \"3rd\", \"4th\")\n\n# add this vector as a new column to our dataframe\ngiss_df$edition &lt;- edition\n\n\nAgain, you can go and check your dataframe and see the new column using either View() function, by clicking on the tab at the top or by typing giss_df in your console window, or by typing the name of the object into the console:\n\n\n\nR code\n\n# inspect\ngiss_df\n\n\n  year page_nos edition\n1 2001      454     1st\n2 2005      517     2nd\n3 2011      560     3rd\n4 2015      477     4th\n\n\nNow we have our dataframe, let us find out a little about it. We can first return the dimensions (the size) of our dataframe by using the dim() function. In your console, type dim(giss_df) and execute.\n\n\n\nR code\n\n# check our dataframe dimensions\ndim(giss_df)\n\n\n[1] 4 3\n\n\nWe can see we have four rows and three columns. We can also finally use our attributes() function to get the attributes of our dataframe. In your console, type attributes(giss_df) and execute:\n\n\n\nR code\n\n# check our dataframe attributes\nattributes(giss_df)\n\n\n$names\n[1] \"year\"     \"page_nos\" \"edition\" \n\n$row.names\n[1] 1 2 3 4\n\n$class\n[1] \"data.frame\"\n\n\n\n\n\n\n\n\nSome important notes to keep in mind:\n\nR is case-sensitive so you need to make sure that you capitalise everything correctly if required.\nThe spaces between the words do not matter but the positions of the commas and brackets do. Remember, if you find the prompt, &gt;, is replaced with a + it is because the command is incomplete. If necessary, hit the escape (esc) key and try again.\nIt is important to come up with good names for your objects. In the case of the majority of our variables, we used an underscore (_) to separate the words. It is good practice to keep the object names as short as possible but they still need to be easy to read and clear what they are referring to. Be aware: you cannot start an object name with a number!\nIf you press the up arrow in the console you will be able to edit the previous lines of code you have inputted."
  },
  {
    "objectID": "04-statistics.html#crime-in-london-ii",
    "href": "04-statistics.html#crime-in-london-ii",
    "title": "1 Programming for Data Analysis",
    "section": "",
    "text": "During Week 1’s computer tutorial, we already installed several R libraries. One of these libraries was called the tidyverse. The tidyverse is a collection of packages that are specifically designed for data wrangling, management, cleaning, analysis and visualisation within RStudio. Whilst in many cases different packages work all slightly differently, all packages of the tidyverse share the underlying design philosophy, grammar, and data structures.\nThe tidyverse itself is treated and loaded as a single package, but this means if you load the tidyverse package within your script (through library(tidyverse)), you will directly have access to all the functions that are part of each of the packages that are within the overall tidyverse. This means you do not have to load each package separately.\n\n\n\n\n\n\nFor more information on the tidyverse have a look at www.tidyverse.org.\n\n\n\nThere are some specific functions in the tidyverse suite of packages that will help us cleaning and preparing our datasets now and in the future, which is one of the main reasons for using this library. Some of the most important and useful functions, from the tidyr and dplyr packages, are:\n\n\n\n\n\n\n\n\nPackage\nFunction\nUse to\n\n\n\n\ndplyr\nselect()\nselect columns\n\n\ndplyr\nfilter()\nselect rows\n\n\ndplyr\nmutate()\ntransform or recode variables\n\n\ndplyr\nsummarise()\nsummarise data\n\n\ndplyr\ngroup_by()\ngroup data into subgroups for further processing\n\n\ntidyr\npivot_longer()\nconvert data from wide format to long format\n\n\ntidyr\npivot_wider()\nconvert long format dataset to wide format\n\n\n\nThese functions all complete very fundamental tasks that we need to manipulate and wrangle our data.\n\n\n\n\n\n\nThe code you just ran asked R to load all functions of the tidyverse. However: these functions are only available for the duration of your R sessions. When you restart your R session, you will have to load these functions again if you want to use them. Another thing to be aware of when loading libraries is that sometimes these functions share a name with another function from one of the base R packages. For instance, there is a select() function in the stats package that conducts linear filtering on a time series. However, after we load the tidyverse package and we would type select() this function will select columns from a dataframe. We therefore sometimes need to specify which function exactly we want. This can be done with a simple command (library::function): stats::select to filter on a time series and dplyr::select to select columns in a dataframe.\n\n\n\n\n\nIn the previous section, R may have seemed fairly labour-intensive. We had to enter all our data manually and each line of code had to be written into the command line. Fortunately this is not routinely the case. In RStudio, we can use scripts to build up our code that we can run repeatedly and save for future use. Before we start a new script, we first want to set up ourselves ready for the rest of our practicals by creating a new project.\nTo put it succinctly, projects in RStudio keep all the files associated with a project together: input data, R scripts, analytical results, figures, etc. This means we can easily keep track of all data, input and output, whilst still creating standalone scripts for each bit of processing analysis we do. It also makes dealing with directories and filepaths a whole lot easier; particularly if you have followed the folder structure that was advised at the start of the module.\nClick on File -&gt; New Project -&gt; Existing Directory and browse to your GEOG0030 folder. Click on Create Project. You should now see your main window switch to this new project and if you check your Files window, you should now see a new R Project called GEOG0030.\n\n\n\n\n\n\nPlease ensure that folder names and file names do not contain spaces or special characters such as * . \" / \\ [ ] : ; | = , &lt; ? &gt; & $ # ! ' { } ( ). Different operating systems and programming languages deal differently with spaces and special characters and as such including these in your folder names and file names can cause many problems and unexpected errors. As an alternative to using white space you can use an underscore _ or hyphen - if you like.\n\n\n\n\n\n\nFor the majority of our analysis work, we will type our code within a script and not the console. Let us create our first script. Click on File -&gt; New File -&gt; R Script. This should give you a blank document that looks a bit like the command line. The difference is that anything you type here can be saved as a script and re-run at a later date.\n\n\n\n\n\nFigure 2: Creating a new script in RStudio. [Enlarge image]\n\n\n\n\nSave your script as: wk4-csv-processing.r. Through our name, we know now that our script was created in Week 4 of Geocomputation and the code it will contain is something to do with csv processing. This will help us a lot in the future when we come to find code that we need for other projects.\nThe first bit of code you will want to add to any script is to add a title. This title should give any reader a quick understanding of what your code achieves. When writing a script it is important to keep notes about what each step is doing. To do this, the hash (#) symbol is put before any code. This comments out that particular line so that R ignores it when the script is run.\nLet us go ahead and give our script a title and include some metadata:\n\n\n\nR code\n\n# Analysing theft in London by month\n# Date: January 2024\n\n\nNow we have our title, the second bit of code we want to include in our script is to load our libraries (i.e. the installed packages we want to use in our script):\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   1.0.2 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\n\n\n\n\n\n\nBy loading simply the tidyverse we gain access to several useful functions. However, when developing a script you might realise that you need to load other libraries as well. When you do this, always add your library to the top of your script. If you ever share your script, it helps the person you are sharing with to recognise quickly if they need to install any additional packages prior to running the code. It also means your libraries do not get lost in the multiple lines of code you are writing.\n\n\n\n\n\n\nThere are two main ways to run a script in RStudio: all at once or by line/chunk. It can be advantageous to pursue with the second option as you first start out to build your script as it allows you to test your code interactively.\n\n\n\nBy clicking: select the line or chunk of code you want to run, then click on Code and choose Run selected lines.\nBy key commands: select the line or chunk of code you want to run and then hold Ctl or Cmd and press Return.\n\n\n\n\n\nBy clicking: select Run on the top-right of the scripting window and choose Run All.\nBy key commands: hold Option plus Ctl or Cmd and R.\n\nIf you are running a script that seems for whatever reason to be stuck or you notice some of your code is wrong, you will need to interrupt R. To do so, click on Session -&gt; Interrupt R. If this does not work, you may have to terminate and restart R.\n\n\n\n\nWhere last week we provided you with a crime dataset, this week you will download and prepare the dataset yourselves.\n\nStart by navigating to data.police.uk. And click on Downloads.\nUnder the data range select January 2021 to December 2021.\nUnder the Custom download tab select Metropolitan Police Service and City of London Police. Leave all other settings and click on Generate file.\n\n\n\n\n\n\nFigure 3: Downloading London’s crime data. [Enlarge image]\n\n\n\n\n\nIt may take a few minutes for the download to be generated, so be patient. Once the Download now button appears, you can download the 2021 crime dataset.\nOnce downloaded, unzip the file. You will notice that the zip file contains 12 individual folders, one for each month in 2021. Each folder contains two files: one containing the data for the Metropolitan Police Service and one for the City of London Police.\nCreate a new folder named all-crime in your data/raw/crime directory and copy all 12 folders containing our data to this new folder.\n\n\n\n\n\n\nFigure 4: Your data folder should now look something like this.. [Enlarge image]\n\n\n\n\n\n\nWe are now ready to get started with using the crime data csv's currently sat in our all-crime folder. To do so, we need to first figure out how to import the csv and understand the data structure it will be in after importing. To read in a csv into R requires the use of a very simple function from the tidyverse library: read_csv().\nWe can look at the help documentation to understand what we need to provide the function (or rather the optional arguments), but as we just want to load single csv, we will go ahead and just use the function with a simple parameter.\n\n\n\nR code\n\n# read in a single csv from our crime data\ncrime_csv &lt;- read_csv(\"data/raw/crime/all-crime/2021-01/2021-01-metropolitan-street.csv\")\n\n\nRows: 84848 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (9): Crime ID, Month, Reported by, Falls within, Location, LSOA code, LS...\ndbl (2): Longitude, Latitude\nlgl (1): Context\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n\n\nIf using a Windows machine, you will need to substitute your forward-slashes (/) with two backslashes (\\\\) whenever you are dealing with file paths.\n\n\n\nWe can explore the csv we have just loaded as our new crime_csv variable and understand the class, attributes and dimensions of our variable:\n\n\n\nR code\n\n# inspect class\nclass(crime_csv)\n\n\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \n\n# inspect dimensions\ndim(crime_csv)\n\n[1] 84848    12\n\n\nWe have found out our variable is a dataframe, containing 84,848 rows and 12 columns. We however do not want just the single csv and instead what to combine all our csv's in our all-crime folder into a single dataframe. How do we do this?\nThis will be the most complicated section of code you will come across today, and we will use some functions that you have not seen before. Copy the following code below into your script, then execute.\n\n\n\nR code\n\n# create a list of all csv files in the crime folder\nall_crime_df &lt;- list.files(path = \"data/raw/crime/all-crime/\", full.names = TRUE, recursive = TRUE) |&gt;\n  # apply the read_csv() function on each of these files\n  lapply(read_csv) |&gt;\n  # combine ('bind') them all together into one\n  bind_rows()\n\n\nDepending on your computer, this might take a little time to process because we have a lot of data to get through. You should see a new dataframe appear in your global environment called all_crime_df, for which we now have 1,079,267 observations.\n\n\n\n\n\n\nIt is a little difficult to explain the code above without going into too much detail and at this stage you are not expected to fully understand what is happening here, but essentially what the code does is:\n\nList all the files found in the filepath: data/raw/crime/all-crime/\nRead each of these as a csv by applying the read_csv() function to all files.\nSticking all rows of all individual dataframes together in a single dataframe.\n\nThese three different actions are combined by using something called a pipe (|&gt;), which we will explain in a bit more detail next week.\n\n\n\n\n\n\nWe can now have a look at our large dataframe in more detail.\n\n\n\nR code\n\n# full inspection of the dataframe\nncol(all_crime_df)\n\n\n[1] 12\n\nnrow(all_crime_df)\n\n[1] 1079267\n\nhead(all_crime_df)\n\n# A tibble: 6 × 12\n  Crime …¹ Month Repor…² Falls…³ Longi…⁴ Latit…⁵ Locat…⁶ LSOA …⁷ LSOA …⁸ Crime…⁹\n  &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  \n1 &lt;NA&gt;     2021… City o… City o… -0.0976    51.5 On or … E01000… City o… Anti-s…\n2 &lt;NA&gt;     2021… City o… City o… -0.0986    51.5 On or … E01000… City o… Anti-s…\n3 455f0a5… 2021… City o… City o… -0.0973    51.5 On or … E01000… City o… Other …\n4 19f0605… 2021… City o… City o… -0.0986    51.5 On or … E01000… City o… Other …\n5 c1554ce… 2021… City o… City o… -0.0976    51.5 On or … E01000… City o… Shopli…\n6 fe0819e… 2021… City o… City o… -0.0976    51.5 On or … E01000… City o… Shopli…\n# … with 2 more variables: `Last outcome category` &lt;chr&gt;, Context &lt;lgl&gt;, and\n#   abbreviated variable names ¹​`Crime ID`, ²​`Reported by`, ³​`Falls within`,\n#   ⁴​Longitude, ⁵​Latitude, ⁶​Location, ⁷​`LSOA code`, ⁸​`LSOA name`, ⁹​`Crime type`\n\n\nYou should now see with have the same number of columns as our previous single csv, but with many more rows. You can also see that the head() function provides us with the first five rows of our dataframe. You can conversely use tail() to provide the last five rows.\nFor now in our analysis, we only want to extract the theft crime in our dataframe, so we need to filter our data based on the Crime type column. However, as we can see, we have a space in our field name for Crime type and, in fact, many of the other fields. As we want to avoid having spaces in our field names when coding, we need to rename our fields. Rename the field names, just as we did with our GIS table earlier:\n\n\n\nR code\n\n# create a new vector containing updated no space / no capital field names\nno_space_names &lt;- c(\"crime_id\", \"month\", \"reported_by\", \"falls_within\", \"longitude\",\n    \"latitude\", \"location\", \"lsoa_code\", \"lsoa_name\", \"crime_type\", \"last_outcome_category\",\n    \"context\")\n\n# rename our df field names using these new names\nnames(all_crime_df) &lt;- no_space_names\n\n\nWe now have our dataframe ready for filtering. To do so, we will use the filter() function from the dplyr package:\n\n\n\nR code\n\n# filter all_crime_df to contain only theft\nall_theft_df &lt;- dplyr::filter(all_crime_df, crime_type == \"Theft from the person\")\n\n\n\n\n\n\n\n\nIf two functions from different packages have the same name, R default to use the function from the package that got loaded last. To avoid confusion, however, it can be a good idea at times to specify the library from which a function should be taken. In this case, we can make it clear that we want to use filter() from the dplyr package instead of the default stats library.\n\n\n\nYou should now see the new variable appear in your environment with 38,229 observations. Now save the dataframe as a csv file.\n\n\n\nR code\n\n# save all_crime_df as csv\nwrite_csv(all_theft_df, \"data/raw/crime/crime-theft-2021-london.csv\")\n\n\n\n\n\n\n\n\nRemember that if using a Windows machine, you might need to substitute your forward-slashes (/) with two backslashes (\\\\) whenever you are dealing with file paths!\n\n\n\nWe now want to do some further housekeeping and create on final dataframe that will allow us to analyse crime in London by month. To do so, we want to count how many thefts occur each month in London. Fortunately, dplyr has another function that will do this for us, known simply as count().\nWhen you go ahead and search the documentation to understand the count() function, you will see that there is only one function called count() at the moment, i.e. the one in the dplyr library, so we do not need to use the additional syntax we used above. Let us go ahead and count the number of thefts in London by month:\n\n\n\nR code\n\n# count in the all_theft_df the number of crimes by month\nmonth_theft_df &lt;- count(all_theft_df, month)\n\n\nWe have stored the output of our count() function to a new dataframe: month_theft_df. Go ahead and look at the dataframe to see the output: it is a very simple table containing simply the month and n, i.e. the number of thefts occurring per month. We can and should go ahead and rename this column to help with our interpretation of the dataframe. We will use a quick approach to do this, that uses selection of the precise column to rename only the second column:\n\n\n\nR code\n\n# rename the second column of our new dataframe to crime_totals\nnames(month_theft_df)[2] &lt;- \"crime_totals\"\n\n\nThis selection is made through the [2] element of code added after the names() function we have used earlier. We will look more at selection, slicing and indexing in next week’s tutorial."
  },
  {
    "objectID": "04-statistics.html#assignment-w04",
    "href": "04-statistics.html#assignment-w04",
    "title": "1 Programming for Data Analysis",
    "section": "",
    "text": "Now we have prepared our dataset, we can conduct some analysis:\n\nWhat was the average number of crimes per month in London in 2021?\nWhat was the median number of crimes per month in London in 2021?\nWhat were the minimum and maximum values of crime in London in 2021?\nBesides descriptive statistics, it would be really useful to generate a simple chart. Use the documentation of the barplot() function to create the barplot below:\n\n\n\n\n\n\n\n\n\n\n\n\nDo not forget to save your script so you can go back to it at a later time. When you close R and are asked if you want to save your workspace: this is not per se necessary. Saving the workspace will keep any variables generated during your current session saved and available in a future session, but so will re-running your script."
  },
  {
    "objectID": "04-statistics.html#wm-w04",
    "href": "04-statistics.html#wm-w04",
    "title": "1 Programming for Data Analysis",
    "section": "",
    "text": "The barplot we have made above is very basic and there are better ways to make nice visualisations. For this we can turn to other R packages that have been developed. In fact, there are many hundreds of packages in R each designed for a specific purpose, some of which you can use to create plots in R. One of those packages is called ggplot2. The ggplot2 package is an implementation of the Grammar of Graphics (Wilkinson 2005): a general scheme for data visualisation that breaks up graphs into semantic components such as scales and layers. ggplot2 can serve as a replacement for the base graphics in R and contains a number of default options that match good visualisation practice. You provide the data, tell ggplot2 how to map variables to aesthetics, what graphical primitives to use, and it takes care of the details. An excellent introduction to ggplot2 can be found in the online, freely available book R for Data Science; written by Hadley Wickham, core developer of ggplot2 and the tidyverse. Have a particularly close look at Chapter 2: Data vizualisation."
  },
  {
    "objectID": "04-statistics.html#byl-w04",
    "href": "04-statistics.html#byl-w04",
    "title": "1 Programming for Data Analysis",
    "section": "",
    "text": "We have managed to take a dataset of over one million records and clean and filter it to create a barplot on theft crime in London. Of course, there is a lot more research and exploratory data analysis that can be done, but this first chart is certainly a step in the right direction. Next week, we will be doing a lot more with our dataset, but hopefully this week has shown you want you can achieve with just a few lines of code. That concludes the tutorial for this week. Back to the reading list you go!"
  },
  {
    "objectID": "02-GIScience.html",
    "href": "02-GIScience.html",
    "title": "1 GIScience and GIS software",
    "section": "",
    "text": "This week’s lecture introduced you to foundational concepts associated with GIScience and GIS software, with particular emphasis on the representation of spatial data and sample design. Out of all our foundational concepts you will come across in the next four weeks, this is probably the most substantial to get to grips with and has both significant theoretical and practical aspects to its learning. The practical component of the week puts some of these learnings into practice by creating a simple visualisation of London’s population in the Census years of 2011 and 2021.\n\n\nThe slides for this week’s lecture can be downloaded here: [Link]\n\n\n\n\n\n\nLongley, P. et al. 2015. Geographic Information Science & Systems, Chapter 2: The Nature of Geographic Data, pp. 33-54. [Link]\nLongley, P. et al. 2015. Geographic Information Science & Systems, Chapter 3: Representing Geography, pp. 55-76. [Link]\nLongley, P. et al. 2015. Geographic Information Science & Systems, Chapter 7: Geographic Data Modeling, pp. 152-172. [Link]\n\n\n\n\n\nGoodchild, M. and Haining, R. 2005. GIS and spatial data analysis: Converging perspectives. Papers in Regional Science 83(1): 363–385. [Link]\nSchurr, C., Müller, M. and Imhof, N. 2020. Who makes geographical knowledge? The gender of Geography’s gatekeepers. The Professional Geographer 72(3): 317-331. [Link]\nYuan, M. 2001. Representing complex geographic phenomena in GIS. Cartography and Geographic Information Science 28(2): 83-96. [Link]\n\n\n\n\n\nThis first computer practical is mainly concerned with creating Attribute Joins so to create choropleth maps. You will be using different types of joins throughout this practical, this module in general, and probably the rest of your career, so it is incredibly important that you understand how these work.\n\n\n\n\n\n\nThe datasets you will create in this practical will be used in coming week’s practicals, so make sure to follow every step and save your data carefully.\n\n\n\nWhen using spatial data, there is generally a specific workflow that you will need to go through and, believe it or not, the majority of this is not actually focused on analysing your data. One of the most often-quoted GIS-related ‘facts’ is that anyone working with spatial data will spend 80% of their time simply finding, retrieving, managing and processing the data before any analysis can be done.\nOne of the reasons behind this is that the data you often need to use is almost never in the format that you require for analysis. Instead, we need to go and find the raw datasets and create the data layers that we want ourselves.\nA typical spatial analysis workflow typically will look something like this:\n\n\n\n\n\n\n\nStep\nAction\n\n\n\n\nidentify\nThink about the data you need to complete your analysis.\n\n\nfind\nFind the data that matches your requirements.\n\n\ndownload\nStore the data on your computer in the right folder.\n\n\nclean\nAssess whether the data needs pre-processing before moving to QGIS or R.\n\n\nload\nLoad the cleaned data into QGIS, R, or software of choice.\n\n\nprocess\nPrepare the data so that they are ready for analysis (e.g. joining, aggregating).\n\n\nanalyse\nExecute the actual analysis that you set out to do.\n\n\nvisualise\nCreate graphs and maps where appropriate.\n\n\n\nAs you can see, the analysis and visualisation parts only make up the last steps of the workflow and instead, the workflow is very top-heavy with data management.\n\n\n\n\n\n\nVery often in GIS-related modules you will be given pre-processed datasets. We will minimise the usage of pre-processed dataset in this module. Because data management is an essential part of your workflow, we will clean the majority of our data from the get-go. This will help you understand the processes that you will need to go through in the future as you search for and download your own data before loading it into your GIS environment.\n\n\n\n\n\nToday we will investigate how the population in London has changed over time. Understanding population change over time and space is spatial analysis at its most fundamental. We can understand a lot just from where population is growing or decreasing, including thinking through the impacts of these changes on the provision of housing, education, health and transport infrastructure. Today we will look at population in London in the Census years of 2011 and 2021 at the so-called Lower Super Output Area (LSOA) geography level (see @w02-download-population below for some details on Administrative Geographies).\n\n\n\n\n\n\nAdministrative geographies are a way of dividing the country into smaller sub-divisions or areas that correspond with the area of responsibility of local authorities and government bodies. These administrative sub-divisions and their associated geography have several important uses, including assigning electoral constituencies and serving as a mechanism for collecting Census data. These geographies are updated as populations evolve and as a result the boundaries of the administrative geographies are subject to either periodic or occasional change.\n\n\n\n\n\n\nIn the UK, finding authoritative data on population and Administrative Geography boundaries is increasingly straightforward. Over the last decade, the UK government has opened up many of its datasets as part of an Open Data precedent that began in 2010 with the creation of data.gov.uk and the Open Government Licence. For our practical today, we will access data from two authoritative data portals:\n\nFor our administrative boundaries, we will download the spatial data through the Open Geography Portal.\nFor our population data, we will download attribute data directly from the Office of National Statistics (ONS).\n\n\n\n\nBefore we download our data, it is important to establish an organised file system that we will use throughout the module. Create a GEOG0030 folder in your Documents folder on your computer and within your GEOG0030 folder, create the following subfolders:\n\n\n\n\n\n\n\nFolder name\nPurpose\n\n\n\n\nraw\nTo store all your raw data files that have not yet been cleaned and processed.\n\n\ndata\nTo store data files that have been cleaned and processed.\n\n\noutput\nTo store all your final output data files.\n\n\nmaps\nTo store the maps you will create.\n\n\n\n\n\n\nWe will start by downloading the administrative geography boundaries for England and Wales:\n\nNavigate to the Open Geography Portal: [Link]\nIn the main menu go to Boundaries -&gt; Census Boundaries  -&gt; Lower Layer Super Output Areas -&gt; 2011 Boundaries.\nClick on LSOA (Dec 2011) Boundaries Generalised Clipped BGC EW V3.\nClick on Download -&gt; Download GeoPackage.\n\nRepeat the above process to also download the 2021 LSOA boundaries: Lower Layer Super Output Areas (2021) Boundaries EW BGC. Make sure you download the Generalised (20m) version.\n\n\n\n\n\n\nDue to boundary changes, we need to download both the 2011 and 2021 LSOA files. More details on the administrative geographies of the UK can be found on the website of the Office for National Statistics. On a different page they also explain the Census 2021 geographies, which include the LSOAs that we are using.\n\n\n\nNow open your GEOG0030/raw/ folder and create a new folder called boundaries. Rename the two files you just downloaded to LSOA2011.gpkg and LSOA2021.gpkg, respectively, and copy these into your boundaries folder.\n\n\n\n\n\n\nYou may have used spatial data before and noticed that we did not download a collection of files known as a shapefile but a GeoPackage instead. Whilst shapefiles are still being used, and you will most likely come across them at some point in your career if you have not already, the more modern, more portable, and open-source GeoPackage file format should be your spatial data format of choice where possible. Have a look at this article on towardsdatascience.com for an excellent explanation on why one should use GeoPackage files over shapefiles where possible: [Link]\n\n\n\nFor our population data, we will use the 2011 and 2021 Census population counts that are made available by the ONS through their Nomis portal.\nTo get the 2011 Census population counts, you should:\n\nNavigate to the Nomis portal: [Link]\nClick on Query data in the Data Downloads panel.\nClick on Census 2011 -&gt; Key Statistics.\nClick on KS101EW - Usual resident population.\nSelect Geography and set 2011 super output areas - lower layer to All.\nClick on Download data at the left hand side of the screen.\nOnce the data is ready for download, download the file to your computer.\n\nTo get the 2021 Census population counts, you should:\n\nNavigate to the Nomis portal: [Link]\nClick on Query data in the Data Downloads panel.\nClick on Census 2021 -&gt; Topic Summaries.\nClick on TS007A - Age by five-year age bands.\nSelect Geography and set 2021 super output areas - lower layer to All.\nClick on Download data at the left hand side of the screen.\nOnce the data is ready for download, download the file to your computer.\n\nIn your GEOG0030/raw/ folder, now create a new folder called population and copy the two Excel files to the newly created population folder. Rename the files you downloaded to: LSOA2011_population.xlsx and LSOA2021_population.xlsx, respectively.\n\n\n\nWhen you open up any of the LSOA spreadsheets in Excel, you will notice that some additional rows of information are contained at the top of each of these files. This extra information will cause issues in QGIS and we therefore need to do some data cleaning. To reduce the chance of errors in later stages, we will also save the data in csv format.\n\n\n\n\n\n\ncsv stands for comma (or character) separated values. A csv file can be thought of as stripped-down Excel spreadsheet in which every column of data is separated by a comma. If you open a csv file within Excel, however, it will look ‘normal’. The advantage of using csv files is that it (should) make it easier to read data into QGIS and R as less things can go wrong. csv does have its limitations, especially when you are dealing with large datasets. A powerful and modern alternative is the parquet format. For an introduction on how to use this, refer to Chapter 23 in Hadley Wickham’s R for Data Science (Second Edition)\n\n\n\n\n\nOpen the LSOA2011_population.xlsx file in Excel. We have two main columns: one with identifying information that distinguishes each area from one another and one with population counts. In addition, there are some less informative rows at the top of the spreadsheet.\nLooking at this we need to take three steps. First, we need to remove the top rows that we do not need. Second, we need to make sure that we extract all the rows with data that are relevant to Greater London. Third, we need to split the information in the first column and put it into two separate columns. The reason for this is that the column contains unique area codes as well as unique area names. However, to be able to effectively link our population data to our spatial data, the codes are much more reliable and much less likely to cause issues than the area names. Putting area codes and area names into separate columns will help us sorting this out.\n\nOpen a new Excel spreadsheet.\nFrom the LSOA2011_population.xlsx spreadsheet, cut (Edit -&gt; Cut) all cells from columns A to B and rows 9 to 4,651 into this new spreadsheet. Row 4,651 denotes the end of the majority of the Greater London LSOAs.\nNow we need to identify the missing LSOAs that were not kept at the top of the file with the other London LSOAs, probably because these LSOAs got introduced at a later point in time. Unfortunately, we can only do this manually by searching for the name of each of the London Boroughs and subsequently cutting the associated rows of data from the spreadsheet and pasting these into the second spreadsheet.\nGo to Edit -&gt; Find -&gt; Find. Type in City of London. Cut (Edit -&gt; Cut) the two rows of data (City of London 001F, City of London 001G) and paste these at the bottom of the second spreadsheet.\nRepeat this process to find all missing LSOAs by searching the names of all 32 London Boroughs.\n\n\n\n\n\n\n\nYou can make this process a bit less tedious by using keyboard shortcuts. On MacOS you can use: cmd + f to open up the find menu, cmd + x to cut data, and cmd + v to paste data. On Windows you can use: ctrl + f to open up the find menu, ctrl + x to cut data, and ctrl + v to paste date.\n\n\n\n\n\n\n\n\n\nNot all of the 32 London Boroughs have missing LSOAs whilst in other cases the missing LSOAs are not grouped together (e.g. Wandsworth and Southwark) so after cutting the data, try Find again to make sure you did not miss any LSOAs. Once you are done cutting and pasting, you should have 4,836 rows of data in your second spreadsheet.\n\n\n\n\n\n\nLondon Boroughs\n\n\n\n\n\nWestminster\nSutton\n\n\nKensington and Chelsea\nCrodyon\n\n\nHammersmith and Fulham\nBromley\n\n\nWandsworth\nLewisham\n\n\nLambeth\nGreenwich\n\n\nSouthwark\nBexley\n\n\nTower Hamlets\nHavering\n\n\nHackney\nBarking and Dagenham\n\n\nIslington\nRedbridge\n\n\nCamden\nNewham\n\n\nBrent\nWaltham Forest\n\n\nEaling\nHaringey\n\n\nHounslow\nEnfield\n\n\nRichmond upon Thames\nBarnet\n\n\nKingston upon Thames\nHarrow\n\n\nMerton\nHillingdon\n\n\n\n\nOnce you copied all the rows of data, rename the columns in the new spreadsheet to lsoa and pop2011, respectively.\nCut all the data from the pop2011 column that is stored in column B and paste these into column C. You now should have a column without any data sat between the lsoa and pop2011 columns.\nHighlight the entire lsoa column and in the Data menu click on the Text to Columns menu button.\n\n\n\n\n\n\nFigure 1: Highlight the lsoa columns and find the Text to Columns menu option. Note that column B does not contain any data. [Enlarge image]\n\n\n\n\n\n\n\n\n\n\nNote that the Text to Columns function might be elsewhere in the menu depending on your version of Excel and/or your operating system.\n\n\n\n\nIn the Text to Columns menu, select the Delimited radio button and click Next.\nUncheck the checkbox for Tab and put : (colon) into the Other box. Click Finish.\nChange the name of the lsoa column to lsoa11_code and change the name of the now populated column B to lsoa11_name.\n\n\n\n\n\n\nFigure 2: Your spreadsheet should now look something like this. [Enlarge image]\n\n\n\n\n\nOne thing that might cause problems when we try to link the population data to the spatial data is that there are still spaces contained in the lsoa11_code column. This is not directly noticeable as these spaces are white space characters at the end of the LSOA codes without any other characters following it (i.e. trailing spaces). This may seem trivial, but we need this column and we therefore need to remove these. We can do this by highlighting the entire lsoa11_code column and in the Home menu opting for Replace under the Find & Select menu.\n\n\n\n\n\n\nFigure 3: Accessing the Replace option. [Enlarge image]\n\n\n\n\n\nIn the Find what box put in a singular white space, using the spacebar on your keyboard, keep the Replace with box empty, and click on Replace all. You should get a message that 4,835 replacements have been made.\n\nOne further bit of formatting that you must do before saving your data is to format our population field. At the moment, you will see that there are commas separating the thousands within our values. If we leave these commas in our values, QGIS will read them as decimal points, creating decimal values of our population. There are many points at which we could resolve this issue, but the easiest point is now.\n\nTo format the pop2011 column, highlight the entire column and right-click on the C cell. Click on Format Cells and set the cells to Number with 0 decimal places. You should see that the commas are now removed from your population values.\nSave your spreadsheet as a csv file into your data folder as LSOA2011_population.csv.\n\n\n\n\n\n\n\nAfter saving the file, Excel might give you a warning along the lines of Possible data loss. You can safely ignore this message as the only ‘information’ you have lost is markup information (e.g. fonts, colours, items in bold, etc.) or formulas within Excel (e.g. if you used Excel formulas to calculate means, medians, etc.). In some cases you might also have options to choose what type of csv you would want to save the spreadsheet as. If so, opt for something along the lines of CSV UTF-8 (Comma-delimited) (.csv).\n\n\n\n\n\n\n\n\n\nDepending on the language settings of your operating system (e.g. Windows, MacOS, Linux) and language settings, csv files might use a different character instead of a comma. This may seem trivial, but it can cause issues when reading the data into a different programme. It is therefore always a good idea to check your csv file in a plain text editor (e.g. Textedit on MacOS or Notepad on Windows). In case you do not see commas (,), but semicolons (;) you can apply a quick and dirty fix to your data by finding and replacing every ; with a , in the same that way we fixed our problematic white space characters in Excel.\n\n\n\n\n\n\nNow the 2011 data is prepared, we can move on to the 2021 data:\n\nOpen the LSOA2021_population.xlsx in Excel. You will notice that the file is formatted largely the same as the LSOA2011_population.xlsx file, although if you were to look closely you will notice that the majority of London data are not at the top this time but that Local Authority Districts are grouped together - and in fact all data pertaining to Greater London are also grouped together. This makes our lives much easier because we can now simply cut the data for each of the 32 Boroughs and City of London in one go.\nOpen a new Excel spreadsheet.\nFrom the LSOA2021_population.xlsx spreadsheet, cut (Edit -&gt; Cut) all cells from columns A to B and rows 19,790 to 2,4783 and paste these into this new spreadsheet.\nTake the remaining steps to prepare the 2021 population steps: split the lsoa column, remove the trailing white space characters from the LSOA code column, and remove the decimal commas in the population count column.\nSave the file as csv into your data folder as LSOA2021_population.csv with the following column names: lsoa21_code, lsoa21_name, and pop2021.\n\n\n\n\n\n\n\nAgain make sure you did not miss any LSOAs. You should end up with 4,995 rows of data in your second spreadsheet.\n\n\n\n\n\n\n\n\n\nWe will now use QGIS to create population maps for the LSOAs in London for both years. To achieve this, we need to join our table data to our spatial datasets and then map our populations for our visual analysis.\n\n\n\n\n\n\nDue to the differences in the LSOA boundaries between 2011 and 2021, we can only make a visual comparison between the two years. You will notice that data interoperability is a key issue that you will face in spatial analysis, particularly when it comes to working with ever-changing administrative geographies.\n\n\n\n\nStart QGIS.\nClick on Project -&gt; New. Save your project as w2-pop-analysis. Remember to save your work throughout the practical.\nBefore we get started with adding data, we will first set the Coordinate Reference System of our Project. Click on Project -&gt; Properties – CRS. In the Filter box, type British National Grid. Select OSGB 1936 / British National Grid - EPSG:27700 and click Apply. Click OK.\n\n\n\n\n\n\n\nWe will explain CRSs and using CRSs in GIS software in more detail next week. For now all you need to know is that the CRS makes sure that the data we plot on the map are plotted in the correct location.\n\n\n\n\n\n\nNow are project is set up, we can start by loading our 2011 spatial layer.\n\nClick on Layer -&gt; Add Layer -&gt; Add Vector Layer.\nWith File as your source type, click on the small three dots button and navigate to your 2011 boundary files.\nHere, we will select the LSOA2011.gpkg dataset. Highlight the file and click Open. Then click Add. You may need to close the box after adding the layer. The 2011 LSOA geography for England and Wales should now be loaded.\n\n\n\n\n\n\nFigure 4: 2011 LSOAs for England and Wales. [Enlarge image]\n\n\n\n\n\n\n\nWe are now going to join our 2011 population data to our 2011 spatial data file. We start by adding the 2011 population data to our project.\n\nClick on Layer -&gt; Add Layer -&gt; Add Delimited Text Layer.\nClick on the three dots button again and navigate to your 2011 population data in your data folder. Your file format should be set to csv. You should have the following boxes ticked under the Record and Field options menu: Decimal separator is comma, First record has field names, Detect field types and Discard empty fields. QGIS does many of these by default, but do double-check.\nSet the Geometry to No geometry (attribute only table) under the Geometry Definition menu. Then click Add and Close. You should now see a table added to your Layers pane.\n\nWe can now join this table data to our spatial data using an Attribute Join.\n\n\n\n\n\n\nAn Attribute Join is one of two types of data joins you will use in spatial analysis (the other is a Spatial join, which we will look at later on in the module). An attribute join essentially allows you to link two datasets together, as long as they share a common attribute to facilitate the ‘matching’ of rows:\n\n\n\n\n\nFigure 5: Attribute Joins. [Enlarge image]\n\n\n\n\nEssentially you need a single unique identifying (UID) field for your records within both datasets: this can be a code, a name or any other string of information. In spatial analysis, we always join our table data to our spatial data. One way to think about it as attaching the table data to the spatial data layer.\nTo make a join work, you need to make sure your ID field is correct across both datasets, i.e. no typos or spelling mistakes. Computers can only follow instructions, so they do not know that St. Thomas in one dataset is the same as St Thomas in another.\nAs a result, whilst in our datasets we have kept both the name and code for both the boundary data and the population data, when creating the join, we will always prefer to use the code over their names. Unlike names, codes reduce the likelihood of error and mismatch because they do not rely on understanding spelling. Common errors, such as adding in spaces or using 0 instead O (and vice versa) can still happen but it is less likely.\n\n\n\nTo make our join work, we need to check that we have a matching UID across both our datasets. We therefore need to look at the tables of both datasets and check what attributes we have that could be used for this possible match.\n\nOpen up the Attribute Tables of each layer and check what fields we have that could be used for the join. We can see that both our respective ‘code’ fields have the same codes (LSOA11CD and lsoa11_code) which seem to contain the same type of information.\nRight-click on your LSOA2011 spatial layer, click on Properties and then click on the Joins tab.\n\nClick on the + button. Make sure the Join Layer is set to LSOA2011_population.\nSet the Join field to lsoa11_code.\nSet the Target field to LSOA11CD.\nClick the Joined Fields box and click to only select the pop2011 field.\nClick on the Custom Field Name Prefix and remove the pre-entered text to leave it blank.\nClick on OK.\nClick on Apply in the main Join tab and then click OK to return to the main QGIS window.\n\n\nWe can now check to see if our join has worked by opening up the Attribute Table of our LSOA11CD spatial layer and looking to see if our LSOAs now have a Population field attached to it.\n\nSort the data in the Attribute Table on the pop2011 field by clicking on the column name: all LSOAs for which we have population data (i.e. only the London LSOAs) are now grouped at the top of the file. Select the first 4,835 rows of data.\n\n\n\n\n\n\n\nYou can select the first row, scroll down to the last row that has population data, hold down the shift button on your keyboard, and click on the last row that has population data to select all rows that we need in one go.\n\n\n\n\n\n\n\n\nFigure 6: All 4,835 London LSOAs selected. [Enlarge image]\n\n\n\n\n\nClose the Attribute Table. Right-click on the LSOA2011 layer and select Export -&gt; Save Selected Features As. Set Format to GeoPackage and click on the small three dots button next to File name and navigate to your raw/boundaries folder. Save the file as LSOA2011_London.gpkg.\nYou can now untick the LSOA2011 containing all the LSOAs for England and Wales. Right-click on the LSOA2011_London layer and click on Zoom to Layer(s).\n\n\n\n\n\n\nFigure 7: All 4,835 2011 London LSOAs. [Enlarge image]\n\n\n\n\n\n\n\n\n\n\nThe main strength of a GUI GIS system is that is really helps us understand how spatial data relate to one another and are visualised. Even with just two spatial layers loaded, we can understand two key concepts of using spatial data within GIS. The first, and this is only really relevant to GUI GIS systems, is that each layer can either be turned on or off, to make it visible or not. The second concept is the order in which your layers are drawn – and this is relevant for both GUI GIS and when using plotting libraries such as ggplot2 or tmap in RStudio. Your layers will be drawn depending on the order in which your layers are either tabled (as in a GUI GIS) or ‘called’ in your function in code. Being aware of this need for ‘order’ is important when we shift to using RStudio and tmap to plot our maps, as if you do not layer your data correctly in your code, your map will end up not looking as you hoped. For us using QGIS right now, the layers will be drawn from bottom to top.\n\n\n\nWe can now finally map the population distribution of London in 2011.\n\nRight-click on the LSOA2011_London layer and click on Properties -&gt; Symbology.\n\nIn the dropdown menu at the top of the window, select Graduated as symbology.\nUnder Value choose pop2011 as your column.\nWe can then change the color ramp to suit our aesthetic preferences. In the Colour ramp dropdown menu select Magma.\nThe final thing we need to do is classify our data - what this simply means is to decide how to group the values in our dataset together to create the graduated representation. We will be looking at classification options in later weeks, but for now, we will use the Natural Breaks option. Open the drop-down menu next to Mode, select Natural Breaks, change it to 7 classes and then click Classify.\nFinally click Apply to style your dataset.\n\n\n\n\n\n\n\n\nUnderstanding what classification is appropriate to visualise your data is an important step within spatial analysis and visualisation, and something you will learn more about in the following weeks. Overall, they should be determined by looking at the distribution of the underlying data.\n\n\n\nYou should now be looking at something like this:\n\n\n\n\n\nFigure 8: The population distribution of London in 2011. [Enlarge image]\n\n\n\n\n\n\n\n\n\n\nWhereas the above map is fine for today, it is technically incorrect because we are showing absolute numbers on a choropleth. This is something we should never do, unless the spatial units are identical in size (e.g. a hexagonal tessellation of an area), because larger areas will draw attention and affect the visualisation.\n\n\n\n\n\n\n\nTo export your map select only the map layers you want to export and then opt for Project -&gt; Import/Export -&gt; Export to Image and save your final map in your maps folder. Next week, we will look at how to style our maps using the main map conventions (adding North Arrows, Scale Bars and Legends, etc.) but for now a simple output will do.\n\n\n\n\nYou now need to repeat the entire process to also create a map for the 2021 Census data. Remember, you need to:\n\nLoad the respective spatial layer.\nLoad the respective population dataset.\nJoin the two datasets together using an Attribute Join.\nExport your joined dataset (London only) as a GeoPackage and save this into your raw/boundaries folder as LSOA2021_London.gpkg. We will use this file in later weeks.\nStyle your data appropriately.\nExport your maps as an image to your output folder.\n\n\n\n\n\n\n\nTo make visual comparisons against our two datasets, theoretically we would need to standardise the breaks at which our classification schemes are set at. To set all two datasets to the same breaks, you can do the following:\n\nRight-click again on the LSOA2011_London dataset and, click on Styles -&gt; Copy Styles -&gt; Symbology.\nNow right-click on the LSOA2021_London file, and click on Styles -&gt; Paste Style -&gt; Symbology. You should now see the classification breaks in the 2021 dataset change to match those in the 2011 data.\nThe final thing you need to do is to change the classification column in the Symbology tab for the 2021 dataset to say pop2021 instead of pop2011.\n\n\n\n\n\n\n\nSave your project so you can go back to it if you need to, other than that that is it for this week. Probably time to start looking at this week’s reading list!"
  },
  {
    "objectID": "02-GIScience.html#slides-w01",
    "href": "02-GIScience.html#slides-w01",
    "title": "1 GIScience and GIS software",
    "section": "",
    "text": "The slides for this week’s lecture can be downloaded here: [Link]"
  },
  {
    "objectID": "02-GIScience.html#reading-w02",
    "href": "02-GIScience.html#reading-w02",
    "title": "1 GIScience and GIS software",
    "section": "",
    "text": "Longley, P. et al. 2015. Geographic Information Science & Systems, Chapter 2: The Nature of Geographic Data, pp. 33-54. [Link]\nLongley, P. et al. 2015. Geographic Information Science & Systems, Chapter 3: Representing Geography, pp. 55-76. [Link]\nLongley, P. et al. 2015. Geographic Information Science & Systems, Chapter 7: Geographic Data Modeling, pp. 152-172. [Link]\n\n\n\n\n\nGoodchild, M. and Haining, R. 2005. GIS and spatial data analysis: Converging perspectives. Papers in Regional Science 83(1): 363–385. [Link]\nSchurr, C., Müller, M. and Imhof, N. 2020. Who makes geographical knowledge? The gender of Geography’s gatekeepers. The Professional Geographer 72(3): 317-331. [Link]\nYuan, M. 2001. Representing complex geographic phenomena in GIS. Cartography and Geographic Information Science 28(2): 83-96. [Link]"
  },
  {
    "objectID": "02-GIScience.html#population-change-in-london",
    "href": "02-GIScience.html#population-change-in-london",
    "title": "1 GIScience and GIS software",
    "section": "",
    "text": "This first computer practical is mainly concerned with creating Attribute Joins so to create choropleth maps. You will be using different types of joins throughout this practical, this module in general, and probably the rest of your career, so it is incredibly important that you understand how these work.\n\n\n\n\n\n\nThe datasets you will create in this practical will be used in coming week’s practicals, so make sure to follow every step and save your data carefully.\n\n\n\nWhen using spatial data, there is generally a specific workflow that you will need to go through and, believe it or not, the majority of this is not actually focused on analysing your data. One of the most often-quoted GIS-related ‘facts’ is that anyone working with spatial data will spend 80% of their time simply finding, retrieving, managing and processing the data before any analysis can be done.\nOne of the reasons behind this is that the data you often need to use is almost never in the format that you require for analysis. Instead, we need to go and find the raw datasets and create the data layers that we want ourselves.\nA typical spatial analysis workflow typically will look something like this:\n\n\n\n\n\n\n\nStep\nAction\n\n\n\n\nidentify\nThink about the data you need to complete your analysis.\n\n\nfind\nFind the data that matches your requirements.\n\n\ndownload\nStore the data on your computer in the right folder.\n\n\nclean\nAssess whether the data needs pre-processing before moving to QGIS or R.\n\n\nload\nLoad the cleaned data into QGIS, R, or software of choice.\n\n\nprocess\nPrepare the data so that they are ready for analysis (e.g. joining, aggregating).\n\n\nanalyse\nExecute the actual analysis that you set out to do.\n\n\nvisualise\nCreate graphs and maps where appropriate.\n\n\n\nAs you can see, the analysis and visualisation parts only make up the last steps of the workflow and instead, the workflow is very top-heavy with data management.\n\n\n\n\n\n\nVery often in GIS-related modules you will be given pre-processed datasets. We will minimise the usage of pre-processed dataset in this module. Because data management is an essential part of your workflow, we will clean the majority of our data from the get-go. This will help you understand the processes that you will need to go through in the future as you search for and download your own data before loading it into your GIS environment.\n\n\n\n\n\nToday we will investigate how the population in London has changed over time. Understanding population change over time and space is spatial analysis at its most fundamental. We can understand a lot just from where population is growing or decreasing, including thinking through the impacts of these changes on the provision of housing, education, health and transport infrastructure. Today we will look at population in London in the Census years of 2011 and 2021 at the so-called Lower Super Output Area (LSOA) geography level (see @w02-download-population below for some details on Administrative Geographies).\n\n\n\n\n\n\nAdministrative geographies are a way of dividing the country into smaller sub-divisions or areas that correspond with the area of responsibility of local authorities and government bodies. These administrative sub-divisions and their associated geography have several important uses, including assigning electoral constituencies and serving as a mechanism for collecting Census data. These geographies are updated as populations evolve and as a result the boundaries of the administrative geographies are subject to either periodic or occasional change.\n\n\n\n\n\n\nIn the UK, finding authoritative data on population and Administrative Geography boundaries is increasingly straightforward. Over the last decade, the UK government has opened up many of its datasets as part of an Open Data precedent that began in 2010 with the creation of data.gov.uk and the Open Government Licence. For our practical today, we will access data from two authoritative data portals:\n\nFor our administrative boundaries, we will download the spatial data through the Open Geography Portal.\nFor our population data, we will download attribute data directly from the Office of National Statistics (ONS).\n\n\n\n\nBefore we download our data, it is important to establish an organised file system that we will use throughout the module. Create a GEOG0030 folder in your Documents folder on your computer and within your GEOG0030 folder, create the following subfolders:\n\n\n\n\n\n\n\nFolder name\nPurpose\n\n\n\n\nraw\nTo store all your raw data files that have not yet been cleaned and processed.\n\n\ndata\nTo store data files that have been cleaned and processed.\n\n\noutput\nTo store all your final output data files.\n\n\nmaps\nTo store the maps you will create.\n\n\n\n\n\n\nWe will start by downloading the administrative geography boundaries for England and Wales:\n\nNavigate to the Open Geography Portal: [Link]\nIn the main menu go to Boundaries -&gt; Census Boundaries  -&gt; Lower Layer Super Output Areas -&gt; 2011 Boundaries.\nClick on LSOA (Dec 2011) Boundaries Generalised Clipped BGC EW V3.\nClick on Download -&gt; Download GeoPackage.\n\nRepeat the above process to also download the 2021 LSOA boundaries: Lower Layer Super Output Areas (2021) Boundaries EW BGC. Make sure you download the Generalised (20m) version.\n\n\n\n\n\n\nDue to boundary changes, we need to download both the 2011 and 2021 LSOA files. More details on the administrative geographies of the UK can be found on the website of the Office for National Statistics. On a different page they also explain the Census 2021 geographies, which include the LSOAs that we are using.\n\n\n\nNow open your GEOG0030/raw/ folder and create a new folder called boundaries. Rename the two files you just downloaded to LSOA2011.gpkg and LSOA2021.gpkg, respectively, and copy these into your boundaries folder.\n\n\n\n\n\n\nYou may have used spatial data before and noticed that we did not download a collection of files known as a shapefile but a GeoPackage instead. Whilst shapefiles are still being used, and you will most likely come across them at some point in your career if you have not already, the more modern, more portable, and open-source GeoPackage file format should be your spatial data format of choice where possible. Have a look at this article on towardsdatascience.com for an excellent explanation on why one should use GeoPackage files over shapefiles where possible: [Link]\n\n\n\nFor our population data, we will use the 2011 and 2021 Census population counts that are made available by the ONS through their Nomis portal.\nTo get the 2011 Census population counts, you should:\n\nNavigate to the Nomis portal: [Link]\nClick on Query data in the Data Downloads panel.\nClick on Census 2011 -&gt; Key Statistics.\nClick on KS101EW - Usual resident population.\nSelect Geography and set 2011 super output areas - lower layer to All.\nClick on Download data at the left hand side of the screen.\nOnce the data is ready for download, download the file to your computer.\n\nTo get the 2021 Census population counts, you should:\n\nNavigate to the Nomis portal: [Link]\nClick on Query data in the Data Downloads panel.\nClick on Census 2021 -&gt; Topic Summaries.\nClick on TS007A - Age by five-year age bands.\nSelect Geography and set 2021 super output areas - lower layer to All.\nClick on Download data at the left hand side of the screen.\nOnce the data is ready for download, download the file to your computer.\n\nIn your GEOG0030/raw/ folder, now create a new folder called population and copy the two Excel files to the newly created population folder. Rename the files you downloaded to: LSOA2011_population.xlsx and LSOA2021_population.xlsx, respectively.\n\n\n\nWhen you open up any of the LSOA spreadsheets in Excel, you will notice that some additional rows of information are contained at the top of each of these files. This extra information will cause issues in QGIS and we therefore need to do some data cleaning. To reduce the chance of errors in later stages, we will also save the data in csv format.\n\n\n\n\n\n\ncsv stands for comma (or character) separated values. A csv file can be thought of as stripped-down Excel spreadsheet in which every column of data is separated by a comma. If you open a csv file within Excel, however, it will look ‘normal’. The advantage of using csv files is that it (should) make it easier to read data into QGIS and R as less things can go wrong. csv does have its limitations, especially when you are dealing with large datasets. A powerful and modern alternative is the parquet format. For an introduction on how to use this, refer to Chapter 23 in Hadley Wickham’s R for Data Science (Second Edition)\n\n\n\n\n\nOpen the LSOA2011_population.xlsx file in Excel. We have two main columns: one with identifying information that distinguishes each area from one another and one with population counts. In addition, there are some less informative rows at the top of the spreadsheet.\nLooking at this we need to take three steps. First, we need to remove the top rows that we do not need. Second, we need to make sure that we extract all the rows with data that are relevant to Greater London. Third, we need to split the information in the first column and put it into two separate columns. The reason for this is that the column contains unique area codes as well as unique area names. However, to be able to effectively link our population data to our spatial data, the codes are much more reliable and much less likely to cause issues than the area names. Putting area codes and area names into separate columns will help us sorting this out.\n\nOpen a new Excel spreadsheet.\nFrom the LSOA2011_population.xlsx spreadsheet, cut (Edit -&gt; Cut) all cells from columns A to B and rows 9 to 4,651 into this new spreadsheet. Row 4,651 denotes the end of the majority of the Greater London LSOAs.\nNow we need to identify the missing LSOAs that were not kept at the top of the file with the other London LSOAs, probably because these LSOAs got introduced at a later point in time. Unfortunately, we can only do this manually by searching for the name of each of the London Boroughs and subsequently cutting the associated rows of data from the spreadsheet and pasting these into the second spreadsheet.\nGo to Edit -&gt; Find -&gt; Find. Type in City of London. Cut (Edit -&gt; Cut) the two rows of data (City of London 001F, City of London 001G) and paste these at the bottom of the second spreadsheet.\nRepeat this process to find all missing LSOAs by searching the names of all 32 London Boroughs.\n\n\n\n\n\n\n\nYou can make this process a bit less tedious by using keyboard shortcuts. On MacOS you can use: cmd + f to open up the find menu, cmd + x to cut data, and cmd + v to paste data. On Windows you can use: ctrl + f to open up the find menu, ctrl + x to cut data, and ctrl + v to paste date.\n\n\n\n\n\n\n\n\n\nNot all of the 32 London Boroughs have missing LSOAs whilst in other cases the missing LSOAs are not grouped together (e.g. Wandsworth and Southwark) so after cutting the data, try Find again to make sure you did not miss any LSOAs. Once you are done cutting and pasting, you should have 4,836 rows of data in your second spreadsheet.\n\n\n\n\n\n\nLondon Boroughs\n\n\n\n\n\nWestminster\nSutton\n\n\nKensington and Chelsea\nCrodyon\n\n\nHammersmith and Fulham\nBromley\n\n\nWandsworth\nLewisham\n\n\nLambeth\nGreenwich\n\n\nSouthwark\nBexley\n\n\nTower Hamlets\nHavering\n\n\nHackney\nBarking and Dagenham\n\n\nIslington\nRedbridge\n\n\nCamden\nNewham\n\n\nBrent\nWaltham Forest\n\n\nEaling\nHaringey\n\n\nHounslow\nEnfield\n\n\nRichmond upon Thames\nBarnet\n\n\nKingston upon Thames\nHarrow\n\n\nMerton\nHillingdon\n\n\n\n\nOnce you copied all the rows of data, rename the columns in the new spreadsheet to lsoa and pop2011, respectively.\nCut all the data from the pop2011 column that is stored in column B and paste these into column C. You now should have a column without any data sat between the lsoa and pop2011 columns.\nHighlight the entire lsoa column and in the Data menu click on the Text to Columns menu button.\n\n\n\n\n\n\nFigure 1: Highlight the lsoa columns and find the Text to Columns menu option. Note that column B does not contain any data. [Enlarge image]\n\n\n\n\n\n\n\n\n\n\nNote that the Text to Columns function might be elsewhere in the menu depending on your version of Excel and/or your operating system.\n\n\n\n\nIn the Text to Columns menu, select the Delimited radio button and click Next.\nUncheck the checkbox for Tab and put : (colon) into the Other box. Click Finish.\nChange the name of the lsoa column to lsoa11_code and change the name of the now populated column B to lsoa11_name.\n\n\n\n\n\n\nFigure 2: Your spreadsheet should now look something like this. [Enlarge image]\n\n\n\n\n\nOne thing that might cause problems when we try to link the population data to the spatial data is that there are still spaces contained in the lsoa11_code column. This is not directly noticeable as these spaces are white space characters at the end of the LSOA codes without any other characters following it (i.e. trailing spaces). This may seem trivial, but we need this column and we therefore need to remove these. We can do this by highlighting the entire lsoa11_code column and in the Home menu opting for Replace under the Find & Select menu.\n\n\n\n\n\n\nFigure 3: Accessing the Replace option. [Enlarge image]\n\n\n\n\n\nIn the Find what box put in a singular white space, using the spacebar on your keyboard, keep the Replace with box empty, and click on Replace all. You should get a message that 4,835 replacements have been made.\n\nOne further bit of formatting that you must do before saving your data is to format our population field. At the moment, you will see that there are commas separating the thousands within our values. If we leave these commas in our values, QGIS will read them as decimal points, creating decimal values of our population. There are many points at which we could resolve this issue, but the easiest point is now.\n\nTo format the pop2011 column, highlight the entire column and right-click on the C cell. Click on Format Cells and set the cells to Number with 0 decimal places. You should see that the commas are now removed from your population values.\nSave your spreadsheet as a csv file into your data folder as LSOA2011_population.csv.\n\n\n\n\n\n\n\nAfter saving the file, Excel might give you a warning along the lines of Possible data loss. You can safely ignore this message as the only ‘information’ you have lost is markup information (e.g. fonts, colours, items in bold, etc.) or formulas within Excel (e.g. if you used Excel formulas to calculate means, medians, etc.). In some cases you might also have options to choose what type of csv you would want to save the spreadsheet as. If so, opt for something along the lines of CSV UTF-8 (Comma-delimited) (.csv).\n\n\n\n\n\n\n\n\n\nDepending on the language settings of your operating system (e.g. Windows, MacOS, Linux) and language settings, csv files might use a different character instead of a comma. This may seem trivial, but it can cause issues when reading the data into a different programme. It is therefore always a good idea to check your csv file in a plain text editor (e.g. Textedit on MacOS or Notepad on Windows). In case you do not see commas (,), but semicolons (;) you can apply a quick and dirty fix to your data by finding and replacing every ; with a , in the same that way we fixed our problematic white space characters in Excel.\n\n\n\n\n\n\nNow the 2011 data is prepared, we can move on to the 2021 data:\n\nOpen the LSOA2021_population.xlsx in Excel. You will notice that the file is formatted largely the same as the LSOA2011_population.xlsx file, although if you were to look closely you will notice that the majority of London data are not at the top this time but that Local Authority Districts are grouped together - and in fact all data pertaining to Greater London are also grouped together. This makes our lives much easier because we can now simply cut the data for each of the 32 Boroughs and City of London in one go.\nOpen a new Excel spreadsheet.\nFrom the LSOA2021_population.xlsx spreadsheet, cut (Edit -&gt; Cut) all cells from columns A to B and rows 19,790 to 2,4783 and paste these into this new spreadsheet.\nTake the remaining steps to prepare the 2021 population steps: split the lsoa column, remove the trailing white space characters from the LSOA code column, and remove the decimal commas in the population count column.\nSave the file as csv into your data folder as LSOA2021_population.csv with the following column names: lsoa21_code, lsoa21_name, and pop2021.\n\n\n\n\n\n\n\nAgain make sure you did not miss any LSOAs. You should end up with 4,995 rows of data in your second spreadsheet.\n\n\n\n\n\n\n\n\n\nWe will now use QGIS to create population maps for the LSOAs in London for both years. To achieve this, we need to join our table data to our spatial datasets and then map our populations for our visual analysis.\n\n\n\n\n\n\nDue to the differences in the LSOA boundaries between 2011 and 2021, we can only make a visual comparison between the two years. You will notice that data interoperability is a key issue that you will face in spatial analysis, particularly when it comes to working with ever-changing administrative geographies.\n\n\n\n\nStart QGIS.\nClick on Project -&gt; New. Save your project as w2-pop-analysis. Remember to save your work throughout the practical.\nBefore we get started with adding data, we will first set the Coordinate Reference System of our Project. Click on Project -&gt; Properties – CRS. In the Filter box, type British National Grid. Select OSGB 1936 / British National Grid - EPSG:27700 and click Apply. Click OK.\n\n\n\n\n\n\n\nWe will explain CRSs and using CRSs in GIS software in more detail next week. For now all you need to know is that the CRS makes sure that the data we plot on the map are plotted in the correct location.\n\n\n\n\n\n\nNow are project is set up, we can start by loading our 2011 spatial layer.\n\nClick on Layer -&gt; Add Layer -&gt; Add Vector Layer.\nWith File as your source type, click on the small three dots button and navigate to your 2011 boundary files.\nHere, we will select the LSOA2011.gpkg dataset. Highlight the file and click Open. Then click Add. You may need to close the box after adding the layer. The 2011 LSOA geography for England and Wales should now be loaded.\n\n\n\n\n\n\nFigure 4: 2011 LSOAs for England and Wales. [Enlarge image]\n\n\n\n\n\n\n\nWe are now going to join our 2011 population data to our 2011 spatial data file. We start by adding the 2011 population data to our project.\n\nClick on Layer -&gt; Add Layer -&gt; Add Delimited Text Layer.\nClick on the three dots button again and navigate to your 2011 population data in your data folder. Your file format should be set to csv. You should have the following boxes ticked under the Record and Field options menu: Decimal separator is comma, First record has field names, Detect field types and Discard empty fields. QGIS does many of these by default, but do double-check.\nSet the Geometry to No geometry (attribute only table) under the Geometry Definition menu. Then click Add and Close. You should now see a table added to your Layers pane.\n\nWe can now join this table data to our spatial data using an Attribute Join.\n\n\n\n\n\n\nAn Attribute Join is one of two types of data joins you will use in spatial analysis (the other is a Spatial join, which we will look at later on in the module). An attribute join essentially allows you to link two datasets together, as long as they share a common attribute to facilitate the ‘matching’ of rows:\n\n\n\n\n\nFigure 5: Attribute Joins. [Enlarge image]\n\n\n\n\nEssentially you need a single unique identifying (UID) field for your records within both datasets: this can be a code, a name or any other string of information. In spatial analysis, we always join our table data to our spatial data. One way to think about it as attaching the table data to the spatial data layer.\nTo make a join work, you need to make sure your ID field is correct across both datasets, i.e. no typos or spelling mistakes. Computers can only follow instructions, so they do not know that St. Thomas in one dataset is the same as St Thomas in another.\nAs a result, whilst in our datasets we have kept both the name and code for both the boundary data and the population data, when creating the join, we will always prefer to use the code over their names. Unlike names, codes reduce the likelihood of error and mismatch because they do not rely on understanding spelling. Common errors, such as adding in spaces or using 0 instead O (and vice versa) can still happen but it is less likely.\n\n\n\nTo make our join work, we need to check that we have a matching UID across both our datasets. We therefore need to look at the tables of both datasets and check what attributes we have that could be used for this possible match.\n\nOpen up the Attribute Tables of each layer and check what fields we have that could be used for the join. We can see that both our respective ‘code’ fields have the same codes (LSOA11CD and lsoa11_code) which seem to contain the same type of information.\nRight-click on your LSOA2011 spatial layer, click on Properties and then click on the Joins tab.\n\nClick on the + button. Make sure the Join Layer is set to LSOA2011_population.\nSet the Join field to lsoa11_code.\nSet the Target field to LSOA11CD.\nClick the Joined Fields box and click to only select the pop2011 field.\nClick on the Custom Field Name Prefix and remove the pre-entered text to leave it blank.\nClick on OK.\nClick on Apply in the main Join tab and then click OK to return to the main QGIS window.\n\n\nWe can now check to see if our join has worked by opening up the Attribute Table of our LSOA11CD spatial layer and looking to see if our LSOAs now have a Population field attached to it.\n\nSort the data in the Attribute Table on the pop2011 field by clicking on the column name: all LSOAs for which we have population data (i.e. only the London LSOAs) are now grouped at the top of the file. Select the first 4,835 rows of data.\n\n\n\n\n\n\n\nYou can select the first row, scroll down to the last row that has population data, hold down the shift button on your keyboard, and click on the last row that has population data to select all rows that we need in one go.\n\n\n\n\n\n\n\n\nFigure 6: All 4,835 London LSOAs selected. [Enlarge image]\n\n\n\n\n\nClose the Attribute Table. Right-click on the LSOA2011 layer and select Export -&gt; Save Selected Features As. Set Format to GeoPackage and click on the small three dots button next to File name and navigate to your raw/boundaries folder. Save the file as LSOA2011_London.gpkg.\nYou can now untick the LSOA2011 containing all the LSOAs for England and Wales. Right-click on the LSOA2011_London layer and click on Zoom to Layer(s).\n\n\n\n\n\n\nFigure 7: All 4,835 2011 London LSOAs. [Enlarge image]\n\n\n\n\n\n\n\n\n\n\nThe main strength of a GUI GIS system is that is really helps us understand how spatial data relate to one another and are visualised. Even with just two spatial layers loaded, we can understand two key concepts of using spatial data within GIS. The first, and this is only really relevant to GUI GIS systems, is that each layer can either be turned on or off, to make it visible or not. The second concept is the order in which your layers are drawn – and this is relevant for both GUI GIS and when using plotting libraries such as ggplot2 or tmap in RStudio. Your layers will be drawn depending on the order in which your layers are either tabled (as in a GUI GIS) or ‘called’ in your function in code. Being aware of this need for ‘order’ is important when we shift to using RStudio and tmap to plot our maps, as if you do not layer your data correctly in your code, your map will end up not looking as you hoped. For us using QGIS right now, the layers will be drawn from bottom to top.\n\n\n\nWe can now finally map the population distribution of London in 2011.\n\nRight-click on the LSOA2011_London layer and click on Properties -&gt; Symbology.\n\nIn the dropdown menu at the top of the window, select Graduated as symbology.\nUnder Value choose pop2011 as your column.\nWe can then change the color ramp to suit our aesthetic preferences. In the Colour ramp dropdown menu select Magma.\nThe final thing we need to do is classify our data - what this simply means is to decide how to group the values in our dataset together to create the graduated representation. We will be looking at classification options in later weeks, but for now, we will use the Natural Breaks option. Open the drop-down menu next to Mode, select Natural Breaks, change it to 7 classes and then click Classify.\nFinally click Apply to style your dataset.\n\n\n\n\n\n\n\n\nUnderstanding what classification is appropriate to visualise your data is an important step within spatial analysis and visualisation, and something you will learn more about in the following weeks. Overall, they should be determined by looking at the distribution of the underlying data.\n\n\n\nYou should now be looking at something like this:\n\n\n\n\n\nFigure 8: The population distribution of London in 2011. [Enlarge image]\n\n\n\n\n\n\n\n\n\n\nWhereas the above map is fine for today, it is technically incorrect because we are showing absolute numbers on a choropleth. This is something we should never do, unless the spatial units are identical in size (e.g. a hexagonal tessellation of an area), because larger areas will draw attention and affect the visualisation.\n\n\n\n\n\n\n\nTo export your map select only the map layers you want to export and then opt for Project -&gt; Import/Export -&gt; Export to Image and save your final map in your maps folder. Next week, we will look at how to style our maps using the main map conventions (adding North Arrows, Scale Bars and Legends, etc.) but for now a simple output will do."
  },
  {
    "objectID": "02-GIScience.html#assignment-w02",
    "href": "02-GIScience.html#assignment-w02",
    "title": "1 GIScience and GIS software",
    "section": "",
    "text": "You now need to repeat the entire process to also create a map for the 2021 Census data. Remember, you need to:\n\nLoad the respective spatial layer.\nLoad the respective population dataset.\nJoin the two datasets together using an Attribute Join.\nExport your joined dataset (London only) as a GeoPackage and save this into your raw/boundaries folder as LSOA2021_London.gpkg. We will use this file in later weeks.\nStyle your data appropriately.\nExport your maps as an image to your output folder.\n\n\n\n\n\n\n\nTo make visual comparisons against our two datasets, theoretically we would need to standardise the breaks at which our classification schemes are set at. To set all two datasets to the same breaks, you can do the following:\n\nRight-click again on the LSOA2011_London dataset and, click on Styles -&gt; Copy Styles -&gt; Symbology.\nNow right-click on the LSOA2021_London file, and click on Styles -&gt; Paste Style -&gt; Symbology. You should now see the classification breaks in the 2021 dataset change to match those in the 2011 data.\nThe final thing you need to do is to change the classification column in the Symbology tab for the 2021 dataset to say pop2021 instead of pop2011."
  },
  {
    "objectID": "02-GIScience.html#byl-w02",
    "href": "02-GIScience.html#byl-w02",
    "title": "1 GIScience and GIS software",
    "section": "",
    "text": "Save your project so you can go back to it if you need to, other than that that is it for this week. Probably time to start looking at this week’s reading list!"
  },
  {
    "objectID": "01-introduction.html",
    "href": "01-introduction.html",
    "title": "1 Geocomputation: An Introduction",
    "section": "",
    "text": "This week’s lecture provided you with a thorough introduction to this Geocomputation module, outlining how and why it is different to a traditional GIScience course. We set the scene for the remainder of the module and explained how the foundational concepts that you will learn in the first half of term sit within the overall module. This week we start easy by setting up our work environment and making sure that we can access the software that we will need over the coming weeks.\n\n\nThe slides for this week’s lecture can be downloaded here: [Link]\n\n\n\n\n\n\nBrundson, C. and Comber, A. 2020. Opening practice: Supporting reproducibility and critical spatial data science. Journal of Geographical Systems 23: 477–496. [Link]\nFranklin, R. 2023. Quantitative methods III: Strength in numbers? Progress in Human Geography. Online First. [Link].\nLongley, P. et al. 2015. Geographic Information Science & Systems, Chapter 1: Geographic Information: Science, Systems, and Society, pp. 1-32. [Link]\nSingleton, A. and Arribas-Bel, D. 2019. Geographic Data Science. Geographical Analysis 53(1): 61-75. [Link]\n\n\n\n\n\nMiller, H. and Goodchild, M. 2015. Data-driven geography. GeoJournal 80: 449–461. [Link]\nGoodchild, M. 2009. Geographic information systems and science: Today and tomorrow. Annals of GIS 15(1): 3-9. [Link]\nWorobey, M. et al. 2022. The Huanan Seafood Wholesale Market in Wuhan was the early epicenter of the COVID-19 pandemic. Science 377(6609): 951-959. [Link]\n\n\n\n\n\nOver the next few weeks, we will be taking a closer look at many of the foundational concepts that will ultimately enable you to confidently and competently analyse spatial data using both programming and GIS software. You will further learn how to plan, structure and conduct your own spatial analysis using programming.\n\n\n\nThis module primarily uses the R programming language, although we will start by using QGIS over the next two weeks to provide you with a visual introduction to the principles of spatial analysis.\n\n\n\n\n\n\nPlease follow the instructions below to install both QGIS and R onto your own personal computer. If you cannot install the software on your personal computer or you are not planning to bring your own laptop to the computer practicals, please refer to the UCL Desktop and RStudio Server section below. Please make sure that you have access to a working installation of QGIS and R (including relevant packages) before the first hands-on practical session next week.\n\n\n\n\n\nQGIS is an open-source graphic user interface GIS with many community developed add-on packages (or plugins) that provide additional functionality to the software. You can download and install QGIS on your personal machine by going to the QGIS website: [Link].\n\n\n\n\n\n\nWe recommend installing the Long Term Release (QGIS 3.28 LTR) as this version should be the most current stable version. For Windows users: be aware that the QGIS installation can be a little slow.\n\n\n\nAfter installation, start QGIS to see if the installation was successful.\n\n\n\nR is a programming language originally designed for conducting statistical analysis and creating graphics. R’s great strength is that it is open source, can be used on any computer operating system, and is free for anyone to use and contribute to. Because of this, it has rapidly become the statistical language of choice for many academics and has a large user community with people constantly contributing new packages to carry out all manner of statistical, graphical, and importantly for us, geographical tasks.\nInstalling R takes a few relatively simple steps involving two pieces of software. First there is the R programme itself. Follow these steps to get it installed on your computer:\n\nNavigate in your browser to the download page: [Link]\nIf you use a Windows computer, click on Download R for Windows. Then click on base. Download and install R 4.3.x for Windows. If you use a Mac computer, click on Download R for macOS and download and install R-4.3.x.arm64.pkg for Apple silicon Macs and R-4.3.x.x86_64.pkg for older Intel-based Macs.\n\nThat is it! You now have installed R onto your own machine. However, to make working with R a little bit easier we also need to install something called an Integrated Development Environment (IDE). We will use RStudio Desktop:\n\nNavigate to the official webpage of RStudio: [Link]\nDownload and install RStudio Desktop on your computer (free version!)\n\nAfter this, start RStudio to see if the installation was successful. If no errors are shown after start starting the programme, you are ready to go.\n\n\n\nThere are two alternatives to installing QGIS and R with RStudio onto your personal device. First, both programmes are available through Desktop@UCL Anywhere as well as all UCL computers on campus. Second, specifically to R, we have RStudio Server available which you can access through your web browser: [Link]\n\n\n\n\n\n\nRStudio Server is a version of R ‘in the cloud’ which you can access through your web browser.\n\n\n\nYou should be able to log onto the RStudio server with your regular UCL username and password. After logging in, you should see the RStudio interface appear.\n\n\n\n\n\nFigure 1: The RStudio Server interface. [Enlarge image]\n\n\n\n\n\n\n\n\n\n\nIf it is the first time you log on to RStudio server you may only see the RStudio interface appear once you have clicked on the start a new session button. More importantly: if you are not on campus, RStudio server will only work with an active Virtual Private Network (VPN) connection that links your personal computer into UCL’s network. Details on setting up a VPN connection can be found in UCL’s VPN connection guides: [Link]\n\n\n\n\n\n\nNow we have installed QGIS and R onto our machines / have access to QGIS and R through UCL’s resources, we need to customise R. Many useful R functions come in packages, these are free libraries of code written and made available by other R users. This includes packages specifically developed for data cleaning, data wrangling, visualisation, mapping, and spatial analysis. To save us some time, we will install all R packages that we will need over the next ten weeks in one go. Start RStudio, and copy and paste the following code into the console window. You can execute the code by hitting Enter. Depending on your computer’s specifications and the internet connection, this may take a while.\n\n\n\nR code\n\n# install packages\ninstall.packages(c(\"tidyverse\", \"sf\", \"tmap\", \"osmdata\", \"spatstat\", \"terra\", \"spdep\",\n    \"dbscan\", \"openair\", \"gstat\", \"dodgr\"))\n\n\n\n\n\n\n\n\nEven if you have used R or RStudio Server before and previously installed some of the packages in the above list, do re-install all packages by running the code above to make sure you have the latest versions. Legacy installations that have not been updated may cause errors when running the code in this workbook.\n\n\n\nOnce you have installed the packages, we need to check whether we can in fact load them into R. Copy and paste the following code into the console, and execute by hitting Enter again.\n\n\n\nR code\n\n# load packages\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\nlibrary(osmdata)\nlibrary(terra)\nlibrary(spatstat)\nlibrary(dbscan)\nlibrary(spdep)\nlibrary(openair)\nlibrary(gstat)\nlibrary(dodgr)\n\n\nYou will see some information printed to your console but as long as you do not get any of the messages below, the installation was successful. If you do get any of the messages below it means that the package was not properly installed, so try to install the package in question again.\n\nError: package or namespace load failed for &lt;packagename&gt;\nError: package '&lt;packagename&gt;' could not be loaded\nError in library(&lt;packagename&gt;) : there is no package called '&lt;packagename&gt;'\n\n\n\n\n\n\n\nMany packages depend on other packages (so-called dependencies). It happens at times when you install a package it does not install all dependencies. If you encounter any of the above errors in reference to a package that you did not explicitly install, it is probably a missing dependency. Simply install the dependency by typing install.packages('&lt;dependencyname&gt;') and try loading all packages again.\n\n\n\n\n\n\nArcGIS Pro is the main commercial GIS software that you may have already used or seen/heard about through other modules or even job adverts. We do not use ArcGIS Pro in this module for the following reasons:\n\nComputing requirements for ArcGIS Pro are substantial and it only operates on the Windows Operating System. For Mac or Linux users, using ArcGIS Pro would require using a Virtual Machine, Docker Installation, or a separate copy of Windows OS running on a separate partition of your hard drive.\nArcGIS is proprietary software, which means you need a license to use the software. For those of us in education, the University covers the cost of this license, but when you leave, you will need to pay for a personal license to continue using the software and repeat any analysis you have used the software for.\nWhilst ArcPro can use pure Python (and even R) as a programming language within it through scripts and notebooks, it primarily relies on its own ArcPy and ArcGIS API for Python packages to run the in-built tools and analytical functions. To use these packages, you still need a license which makes it difficult to share your code with others if they do not have their own ArcGIS license.\n\nRecent developments in the ArcPro software, however, does make it an attractive tool for spatial data science and quantitative geography: it has cross-user functionality, from data analysts to those focused more on cartography and visualisation with in-built bridges to Adobe’s Creative Suite. We therefore do not want to put you off looking into ArcGIS in the future, but during this module we want to ensure the reproducibility of your work.\n\n\n\n\nYou should now be all ready to go with the computer practicals the coming week. Nice and easy. Time to look at the articles on the reading list? That is it for this week!"
  },
  {
    "objectID": "01-introduction.html#slides-w01",
    "href": "01-introduction.html#slides-w01",
    "title": "1 Geocomputation: An Introduction",
    "section": "",
    "text": "The slides for this week’s lecture can be downloaded here: [Link]"
  },
  {
    "objectID": "01-introduction.html#reading-w01",
    "href": "01-introduction.html#reading-w01",
    "title": "1 Geocomputation: An Introduction",
    "section": "",
    "text": "Brundson, C. and Comber, A. 2020. Opening practice: Supporting reproducibility and critical spatial data science. Journal of Geographical Systems 23: 477–496. [Link]\nFranklin, R. 2023. Quantitative methods III: Strength in numbers? Progress in Human Geography. Online First. [Link].\nLongley, P. et al. 2015. Geographic Information Science & Systems, Chapter 1: Geographic Information: Science, Systems, and Society, pp. 1-32. [Link]\nSingleton, A. and Arribas-Bel, D. 2019. Geographic Data Science. Geographical Analysis 53(1): 61-75. [Link]\n\n\n\n\n\nMiller, H. and Goodchild, M. 2015. Data-driven geography. GeoJournal 80: 449–461. [Link]\nGoodchild, M. 2009. Geographic information systems and science: Today and tomorrow. Annals of GIS 15(1): 3-9. [Link]\nWorobey, M. et al. 2022. The Huanan Seafood Wholesale Market in Wuhan was the early epicenter of the COVID-19 pandemic. Science 377(6609): 951-959. [Link]"
  },
  {
    "objectID": "01-introduction.html#getting-started",
    "href": "01-introduction.html#getting-started",
    "title": "1 Geocomputation: An Introduction",
    "section": "",
    "text": "Over the next few weeks, we will be taking a closer look at many of the foundational concepts that will ultimately enable you to confidently and competently analyse spatial data using both programming and GIS software. You will further learn how to plan, structure and conduct your own spatial analysis using programming."
  },
  {
    "objectID": "01-introduction.html#software",
    "href": "01-introduction.html#software",
    "title": "1 Geocomputation: An Introduction",
    "section": "",
    "text": "This module primarily uses the R programming language, although we will start by using QGIS over the next two weeks to provide you with a visual introduction to the principles of spatial analysis.\n\n\n\n\n\n\nPlease follow the instructions below to install both QGIS and R onto your own personal computer. If you cannot install the software on your personal computer or you are not planning to bring your own laptop to the computer practicals, please refer to the UCL Desktop and RStudio Server section below. Please make sure that you have access to a working installation of QGIS and R (including relevant packages) before the first hands-on practical session next week.\n\n\n\n\n\nQGIS is an open-source graphic user interface GIS with many community developed add-on packages (or plugins) that provide additional functionality to the software. You can download and install QGIS on your personal machine by going to the QGIS website: [Link].\n\n\n\n\n\n\nWe recommend installing the Long Term Release (QGIS 3.28 LTR) as this version should be the most current stable version. For Windows users: be aware that the QGIS installation can be a little slow.\n\n\n\nAfter installation, start QGIS to see if the installation was successful.\n\n\n\nR is a programming language originally designed for conducting statistical analysis and creating graphics. R’s great strength is that it is open source, can be used on any computer operating system, and is free for anyone to use and contribute to. Because of this, it has rapidly become the statistical language of choice for many academics and has a large user community with people constantly contributing new packages to carry out all manner of statistical, graphical, and importantly for us, geographical tasks.\nInstalling R takes a few relatively simple steps involving two pieces of software. First there is the R programme itself. Follow these steps to get it installed on your computer:\n\nNavigate in your browser to the download page: [Link]\nIf you use a Windows computer, click on Download R for Windows. Then click on base. Download and install R 4.3.x for Windows. If you use a Mac computer, click on Download R for macOS and download and install R-4.3.x.arm64.pkg for Apple silicon Macs and R-4.3.x.x86_64.pkg for older Intel-based Macs.\n\nThat is it! You now have installed R onto your own machine. However, to make working with R a little bit easier we also need to install something called an Integrated Development Environment (IDE). We will use RStudio Desktop:\n\nNavigate to the official webpage of RStudio: [Link]\nDownload and install RStudio Desktop on your computer (free version!)\n\nAfter this, start RStudio to see if the installation was successful. If no errors are shown after start starting the programme, you are ready to go.\n\n\n\nThere are two alternatives to installing QGIS and R with RStudio onto your personal device. First, both programmes are available through Desktop@UCL Anywhere as well as all UCL computers on campus. Second, specifically to R, we have RStudio Server available which you can access through your web browser: [Link]\n\n\n\n\n\n\nRStudio Server is a version of R ‘in the cloud’ which you can access through your web browser.\n\n\n\nYou should be able to log onto the RStudio server with your regular UCL username and password. After logging in, you should see the RStudio interface appear.\n\n\n\n\n\nFigure 1: The RStudio Server interface. [Enlarge image]\n\n\n\n\n\n\n\n\n\n\nIf it is the first time you log on to RStudio server you may only see the RStudio interface appear once you have clicked on the start a new session button. More importantly: if you are not on campus, RStudio server will only work with an active Virtual Private Network (VPN) connection that links your personal computer into UCL’s network. Details on setting up a VPN connection can be found in UCL’s VPN connection guides: [Link]\n\n\n\n\n\n\nNow we have installed QGIS and R onto our machines / have access to QGIS and R through UCL’s resources, we need to customise R. Many useful R functions come in packages, these are free libraries of code written and made available by other R users. This includes packages specifically developed for data cleaning, data wrangling, visualisation, mapping, and spatial analysis. To save us some time, we will install all R packages that we will need over the next ten weeks in one go. Start RStudio, and copy and paste the following code into the console window. You can execute the code by hitting Enter. Depending on your computer’s specifications and the internet connection, this may take a while.\n\n\n\nR code\n\n# install packages\ninstall.packages(c(\"tidyverse\", \"sf\", \"tmap\", \"osmdata\", \"spatstat\", \"terra\", \"spdep\",\n    \"dbscan\", \"openair\", \"gstat\", \"dodgr\"))\n\n\n\n\n\n\n\n\nEven if you have used R or RStudio Server before and previously installed some of the packages in the above list, do re-install all packages by running the code above to make sure you have the latest versions. Legacy installations that have not been updated may cause errors when running the code in this workbook.\n\n\n\nOnce you have installed the packages, we need to check whether we can in fact load them into R. Copy and paste the following code into the console, and execute by hitting Enter again.\n\n\n\nR code\n\n# load packages\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\nlibrary(osmdata)\nlibrary(terra)\nlibrary(spatstat)\nlibrary(dbscan)\nlibrary(spdep)\nlibrary(openair)\nlibrary(gstat)\nlibrary(dodgr)\n\n\nYou will see some information printed to your console but as long as you do not get any of the messages below, the installation was successful. If you do get any of the messages below it means that the package was not properly installed, so try to install the package in question again.\n\nError: package or namespace load failed for &lt;packagename&gt;\nError: package '&lt;packagename&gt;' could not be loaded\nError in library(&lt;packagename&gt;) : there is no package called '&lt;packagename&gt;'\n\n\n\n\n\n\n\nMany packages depend on other packages (so-called dependencies). It happens at times when you install a package it does not install all dependencies. If you encounter any of the above errors in reference to a package that you did not explicitly install, it is probably a missing dependency. Simply install the dependency by typing install.packages('&lt;dependencyname&gt;') and try loading all packages again.\n\n\n\n\n\n\nArcGIS Pro is the main commercial GIS software that you may have already used or seen/heard about through other modules or even job adverts. We do not use ArcGIS Pro in this module for the following reasons:\n\nComputing requirements for ArcGIS Pro are substantial and it only operates on the Windows Operating System. For Mac or Linux users, using ArcGIS Pro would require using a Virtual Machine, Docker Installation, or a separate copy of Windows OS running on a separate partition of your hard drive.\nArcGIS is proprietary software, which means you need a license to use the software. For those of us in education, the University covers the cost of this license, but when you leave, you will need to pay for a personal license to continue using the software and repeat any analysis you have used the software for.\nWhilst ArcPro can use pure Python (and even R) as a programming language within it through scripts and notebooks, it primarily relies on its own ArcPy and ArcGIS API for Python packages to run the in-built tools and analytical functions. To use these packages, you still need a license which makes it difficult to share your code with others if they do not have their own ArcGIS license.\n\nRecent developments in the ArcPro software, however, does make it an attractive tool for spatial data science and quantitative geography: it has cross-user functionality, from data analysts to those focused more on cartography and visualisation with in-built bridges to Adobe’s Creative Suite. We therefore do not want to put you off looking into ArcGIS in the future, but during this module we want to ensure the reproducibility of your work."
  },
  {
    "objectID": "01-introduction.html#byl-w01",
    "href": "01-introduction.html#byl-w01",
    "title": "1 Geocomputation: An Introduction",
    "section": "",
    "text": "You should now be all ready to go with the computer practicals the coming week. Nice and easy. Time to look at the articles on the reading list? That is it for this week!"
  },
  {
    "objectID": "06-operations.html",
    "href": "06-operations.html",
    "title": "1 Analysing Spatial Patterns I: Geometric Operations and Spatial Queries",
    "section": "",
    "text": "This week, we look at geometric operations and spatial queries — the fundamental building blocks when it comes to spatial data processing and analysis. This includes operations such as calculating the distances separating one or more spatial objects, running a buffer analysis, and conducting point-in-polygon calculations.\n\n\nThe slides for this week’s lecture can be downloaded here: [Link]\n\n\n\n\n\n\nLovelace, R., Nowosad, J. and Muenchow, J. 2021. Geocomputation with R, Chapter 4: Spatial data operations. [Link]\nLovelace, R., Nowosad, J. and Muenchow, J. 2021. Geocomputation with R, Chapter 5: Geometry operations. [Link]\nLovelace, R., Nowosad, J. and Muenchow, J. 2021. Geocomputation with R, Chapter 6: Reprojecting geographic data. [Link]\n\n\n\n\n\nHoulden, V. et al. 2019. A spatial analysis of proximate greenspace and mental wellbeing in London. Applied Geography 109: 102036. [Link]\nMalleson, N. and Andresen, M. 2016. Exploring the impact of ambient population measures on London crime hotspots. Journal of Criminal Justice 46: 52-63. [Link]\n\n\n\n\n\nThis week, we will be investigating bike theft in London in 2021 and look to confirm the hypothesis that bike theft primarily occurs near tube and train stations. We will be investigating its distribution across London using the point data provided within our crime dataset. We will then compare this distribution to the location of train and tube stations using specific geometric operations and spatial queries that can compare the geometry of two (or more) datasets. We will also learn how to download data from OpenStreetMap as well as use an interactive version of tmap to explore the distribution of the locations of individual bike theft against the locations of these stations.\n\n\nOpen a new script within your GEOG0030 project and save this script as wk6-bike-theft-analysis.r. At the top of your script, add the following metadata:\n\n\n\nR code\n\n# Analysing bike theft in London using geometric analysis\n# Date: January 2024\n\n\nNow let us add all of the libraries we will be using today to the top of our script:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\nlibrary(osmdata)\n\n\n\n\n\nThis week, we will start off using three datasets: the London MSOA boundaries for 2021, recorded crime in London for 2021 from data.police.uk, and the locations of the train and tube stations from Transport for London. We already downloaded the crime data for 2021 during Week 4’s computer tutorial and we also saved the 2021 London MSOA boundaries last week, so we only need to download a dataset containing train and tube stations in London.\n\n\n\nFile\nType\nLink\n\n\n\n\nTrain and tube stations in London\nkml\nDownload\n\n\n\nOnce downloaded, move your tfl_stations.kml download to your raw data folder and create a new transport folder to contain it. After this, let’s load our London MSOA file:\n\n\n\nR code\n\n# read in our MSOA GeoPackage\nmsoa_london &lt;- st_read(\"data/raw/boundaries/MSOA2021_London.gpkg\")\n\n\nReading layer `MSOA2021_London' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/raw/boundaries/MSOA2021_London.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1002 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n\n\nCheck the CRS of our msoa_london spatial dataframe:\n\n\n\nR code\n\n# inspect CRS\nst_crs(msoa_london)\n\n\nCoordinate Reference System:\n  User input: OSGB36 / British National Grid \n  wkt:\nPROJCRS[\"OSGB36 / British National Grid\",\n    BASEGEOGCRS[\"OSGB36\",\n        DATUM[\"Ordnance Survey of Great Britain 1936\",\n            ELLIPSOID[\"Airy 1830\",6377563.396,299.3249646,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4277]],\n    CONVERSION[\"British National Grid\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",49,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",-2,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996012717,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",400000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",-100000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"United Kingdom (UK) - offshore to boundary of UKCS within 49°45'N to 61°N and 9°W to 2°E; onshore Great Britain (England, Wales and Scotland). Isle of Man onshore.\"],\n        BBOX[49.75,-9,61.01,2.01]],\n    ID[\"EPSG\",27700]]\n\n\nOf course it should be of no surprise that our msoa_lon spatial dataframe is projected in OSGB36 / British National Grid, however, it is always good to check and avoid problems down the line. Let’s go ahead and read in our tfl_stations dataset as well:\n\n\n\nR code\n\n# load stations\nlondon_stations &lt;- read_sf(\"data/raw/transport/tfl_stations.kml\")\n\n\nThis dataset is provided as a kml file, which stands for Keyhole Markup Language (KML). KML was originally created as a file format used to display geographic data in Google Earth. So we definitely need to check what CRS this dataset is in and decide whether we need to reproject the data.\n\n\n\nR code\n\n# inspect CRS\nst_crs(london_stations)\n\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\nThe result informs us that we are going to need to reproject our data in order to use this dataframe with our msoa_london spatial dataframe. Luckily in R and the sf library, this reprojection is a relatively straightforward with the st_transform() function. The function is very simple to use: you only need to provide the function with the input dataset that you want to reproject and the EPSG code of the target CRS.\n\n\n\nR code\n\n# reproject our data from WGS84 to BNG\nlondon_stations &lt;- st_transform(london_stations, 27700)\n\n\nWe can double-check whether our new variable is in the correct CRS by using the st_crs() function again:\n\n\n\nR code\n\n# inspect CRS\nst_crs(london_stations)\n\n\nCoordinate Reference System:\n  User input: EPSG:27700 \n  wkt:\nPROJCRS[\"OSGB36 / British National Grid\",\n    BASEGEOGCRS[\"OSGB36\",\n        DATUM[\"Ordnance Survey of Great Britain 1936\",\n            ELLIPSOID[\"Airy 1830\",6377563.396,299.3249646,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4277]],\n    CONVERSION[\"British National Grid\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",49,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",-2,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996012717,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",400000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",-100000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"United Kingdom (UK) - offshore to boundary of UKCS within 49°45'N to 61°N and 9°W to 2°E; onshore Great Britain (England, Wales and Scotland). Isle of Man onshore.\"],\n        BBOX[49.75,-9,61.01,2.01]],\n    ID[\"EPSG\",27700]]\n\n\nYou should see that our london_stations spatial dataframe is now in OSGB36 / British National Grid. We are now ready to load our final dataset - our collection of csv's that contain the crime data for London for 2021. We can do this by running the code from Week 4’s computer tutorial:\n\n\n\nR code\n\n# create a list of all csv files in the crime folder\nall_crime_df &lt;- list.files(path = \"data/raw/crime/all-crime/\", full.names = TRUE, recursive = TRUE) |&gt;\n  # apply the read_csv() function on each of these files\n  lapply(read_csv) |&gt;\n  # combine ('bind') them all together into one\n  bind_rows()\n\n\nNow we have loaded all crime data again, we want to do three things:\n\nExtract only those crimes that are bicycle thefts.\nConvert our csv into a spatial dataframe that shows the locations of our crimes, using the recorded latitude and longitudes.\nReproject the latitudes and longitudes from WGS84: EPSG 4326 to British National Grid: EPSG 27700.\n\n\n\n\nR code\n\n# filter all crimes by bicycle thefts only\nbike_theft &lt;- all_crime_df |&gt;\n  # filter according to crime type, filter out crimes with no location data\n  filter(`Crime type` == \"Bicycle theft\" & !is.na(Longitude) & !is.na(Latitude)) |&gt;\n  # only keep the longitude and latitude columns\n  dplyr::select(Longitude, Latitude) |&gt;\n  # transform into a point spatial dataframe, set CRS to WGS84\n  st_as_sf(coords = c(\"Longitude\", \"Latitude\"), crs = 4236) |&gt;\n  # transform into BNG\n  st_transform(27700)\n\n\nLet’s go ahead and save the bike_theft spatial layer as a GeoPackage:\n\n\n\nR code\n\n# write to GeoPackage\nst_write(bike_theft, 'data/data/LondonBikeTheft2021.gpkg')\n\n\nWe now have our three datasets loaded, it is time for a little data inspection. We can see just from our Environment window that in total, we have 302 train and tube stations and 20,768 crimes to look at in our analysis. We can double-check the attributes of our newly created spatial dataframes to see what data we have to work with. You can either do this manually by clicking on the variable, or using commands such as head(), summary() and names() to get an understanding of our dataframe structures and the fieldnames present.\nFor our bicycle theft data, we actually only have a geometry column because this is all that we extracted from our collection of crime csv files. For our london_stations spatial dataframe, we have a little more information, including the name of the station, its address, and as its geometry.\nNow, let’s map all three layers of data onto a single map using tmap:\n\n\n\nR code\n\n# plot our London MSOAs\ntm_shape(msoa_london) +\n  tm_fill() +\n  # then add bike crime\n  tm_shape(bike_theft) +\n  tm_dots(\n    col = \"blue\"\n  ) +\n  # then add stations\n  tm_shape(london_stations) +\n  tm_dots(\n    col = \"red\"\n  )\n\n\n\n\n\nFigure 1: Quick thematic map.\n\n\n\n\nLet’s think about the distribution of our data: we can already see that our bike theft is clearly highly concentrated in the centre of London although we can certainly see some clusters in other areas. Let’s go ahead and temporarily remove the bike theft data from our map for now to see where our tube and train stations are located.\nTo remove the bike data, simply comment out the (#) the relevant line of code:\n\n\n\nR code\n\n# plot our London MSOAs\ntm_shape(msoa_london) +\n  tm_fill() +\n  # then add bike crime\n  tm_shape(bike_theft) +\n  tm_dots(\n    col = \"blue\"\n  ) +\n  # then add stations\n  tm_shape(london_stations) +\n  tm_dots(\n    col = \"red\"\n  )\n\n\n\n\n\nFigure 2: Quick thematic map.\n\n\n\n\nWe can see our train and tube stations are only present in primarily the north of London and not really present in the south. This is not quite right and in fact it seems that our dataset only contains those train stations used by Transport for London within the underground network rather than all the stations in London. We will need to fix this before conducting our full analysis. A second issue with our dataset is that the london_stations spatial dataframe extends beyond our London boundaries. The same applies to our bike_theft spatial dataframe.\n\n\n\nWhen we want to reduce a dataset to the spatial extent of another, there are two different approaches: a subset or a clip. Each deal with the geometry of the resulting dataset in slightly different ways.\n\nA subset operation is what is known in GIScience-speak as a select by location query. In this case, our subset will return the full geometry of each observation feature of the input layer that intersects with our second layer. Any geometry that does not intersect with our second layer will be removed from the geometry of our resulting layer.\nA clip works a bit like a cookie-cutter: it will take the geometry of the input layer (i.e. the layer you want to clip), places a ‘cookie-cutter’ layer on top (i.e. the layer you want to clip by) and then returns only the parts of the input layer contained within the cookie-cutter. This will mean that the geometry of our resulting layer will be modified, if it contains observation features that extend further than the ‘cookie-cutter’ extent it will literally ‘cut’ the geometry of our data.\n\n\n\n\n\n\n\nBecause we are using point data, we can use either approach because it is not possible to split the geometry of a single point feature. When it comes to polygon and line data, the different approaches are likely to result in different results.\n\n\n\nEach approach is implemented differently in R. To subset our data, we only need to use the base R library to selection using [] brackets:\n\n\n\nR code\n\n# subset\nbike_theft &lt;- bike_theft[msoa_london, ]\nlondon_stations &lt;- london_stations[msoa_london, ]\n\n\nIf we want to clip our data, we need to use the st_intersection() function from the sf library.\n\n\n\nR code\n\n# clip\nbike_theft &lt;- bike_theft |&gt;\n    st_intersection(msoa_london)\n\n\nWarning: attribute variables are assumed to be spatially constant throughout all\ngeometries\n\nlondon_stations &lt;- london_stations |&gt;\n    st_intersection(msoa_london)\n\nWarning: attribute variables are assumed to be spatially constant throughout all\ngeometries\n\n\n\n\n\n\n\n\nOut of the two, the subset approach is the fastest to use as R is simply comparing the geometries rather than also editing the geometries, but which approach you use with future data is always dependent on your data and the output you need.\n\n\n\nBefore we go ahead and sort out our london_stations spatial dataframe, we are going to look at how we can dissolve our msoa_london spatial dataframe into a single feature. Reducing a spatial dataframe to a single observation is often required when using R and sf’s geometric operations to complete geometric comparisons. In addition, sometimes we simply want to map an outline of an area, such as London, rather than add in the additional spatial complexities of internal boundaries. To achieve just a single observation that represents the outline geometry of our dataset, we use the geometric operation st_union().\n\n\n\n\n\n\nYou can also use the st_union() function to combine two spatial datasets into one. This can be used to merge data together that are of the same spatial type. E.g. if you have a file that contains the MSOAs pertaining to England and a file that contains the MSOAs pertaining to Wales, you can use st_union to create one file that contains the MSOAs for both countries.\n\n\n\n\n\n\nR code\n\n# union\nlondon_outline &lt;- msoa_london |&gt;\n    st_union()\n\n\nYou should see that our london_outline spatial data frame only has one observation. You can now go ahead and plot() your london_outline spatial dataframe from your console and see what it looks like:\n\n\n\nR code\n\nplot(london_outline)\n\n\n\n\n\nFigure 3: Quick plot of the London outline.\n\n\n\n\n\n\n\nBack to our train and tube stations. We have seen that our current london_stations spatial dataframe does not provide the coverage of train stations in London that we expected. To add in our missing data, we will be using OpenStreetMap.\n\n\n\n\n\n\nOpenStreetMap (OSM) is a free editable map of the world,although its spatial coverage is still unequal across the world. In addition, as you will find if you use the data, the accuracy and quality of the data can often be quite questionable or simply missing attribute details that we would like to have, e.g. types of roads and their speed limits, to complete specific types of spatial analysis. As a result, do not expect OSM to contain every piece of spatial data that you would want.\n\n\n\nWhilst there are various approaches to downloading data from OpenStreetMap, we will use the osmdata library to directly extract our required OpenStreetMap (OSM) data into a variable. The osmdata library grants access within R to the Overpass API that allows us to run queries on OSM data and then import the data as sf object. These queries are at the heart of these data downloads.\nTo use the library (and API), we need to know how to write and run a query, which requires identifying the key and value that we need within our query to select the correct data. Essentially every map element (whether a point, line or polygon) in OSM is tagged with different attribute data. In our case, we are looking for train stations, which fall under the key, Public Transport, with a value of station as outlined in their wiki. These keys and values are used in our queries to extract only map elements of that feature type - to find out how a feature is tagged in OSM is simply a case of reading through the OSM documentation and becoming familiar with their keys and values.\nIn addition to this key-value pair, we also need to obtain the bounding box of where we want our data to be extracted from to limit the search to the specific area of interest. Let’s try to extract elements from OSM that are tagged as public_transport = station from OSM into an osmdata_sf() object:\n\n\n\nR code\n\n# extract latitude, longitude bounding box from London outline\np_bbox &lt;- st_bbox(st_transform(london_outline, 4326))\n\n# call OverPassQuery function\nlondon_stations_osm &lt;- opq(bbox = p_bbox) |&gt;\n  # add key, values\n  add_osm_feature(key = \"public_transport\", value = \"station\") |&gt;\n  # to sf\n  osmdata_sf()\n\n\n\n\n\n\n\n\nIn some instances the OSM query will return an error, especially when several people from the same location are executing the exact same query at the same time. If that is the case you can download the london_stations_osm object here: [Download]. After downloading, you can copy the file to your working directory and load the object using the load() function.\n\n\n\nWhen we download OSM data, and extract it as above, our query will return all elements tagged as our key-value pair into our osmdata_sf() OSM data object. This means all elements associated with our tag will be returned: any points, lines and polygons. We might think with our public_transport = station tag, we would only return point data representing our train and tube stations in London. But if we use the summary() function on our london_stations_osm OSM data object, we can see that not only a lot of other data is stored in our OSM data object (including the bounding box we used within our query, plus metadata about our query), but our query has also returned both points and polygons stored within this OSM data object as individual spatial data frames.\nTo extract only the points of our tube and train stations from our london_stations_osm OSM data object, we simply need to extract this from the dataframe and store this under a separate variable.\n\n\n\nR code\n\n# extract only points\nlondon_stations_osm &lt;- london_stations_osm$osm_points |&gt;\n  # add projection information\n  st_set_crs(4326) |&gt;\n  # reproject\n  st_transform(27700) |&gt;\n  # clip\n  st_intersection(london_outline) |&gt;\n  # select relevant attributes |&gt;\n  dplyr::select(c(\"osm_id\", \"name\", \"network\", \"operator\", \"public_transport\", \"railway\"))\n\n\nWarning: attribute variables are assumed to be spatially constant throughout all\ngeometries\n\n# inspect\nplot(london_stations_osm)\n\n\n\n\nFigure 4: Points marked as stations extracted from OpenStreetMap.\n\n\n\n\nWith the accuracy of OSM a little questionable, we want to complete some data validation tasks to check its quality and to confirm that it at least contains the data we see in our authoritative london_stations spatial dataframe. The total number of data points also seems rather high. In fact, a quick search online can tell us that there are 272 tube stations in the London network as well as 339 train stations in Greater London.\nAs we can see in our plot above, not all of our stations appear to be of the same value in our railway field. If we check the field using our count() function, you will see that there are some different values and NAs in our dataset:\n\n\n\nR code\n\n# count\ncount(london_stations_osm, railway)\n\n\nSimple feature collection with 7 features and 2 fields\nGeometry type: MULTIPOINT\nDimension:     XY\nBounding box:  xmin: 505078.2 ymin: 159027.2 xmax: 556185.7 ymax: 200138.6\nProjected CRS: OSGB36 / British National Grid\n           railway    n                       geometry\n1         entrance    2 MULTIPOINT ((532814.8 16572...\n2 railway_crossing    2 MULTIPOINT ((523304.2 17881...\n3          station  608 MULTIPOINT ((505078.2 17673...\n4             stop   25 MULTIPOINT ((513225.1 18452...\n5  subway_entrance   44 MULTIPOINT ((513239 184507....\n6           switch    9 MULTIPOINT ((523304.3 17879...\n7             &lt;NA&gt; 4304 MULTIPOINT ((505595.9 18418...\n\n\nAs we can see, not every feature in our london_stations_osm spatial dataframe is recorded as a station and we have a high number of NAs which are unlikely to represent actual stations. The number of points marked as station in the railway field are most likely the only points in our dataset that represent actual stations. There is still a difference between the official numbers and the OSM extract, but we will go on use this information to clean up the OSM dataset:\n\n\n\nR code\n\n# extract train and tube stations\nlondon_stations_osm &lt;- london_stations_osm |&gt;\n    filter(railway == \"station\")\n\n\nWe have now cleaned our london_stations_osm spatial dataframe to remove all those points within our dataset that are not tagged as railway == \"station\". Our london_stations spatial dataframe is of course an authoritative dataset from TfL, so we hope that this data is accurate, albeit incomplete, and at very least does not contain stations that do not exist. Therefore, it would be helpful if we could compare our two datasets to one another spatially to double-check that our london_stations_osm spatial dataframe contains all the data found within our london_stations spatial dataframe. We can first look at this by comparing their distributions visually on a map.\n\n\n\nR code\n\n# add London outline\ntm_shape(london_outline) +\n  tm_fill() +\n  # add OSM station data\n  tm_shape(london_stations_osm) +\n  tm_dots(\n    col = \"black\"\n  ) +\n  # add TfL station data\n  tm_shape(london_stations) +\n  tm_dots(\n    col = \"red\"\n  ) +\n  # add north arrow\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"bottom\")\n  ) +\n  # add scale bar\n  tm_scale_bar(\n    breaks = c(0, 5, 10, 15, 20),\n    position = c(\"left\", \"bottom\")\n  ) +\n  # add credits\n  tm_credits(\"© OpenStreetMap contributors\")\n\n\n\n\n\nFigure 5: Map of TfL and OSM stations.\n\n\n\n\nWhat we can see is that it looks like our OSM data actual does a much better job at covering all train and tube stations across London but still it is pretty hard to get a sense of comparison from a static map like this whether it contains all of the tube and train stations in our london_stations spatial dataframe. An interactive map would enable us to interrogate the spatial coverage of our two station spatial dataframes further. To do so, we use the tmap_mode() function and change it from its default plot() mode to a view() model:\n\n\n\nR code\n\n# change tmap mode to interactive\ntmap_mode(\"view\")\n\n# add London outline\ntm_shape(london_outline) +\n  tm_borders() +\n  # add OSM station data\n  tm_shape(london_stations_osm) +\n  tm_dots(\n    col = \"black\"\n  ) +\n  # add TfL station data\n  tm_shape(london_stations) +\n  tm_dots(\n    col = \"red\"\n  ) +\n  # add basemap\n  tm_basemap(c(StreetMap = \"OpenStreetMap\"))\n\n\n\n\n\n\nFigure 6: Interactive map of TfL and OSM stations.\n\n\n\nUsing the interactive map, what we can see is that whilst we do have overlap with our datasets, and more importantly, our london_stations_osm spatial dataframe seems to contain all of the data within the london_stations spatial dataframe, although there are definitely differences in their precise location. Now depending on what level of accuracy we are willing to accept with our assumption that our OSM data contains the same data as our Transport for London data, we could leave our comparison here and move forward with our analysis. There are, however, several more steps we could complete to validate this assumption. The easiest first step is to simply reverse the order of our datasets to check that each london_stations spatial dataframe point is covered by reversing the drawing order:\n\n\n\nR code\n\n# add London outline\ntm_shape(london_outline) +\n  tm_borders() +\n  # add TfL station data\n  tm_shape(london_stations) +\n  tm_dots(\n    col = \"red\"\n  ) +\n  # add OSM station data\n  tm_shape(london_stations_osm) +\n  tm_dots(\n    col = \"black\"\n  ) +\n  # add basemap\n  tm_basemap(c(StreetMap = \"OpenStreetMap\"))\n\n\n\n\n\n\nFigure 7: Interactive map of TfL and OSM stations.\n\n\n\n\n\n\nThe comparison looks pretty good but still the question is: can we be sure? Using geometric operations and spatial queries, we can look to find if any of our stations in our london_stations spatial dataframe are not present the london_stations_osm spatial dataframe. We can use specific geometric operations and/or queries that let us check whether or not all points within our london_stations spatial dataframe spatially intersect with our london_stations_osm spatial dataframe, i.e. we can complete the opposite of the clip/intersection that we conducted earlier. The issue we face, however is that, as we saw above, our points are slightly offset from one another as the datasets have ultimately given the same stations slightly different locations. This offset means we need to think a little about the geometric operation or spatial query that we want to use.\nWe will approach this question in two different ways to highlight the differences between geometric operations and spatial queries:\n\nWe will use geometric operations to generate geometries that highlight missing stations from our london_stations spatial dataframe (i.e. ones that are not present in thelondon_stations_osm spatial dataframe).\nWe will use spatial queries to provide us with a list of features in our london_stations spatial dataframe that do not meet our spatial requirements (i.e. are not present in thelondon_stations_osm spatial dataframe).\n\n\n\nAs highlighted above, the offset between our spatial dataframes adds a little complexity to our geometric operations code. To be able to make our direct spatial comparisons across our spatial dataframes, what we first need to do is try to snap the geometry of our london_stations spatial dataframe to our london_stations_osm spatial dataframe for points within a given distance threshold. This will mean that any points in the london_stations spatial dataframe that are within a specific distance of the london_stations_osm spatial dataframe will have their geometry changed to that of the london_stations_osm spatial dataframe.\n\n\n\n\n\nFigure 8: Snapping points to a line. In our case we snap our points to other points. [Enlarge image]\n\n\n\n\nBy placing a threshold on this snap, we stop too many points moving about if they are unlikely to be representing the same station (e.g. further than 150m or so away) but this still allows us to create more uniformity across our datasets’ geometries (and tries to reduce the uncertainty we add by completing this process).\nSnap our our london_stations spatial dataframe to our london_stations_osm spatial dataframe for points within a 150m distance threshold:\n\n\n\nR code\n\n# snap points\nlondon_stations_snap &lt;- london_stations |&gt;\n    st_snap(london_stations_osm, 150)\n\n\nNow we have out snapped geometry, we can look to compare our two datasets to calculate whether or not our london_stations_osm spatial dataframe is missing any data from our london_stations_snap spatial dataframe. To do so, we will use the st_difference() function which will return us the geometries of those points in our london_stations_snap spatial dataframe that are missing in our our london_stations_osm spatial dataframe. However, to use this function successfully we need to convert our our london_stations_osm spatial dataframe into a single geometry first. To simplify our london_stations_osm spatial dataframe into a single geometry, we simply use the st_union() code we used with our London outline above:\n\n\n\nR code\n\n# create a single geometry\nlondon_stations_osm_compare &lt;- london_stations_osm |&gt;\n    st_union()\n\n# compare the geometries\nmissing_stations &lt;- st_difference(london_stations_snap, london_stations_osm_compare)\n\n\nWarning: attribute variables are assumed to be spatially constant throughout all\ngeometries\n\n\nYou should now find that we apparently have 3 missing stations in our london_stations_osm spatial dataframe. We can plot these missing stations against our london_stations_osm spatial dataframe and confirm whether these stations are indeed missing or not.\n\n\n\nR code\n\n# add OSM station data\ntm_shape(london_stations_osm) +\n  tm_dots(\n    col = \"black\"\n  ) +\n  # add missing station data\n  tm_shape(missing_stations) +\n  tm_dots(\n    col = \"green\"\n  ) +\n  # add basemap\n  tm_basemap(c(StreetMap = \"OpenStreetMap\"))\n\n\n\n\n\n\nFigure 9: Interactive map of TfL and OSM stations after the snapping operation.\n\n\n\nWhen you investigate the missing stations, you can actually see that our london_stations_osm spatial dataframe dataset is actually more accurate than the TfL locations. All ‘missing’ stations are not in fact missing but simply at a greater offset than 150m. We can safely suggest that we can move forward with only using the london_stations_osm spatial dataframe and do not need to follow through with adding any more data to this dataset. Before we move on, let’s save the data as a GeoPackage.\n\n\n\nR code\n\n# write to GeoPackage\nst_write(london_stations_osm, \"data/data/LondonStations.gpkg\")\n\n\n\n\n\nBefore we go ahead and move forward with our analysis, we will have a look at how we can implement the above quantification using spatial queries instead of geometric operations. Usually, when we want to find out if two spatial dataframes have the same or similar geometries, we would use one of the following queries:\n\nst_equals()\nst_intersects()\nst_crosses()\nst_overlaps()\nst_touches()\n\nUltimately which query or spatial relationship conceptualisation you would choose would depend on the qualifications you are trying to place on your dataset. In our case, considering our london_stations spatial dataframe and our london_stations_osm spatial dataframe, we again have to consider the offset between our datasets. We could, of course, snap our spatial dataframe as above but wouldn’t it be great if we could skip this step?\nTo do so, instead of snapping the london_stations spatial dataframe to the london_stations_osm spatial dataframe, we can use the st_is_within_distance() spatial query to ask whether our points in our london_stations spatial dataframe are within 150m of our london_stations_osm spatial dataframe. This ultimately means we can skip the snapping and st_difference() steps and complete our processing in two simple steps.\n\n\n\n\n\n\nOne thing to be aware of when running spatial queries in R and sf is that whichever spatial dataframe is the comparison geometry (i.e. spatial dataframe y in our queries), this spatial dataframe must be a single geometry (as we saw above in our st_difference() geometric operation). If it is not a single geometry, then the query will be run x number of observations times y spatial dataframe number of observations times, which is not the output that we want. By converting our comparison spatial dataframe to a single geometry, the query is only run for the number of observations in x.\nYou should also be aware that any of our spatial queries will return one of two potential outputs: a list detailing the indexes of all those observation features in x that do intersect with y, or a matrix that contains a TRUE or FALSE statement about this relationship. To define whether we want a list or a matrix output, we set the sparse parameter within our query to TRUE or FALSE respectively.\n\n\n\nQuery whether the points in our london_stations spatial dataframe are within 150m of our london_stations_osm spatial dataframe:\n\n\n\nR code\n\n# create a single geometry\nlondon_stations_osm_compare &lt;- london_stations_osm |&gt;\n    st_union()\n\n# compare the two point geometries\nlondon_stations$in_osm_data &lt;- london_stations |&gt;\n    st_is_within_distance(london_stations_osm_compare, dist = 150, sparse = FALSE)\n\n\nWe can go ahead and count() the results of our query.\n\n\n\nR code\n\n# count\ncount(london_stations, in_osm_data)\n\n\nSimple feature collection with 2 features and 2 fields\nGeometry type: MULTIPOINT\nDimension:     XY\nBounding box:  xmin: 505625.9 ymin: 168579.9 xmax: 556144.8 ymax: 196387.1\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 2 × 3\n  in_osm_data[,1]     n                                                 geometry\n* &lt;lgl&gt;           &lt;int&gt;                                         &lt;MULTIPOINT [m]&gt;\n1 FALSE               3 ((533644.3 188926.1), (537592.5 179814.9), (538301.6 17…\n2 TRUE              283 ((505625.9 184164.2), (507565.2 185008.3), (507584.9 17…\n\n\nGreat - we can see we have 3 stations missing (FALSE observations), just like we had in our geometric operations approach. Now we have done our checks and we know that we can move forward with using our tube and train station file, we should save a copy for usage at a later stage. Save your london_stations_osm spatial dataframe under the transport folder in your raw directory as a shapefile:\n\n\n\nR code\n\n# write to GeoPackage\nst_write(london_stations_osm, \"data/raw/transport/osm_stations.gpkg\")\n\n\n\n\n\n\nWe now have our London bike theft and train stations ready for analysis and we just need to complete one last step of processing with this dataset to find out whether or not bike theft occurs more often near to a train station. As above, we can use both geometric operations or spatial queries to complete this analysis.\n\n\nOur first approach using geometric operations will involve the creation of a buffer around each train station to then identify which bike thefts occur within 400m of a train or tube station.When it comes to buffers, we need to consider two main things: what distance will we use (and are we in the right CRS to use a buffer) and whether we want individual buffers or a single buffer.\n\n\n\n\n\nFigure 10: A single versus multiple buffer. The single buffer represents a dissolved version of the multiple buffer option. [Enlarge image]\n\n\n\n\nIn terms of CRS, we want to make sure we use a CRS that defines its measurement units in metres. If our CRS does not use metres as its measurement unit, it might be in a base unit of an Arc Degree or something else that creates difficulties when converting between a required metre distance and the measurement unit of that CRS. In our case, we are using British National Grid and, luckily for us, the units of the CRS is metres, so we do not need to worry about this.\n\n\n\nR code\n\n# generate a 400m buffer\nstation_400m_buffer &lt;- london_stations_osm |&gt;\n    st_buffer(dist = 400) |&gt;\n    st_union()\n\n\nYou can then go ahead and plot our buffer to see the results, entering plot(station_400m_buffer) within the console:\n\n\n\nR code\n\nplot(station_400m_buffer)\n\n\n\n\n\nFigure 11: Quick plot of the stations with their 400m buffers\n\n\n\n\nTo find out which bike thefts have occurred within 400m of a station, we will use the st_intersects() function.\n\n\n\n\n\n\nOne thing to note is that there is a difference between st_intersects() and the st_intersections() function we have been used so far. Unlike the st_intersections() function which creates a ‘clip’ of our dataset, i.e. produces a new spatial dataframe containing the clipped geometry, the st_intersects() function simply identifies whether “x and y geometry share any space”. As explained above, as with all spatial queries, the st_intersects() function can produce two different outputs: either a list detailing the indexes of all those observation features in x that do intersect with y or a matrix that contains a TRUE or FALSE statement about this relationship. As with our previous spatial query, we will continue to use the matrix approach: this means for every single bike theft in London, we will know whether or not it occurred within our chosen distance of a train station. We can then join this as a new column to our bike_theft spatial dataframe.\n\n\n\nTo detect which bike thefts occur within 400m of a train or tube station, we can do:\n\n\n\nR code\n\n# intersect buffers with thefts\nbike_theft$d400 &lt;- bike_theft |&gt;\n    st_intersects(station_400m_buffer, sparse = FALSE)\n\n\nWe could go ahead and recode this to create a 1 or 0, or YES or NO after processing, but for now we will leave it as TRUE or FALSE. We can go ahead and now visualise our bike_theft based on this column, to see those occurring near to a train station:\n\n\n\nR code\n\n# set tmap back to plot\ntmap_mode(\"plot\")\n\n# add London outline\ntm_shape(london_outline) +\n  tm_borders() +\n  # add bike theft\n  tm_shape(bike_theft) +\n  tm_dots(\n    col = \"d400\",\n    palette = \"BuGn\"\n  ) +\n  # add train stations\n  tm_shape(london_stations_osm) +\n  tm_dots(\n    palette = \"gray\"\n  ) +\n  # add credits\n  tm_credits(\"© OpenStreetMap contributors\")\n\n\n\n\n\nFigure 12: Map of bike thefts in the vicinity of stations.\n\n\n\n\nIt should be of no surprise that visually we can of course see some defined clusters of our points around the various train stations. We can then utilise this resulting dataset to calculate the percentage of bike thefts have occurred at this distance, but first we will look at the spatial query approach to obtaining the same result.\n\n\n\nLike earlier, we can use the st_is_within_distance() function to identify those bike thefts that fall within 400m of a tube or train station in London.\nWe will again need to use the single geometry version of our london_stations_osm spatial dataframe for this comparison with sparse = FALSE to create a matrix that we assign to our bike_theft spatial dataframe as a new column:\n\n\n\nR code\n\n# compare the two point geometries\nbike_theft$d400_sq &lt;- bike_theft |&gt;\n    st_is_within_distance(london_stations_osm_compare, dist = 400, sparse = FALSE)\n\n\nWe can count or map the outputs of our two different approaches to check that we have the same output. If you were to do this you will see that we have achieved the exact same output with fewer lines of code and, as a result, quicker processing. However, unlike with the geometric operations, we do not have a buffer to visualise this distance around a train station, which we might want to do, for example, for maps in a report or presentation. Once again, it will be up to you to determine which approach you prefer to use. Some people prefer using the more visual techniques of geometric operations, whereas others might find spatial queries to answer the same questions.\n\n\n\nNow we have for each bike theft in our bike_theft spatial dataframe an attribute that contains information about whether the theft occurred within 400m of a train or tube station or not. We can use the count() function to find out just how many thefts fall in each of these categories.\n\n\n\nR code\n\n# count thefts within 400m of a station\ncount(bike_theft, d400_sq)\n\n\nSimple feature collection with 2 features and 2 fields\nGeometry type: MULTIPOINT\nDimension:     XY\nBounding box:  xmin: 504839.3 ymin: 158502.4 xmax: 557575.6 ymax: 199969.7\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 2 × 3\n  d400_sq[,1]     n                                                     geometry\n* &lt;lgl&gt;       &lt;int&gt;                                             &lt;MULTIPOINT [m]&gt;\n1 FALSE        9896 ((504937.2 184142.4), (505034.2 182358.6), (505048.2 184104…\n2 TRUE        10633 ((504839.3 175647.3), (504861.2 175885.4), (505339.2 184226…\n\n\nAlmost 50 per cent of bike thefts occur within 400m of a train station. Our main hypothesis that bike thefts occur primarily near train and tube stations is perhaps not quite proven, but so far we have managed to quantify that a substantial amount of bike thefts do occur within 400m of these areas.\nIn a final step, we will conduct a very familiar procedure: aggregating our data to the MSOA level. At the moment, we have now calculated for each bike theft whether or not it occurs within 400m of a tube or train station. We can use this to see if specific MSOAs are hotspots of bike crimes near stations across London. To do this, we will be using the same process we used in in previous weeks: counting the number of points in each of our polygons.\nTo create a point-in-polygon count within sf, we use the st_intersects() function again but instead of using the matrix output of TRUE or FALSE that we have used before, what we actually want to extract from our function is the total number of points it identifies as intersecting with our msoa_london spatial dataframe. To achieve this, we use the lengths() function from the base R package to count the number of wards returned within the index list its sparse output creates.\nRemember, this sparse output creates a list of the bike thefts (by their index) that intersect with each MSOA. The lengths() function will return the length of this list, i.e. how many bike thefts each MSOA contains or, in other words, a point-in-polygon count. This time around therefore we do not set the sparse function to FALSE but leave it as TRUE (its default) by not entering the parameter. As a result, we can calculate the number of bike thefts per MSOA and the number of bike thefts within 400m of a station per MSOA and use this to generate a theft rate for each MSOA of the number of bikes thefts that occur near a train station for identification of these hotspots.\n\n\n\nR code\n\n# point in polygon\nmsoa_london$total_bike_theft &lt;- lengths(st_intersects(msoa_london, bike_theft))\n\n# point in polygon, only thefts within 400m of a station\nmsoa_london$station_bike_theft &lt;- lengths(st_intersects(msoa_london, filter(bike_theft,\n    d400_sq == TRUE)))\n\n\n\n\n\n\n\n\nWe are looking specifically at the phenomena of whether bike theft occurs near to a train or tube station or not. By normalising by the total bike theft, we are creating a rate that shows specifically where there are hotspots of bike theft near train stations. This, however, will be of course influenced by the number of train stations within a MSOA, the size of the MSOA, and of course the number of bikes and potentially daytime and residential populations within an area.\n\n\n\nCalculate the rate of bike theft within 400m of a train or tube station out of all bike thefts for each MSOA:\n\n\n\nR code\n\n# theft rate\nmsoa_london &lt;- msoa_london |&gt;\n    mutate(rate_bike_theft = (station_bike_theft/total_bike_theft) * 100)\n\n\n\n\n\n\n\nNow we worked through all this, for this week’s assignment:\n\nCreate a proper map of the rate of bike thefts within 400m of a train or tube station.\nRe-run the above analysis at four more distances: 100m, 200m, 300m, 500m and calculate the percentage of bike theft at these different distances. You can choose whether you would like to use the geometric operations or spatial queries approach.\nThere are clear differences between using different buffer distances. What do you think could explain the substantial differences in counts as we increase the distance from 100 to 200m?\n\n\n\n\n\n\nThe book Data Skills for Reproducible Research provides an excellent overview of skills needed for reproducible and open research using R and the tidyverse packages. To get started: have a look at Chapter 2: Reproducible Workflows, Chapter 7: Data Wrangling, and Chapter 8: Iterations and Functions.\n\n\n\n\nAnd that is how you can conduct basic geometric operations and spatial queries using R and sf. More RGIS coming in the next few weeks, but this concludes the tutorial for this week. Time to check out that reading list?"
  },
  {
    "objectID": "06-operations.html#slides-w06",
    "href": "06-operations.html#slides-w06",
    "title": "1 Analysing Spatial Patterns I: Geometric Operations and Spatial Queries",
    "section": "",
    "text": "The slides for this week’s lecture can be downloaded here: [Link]"
  },
  {
    "objectID": "06-operations.html#reading-w06",
    "href": "06-operations.html#reading-w06",
    "title": "1 Analysing Spatial Patterns I: Geometric Operations and Spatial Queries",
    "section": "",
    "text": "Lovelace, R., Nowosad, J. and Muenchow, J. 2021. Geocomputation with R, Chapter 4: Spatial data operations. [Link]\nLovelace, R., Nowosad, J. and Muenchow, J. 2021. Geocomputation with R, Chapter 5: Geometry operations. [Link]\nLovelace, R., Nowosad, J. and Muenchow, J. 2021. Geocomputation with R, Chapter 6: Reprojecting geographic data. [Link]\n\n\n\n\n\nHoulden, V. et al. 2019. A spatial analysis of proximate greenspace and mental wellbeing in London. Applied Geography 109: 102036. [Link]\nMalleson, N. and Andresen, M. 2016. Exploring the impact of ambient population measures on London crime hotspots. Journal of Criminal Justice 46: 52-63. [Link]"
  },
  {
    "objectID": "06-operations.html#bike-theft-w06",
    "href": "06-operations.html#bike-theft-w06",
    "title": "1 Analysing Spatial Patterns I: Geometric Operations and Spatial Queries",
    "section": "",
    "text": "This week, we will be investigating bike theft in London in 2021 and look to confirm the hypothesis that bike theft primarily occurs near tube and train stations. We will be investigating its distribution across London using the point data provided within our crime dataset. We will then compare this distribution to the location of train and tube stations using specific geometric operations and spatial queries that can compare the geometry of two (or more) datasets. We will also learn how to download data from OpenStreetMap as well as use an interactive version of tmap to explore the distribution of the locations of individual bike theft against the locations of these stations.\n\n\nOpen a new script within your GEOG0030 project and save this script as wk6-bike-theft-analysis.r. At the top of your script, add the following metadata:\n\n\n\nR code\n\n# Analysing bike theft in London using geometric analysis\n# Date: January 2024\n\n\nNow let us add all of the libraries we will be using today to the top of our script:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\nlibrary(osmdata)\n\n\n\n\n\nThis week, we will start off using three datasets: the London MSOA boundaries for 2021, recorded crime in London for 2021 from data.police.uk, and the locations of the train and tube stations from Transport for London. We already downloaded the crime data for 2021 during Week 4’s computer tutorial and we also saved the 2021 London MSOA boundaries last week, so we only need to download a dataset containing train and tube stations in London.\n\n\n\nFile\nType\nLink\n\n\n\n\nTrain and tube stations in London\nkml\nDownload\n\n\n\nOnce downloaded, move your tfl_stations.kml download to your raw data folder and create a new transport folder to contain it. After this, let’s load our London MSOA file:\n\n\n\nR code\n\n# read in our MSOA GeoPackage\nmsoa_london &lt;- st_read(\"data/raw/boundaries/MSOA2021_London.gpkg\")\n\n\nReading layer `MSOA2021_London' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/raw/boundaries/MSOA2021_London.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1002 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n\n\nCheck the CRS of our msoa_london spatial dataframe:\n\n\n\nR code\n\n# inspect CRS\nst_crs(msoa_london)\n\n\nCoordinate Reference System:\n  User input: OSGB36 / British National Grid \n  wkt:\nPROJCRS[\"OSGB36 / British National Grid\",\n    BASEGEOGCRS[\"OSGB36\",\n        DATUM[\"Ordnance Survey of Great Britain 1936\",\n            ELLIPSOID[\"Airy 1830\",6377563.396,299.3249646,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4277]],\n    CONVERSION[\"British National Grid\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",49,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",-2,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996012717,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",400000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",-100000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"United Kingdom (UK) - offshore to boundary of UKCS within 49°45'N to 61°N and 9°W to 2°E; onshore Great Britain (England, Wales and Scotland). Isle of Man onshore.\"],\n        BBOX[49.75,-9,61.01,2.01]],\n    ID[\"EPSG\",27700]]\n\n\nOf course it should be of no surprise that our msoa_lon spatial dataframe is projected in OSGB36 / British National Grid, however, it is always good to check and avoid problems down the line. Let’s go ahead and read in our tfl_stations dataset as well:\n\n\n\nR code\n\n# load stations\nlondon_stations &lt;- read_sf(\"data/raw/transport/tfl_stations.kml\")\n\n\nThis dataset is provided as a kml file, which stands for Keyhole Markup Language (KML). KML was originally created as a file format used to display geographic data in Google Earth. So we definitely need to check what CRS this dataset is in and decide whether we need to reproject the data.\n\n\n\nR code\n\n# inspect CRS\nst_crs(london_stations)\n\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\nThe result informs us that we are going to need to reproject our data in order to use this dataframe with our msoa_london spatial dataframe. Luckily in R and the sf library, this reprojection is a relatively straightforward with the st_transform() function. The function is very simple to use: you only need to provide the function with the input dataset that you want to reproject and the EPSG code of the target CRS.\n\n\n\nR code\n\n# reproject our data from WGS84 to BNG\nlondon_stations &lt;- st_transform(london_stations, 27700)\n\n\nWe can double-check whether our new variable is in the correct CRS by using the st_crs() function again:\n\n\n\nR code\n\n# inspect CRS\nst_crs(london_stations)\n\n\nCoordinate Reference System:\n  User input: EPSG:27700 \n  wkt:\nPROJCRS[\"OSGB36 / British National Grid\",\n    BASEGEOGCRS[\"OSGB36\",\n        DATUM[\"Ordnance Survey of Great Britain 1936\",\n            ELLIPSOID[\"Airy 1830\",6377563.396,299.3249646,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4277]],\n    CONVERSION[\"British National Grid\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",49,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",-2,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996012717,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",400000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",-100000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"United Kingdom (UK) - offshore to boundary of UKCS within 49°45'N to 61°N and 9°W to 2°E; onshore Great Britain (England, Wales and Scotland). Isle of Man onshore.\"],\n        BBOX[49.75,-9,61.01,2.01]],\n    ID[\"EPSG\",27700]]\n\n\nYou should see that our london_stations spatial dataframe is now in OSGB36 / British National Grid. We are now ready to load our final dataset - our collection of csv's that contain the crime data for London for 2021. We can do this by running the code from Week 4’s computer tutorial:\n\n\n\nR code\n\n# create a list of all csv files in the crime folder\nall_crime_df &lt;- list.files(path = \"data/raw/crime/all-crime/\", full.names = TRUE, recursive = TRUE) |&gt;\n  # apply the read_csv() function on each of these files\n  lapply(read_csv) |&gt;\n  # combine ('bind') them all together into one\n  bind_rows()\n\n\nNow we have loaded all crime data again, we want to do three things:\n\nExtract only those crimes that are bicycle thefts.\nConvert our csv into a spatial dataframe that shows the locations of our crimes, using the recorded latitude and longitudes.\nReproject the latitudes and longitudes from WGS84: EPSG 4326 to British National Grid: EPSG 27700.\n\n\n\n\nR code\n\n# filter all crimes by bicycle thefts only\nbike_theft &lt;- all_crime_df |&gt;\n  # filter according to crime type, filter out crimes with no location data\n  filter(`Crime type` == \"Bicycle theft\" & !is.na(Longitude) & !is.na(Latitude)) |&gt;\n  # only keep the longitude and latitude columns\n  dplyr::select(Longitude, Latitude) |&gt;\n  # transform into a point spatial dataframe, set CRS to WGS84\n  st_as_sf(coords = c(\"Longitude\", \"Latitude\"), crs = 4236) |&gt;\n  # transform into BNG\n  st_transform(27700)\n\n\nLet’s go ahead and save the bike_theft spatial layer as a GeoPackage:\n\n\n\nR code\n\n# write to GeoPackage\nst_write(bike_theft, 'data/data/LondonBikeTheft2021.gpkg')\n\n\nWe now have our three datasets loaded, it is time for a little data inspection. We can see just from our Environment window that in total, we have 302 train and tube stations and 20,768 crimes to look at in our analysis. We can double-check the attributes of our newly created spatial dataframes to see what data we have to work with. You can either do this manually by clicking on the variable, or using commands such as head(), summary() and names() to get an understanding of our dataframe structures and the fieldnames present.\nFor our bicycle theft data, we actually only have a geometry column because this is all that we extracted from our collection of crime csv files. For our london_stations spatial dataframe, we have a little more information, including the name of the station, its address, and as its geometry.\nNow, let’s map all three layers of data onto a single map using tmap:\n\n\n\nR code\n\n# plot our London MSOAs\ntm_shape(msoa_london) +\n  tm_fill() +\n  # then add bike crime\n  tm_shape(bike_theft) +\n  tm_dots(\n    col = \"blue\"\n  ) +\n  # then add stations\n  tm_shape(london_stations) +\n  tm_dots(\n    col = \"red\"\n  )\n\n\n\n\n\nFigure 1: Quick thematic map.\n\n\n\n\nLet’s think about the distribution of our data: we can already see that our bike theft is clearly highly concentrated in the centre of London although we can certainly see some clusters in other areas. Let’s go ahead and temporarily remove the bike theft data from our map for now to see where our tube and train stations are located.\nTo remove the bike data, simply comment out the (#) the relevant line of code:\n\n\n\nR code\n\n# plot our London MSOAs\ntm_shape(msoa_london) +\n  tm_fill() +\n  # then add bike crime\n  tm_shape(bike_theft) +\n  tm_dots(\n    col = \"blue\"\n  ) +\n  # then add stations\n  tm_shape(london_stations) +\n  tm_dots(\n    col = \"red\"\n  )\n\n\n\n\n\nFigure 2: Quick thematic map.\n\n\n\n\nWe can see our train and tube stations are only present in primarily the north of London and not really present in the south. This is not quite right and in fact it seems that our dataset only contains those train stations used by Transport for London within the underground network rather than all the stations in London. We will need to fix this before conducting our full analysis. A second issue with our dataset is that the london_stations spatial dataframe extends beyond our London boundaries. The same applies to our bike_theft spatial dataframe.\n\n\n\nWhen we want to reduce a dataset to the spatial extent of another, there are two different approaches: a subset or a clip. Each deal with the geometry of the resulting dataset in slightly different ways.\n\nA subset operation is what is known in GIScience-speak as a select by location query. In this case, our subset will return the full geometry of each observation feature of the input layer that intersects with our second layer. Any geometry that does not intersect with our second layer will be removed from the geometry of our resulting layer.\nA clip works a bit like a cookie-cutter: it will take the geometry of the input layer (i.e. the layer you want to clip), places a ‘cookie-cutter’ layer on top (i.e. the layer you want to clip by) and then returns only the parts of the input layer contained within the cookie-cutter. This will mean that the geometry of our resulting layer will be modified, if it contains observation features that extend further than the ‘cookie-cutter’ extent it will literally ‘cut’ the geometry of our data.\n\n\n\n\n\n\n\nBecause we are using point data, we can use either approach because it is not possible to split the geometry of a single point feature. When it comes to polygon and line data, the different approaches are likely to result in different results.\n\n\n\nEach approach is implemented differently in R. To subset our data, we only need to use the base R library to selection using [] brackets:\n\n\n\nR code\n\n# subset\nbike_theft &lt;- bike_theft[msoa_london, ]\nlondon_stations &lt;- london_stations[msoa_london, ]\n\n\nIf we want to clip our data, we need to use the st_intersection() function from the sf library.\n\n\n\nR code\n\n# clip\nbike_theft &lt;- bike_theft |&gt;\n    st_intersection(msoa_london)\n\n\nWarning: attribute variables are assumed to be spatially constant throughout all\ngeometries\n\nlondon_stations &lt;- london_stations |&gt;\n    st_intersection(msoa_london)\n\nWarning: attribute variables are assumed to be spatially constant throughout all\ngeometries\n\n\n\n\n\n\n\n\nOut of the two, the subset approach is the fastest to use as R is simply comparing the geometries rather than also editing the geometries, but which approach you use with future data is always dependent on your data and the output you need.\n\n\n\nBefore we go ahead and sort out our london_stations spatial dataframe, we are going to look at how we can dissolve our msoa_london spatial dataframe into a single feature. Reducing a spatial dataframe to a single observation is often required when using R and sf’s geometric operations to complete geometric comparisons. In addition, sometimes we simply want to map an outline of an area, such as London, rather than add in the additional spatial complexities of internal boundaries. To achieve just a single observation that represents the outline geometry of our dataset, we use the geometric operation st_union().\n\n\n\n\n\n\nYou can also use the st_union() function to combine two spatial datasets into one. This can be used to merge data together that are of the same spatial type. E.g. if you have a file that contains the MSOAs pertaining to England and a file that contains the MSOAs pertaining to Wales, you can use st_union to create one file that contains the MSOAs for both countries.\n\n\n\n\n\n\nR code\n\n# union\nlondon_outline &lt;- msoa_london |&gt;\n    st_union()\n\n\nYou should see that our london_outline spatial data frame only has one observation. You can now go ahead and plot() your london_outline spatial dataframe from your console and see what it looks like:\n\n\n\nR code\n\nplot(london_outline)\n\n\n\n\n\nFigure 3: Quick plot of the London outline.\n\n\n\n\n\n\n\nBack to our train and tube stations. We have seen that our current london_stations spatial dataframe does not provide the coverage of train stations in London that we expected. To add in our missing data, we will be using OpenStreetMap.\n\n\n\n\n\n\nOpenStreetMap (OSM) is a free editable map of the world,although its spatial coverage is still unequal across the world. In addition, as you will find if you use the data, the accuracy and quality of the data can often be quite questionable or simply missing attribute details that we would like to have, e.g. types of roads and their speed limits, to complete specific types of spatial analysis. As a result, do not expect OSM to contain every piece of spatial data that you would want.\n\n\n\nWhilst there are various approaches to downloading data from OpenStreetMap, we will use the osmdata library to directly extract our required OpenStreetMap (OSM) data into a variable. The osmdata library grants access within R to the Overpass API that allows us to run queries on OSM data and then import the data as sf object. These queries are at the heart of these data downloads.\nTo use the library (and API), we need to know how to write and run a query, which requires identifying the key and value that we need within our query to select the correct data. Essentially every map element (whether a point, line or polygon) in OSM is tagged with different attribute data. In our case, we are looking for train stations, which fall under the key, Public Transport, with a value of station as outlined in their wiki. These keys and values are used in our queries to extract only map elements of that feature type - to find out how a feature is tagged in OSM is simply a case of reading through the OSM documentation and becoming familiar with their keys and values.\nIn addition to this key-value pair, we also need to obtain the bounding box of where we want our data to be extracted from to limit the search to the specific area of interest. Let’s try to extract elements from OSM that are tagged as public_transport = station from OSM into an osmdata_sf() object:\n\n\n\nR code\n\n# extract latitude, longitude bounding box from London outline\np_bbox &lt;- st_bbox(st_transform(london_outline, 4326))\n\n# call OverPassQuery function\nlondon_stations_osm &lt;- opq(bbox = p_bbox) |&gt;\n  # add key, values\n  add_osm_feature(key = \"public_transport\", value = \"station\") |&gt;\n  # to sf\n  osmdata_sf()\n\n\n\n\n\n\n\n\nIn some instances the OSM query will return an error, especially when several people from the same location are executing the exact same query at the same time. If that is the case you can download the london_stations_osm object here: [Download]. After downloading, you can copy the file to your working directory and load the object using the load() function.\n\n\n\nWhen we download OSM data, and extract it as above, our query will return all elements tagged as our key-value pair into our osmdata_sf() OSM data object. This means all elements associated with our tag will be returned: any points, lines and polygons. We might think with our public_transport = station tag, we would only return point data representing our train and tube stations in London. But if we use the summary() function on our london_stations_osm OSM data object, we can see that not only a lot of other data is stored in our OSM data object (including the bounding box we used within our query, plus metadata about our query), but our query has also returned both points and polygons stored within this OSM data object as individual spatial data frames.\nTo extract only the points of our tube and train stations from our london_stations_osm OSM data object, we simply need to extract this from the dataframe and store this under a separate variable.\n\n\n\nR code\n\n# extract only points\nlondon_stations_osm &lt;- london_stations_osm$osm_points |&gt;\n  # add projection information\n  st_set_crs(4326) |&gt;\n  # reproject\n  st_transform(27700) |&gt;\n  # clip\n  st_intersection(london_outline) |&gt;\n  # select relevant attributes |&gt;\n  dplyr::select(c(\"osm_id\", \"name\", \"network\", \"operator\", \"public_transport\", \"railway\"))\n\n\nWarning: attribute variables are assumed to be spatially constant throughout all\ngeometries\n\n# inspect\nplot(london_stations_osm)\n\n\n\n\nFigure 4: Points marked as stations extracted from OpenStreetMap.\n\n\n\n\nWith the accuracy of OSM a little questionable, we want to complete some data validation tasks to check its quality and to confirm that it at least contains the data we see in our authoritative london_stations spatial dataframe. The total number of data points also seems rather high. In fact, a quick search online can tell us that there are 272 tube stations in the London network as well as 339 train stations in Greater London.\nAs we can see in our plot above, not all of our stations appear to be of the same value in our railway field. If we check the field using our count() function, you will see that there are some different values and NAs in our dataset:\n\n\n\nR code\n\n# count\ncount(london_stations_osm, railway)\n\n\nSimple feature collection with 7 features and 2 fields\nGeometry type: MULTIPOINT\nDimension:     XY\nBounding box:  xmin: 505078.2 ymin: 159027.2 xmax: 556185.7 ymax: 200138.6\nProjected CRS: OSGB36 / British National Grid\n           railway    n                       geometry\n1         entrance    2 MULTIPOINT ((532814.8 16572...\n2 railway_crossing    2 MULTIPOINT ((523304.2 17881...\n3          station  608 MULTIPOINT ((505078.2 17673...\n4             stop   25 MULTIPOINT ((513225.1 18452...\n5  subway_entrance   44 MULTIPOINT ((513239 184507....\n6           switch    9 MULTIPOINT ((523304.3 17879...\n7             &lt;NA&gt; 4304 MULTIPOINT ((505595.9 18418...\n\n\nAs we can see, not every feature in our london_stations_osm spatial dataframe is recorded as a station and we have a high number of NAs which are unlikely to represent actual stations. The number of points marked as station in the railway field are most likely the only points in our dataset that represent actual stations. There is still a difference between the official numbers and the OSM extract, but we will go on use this information to clean up the OSM dataset:\n\n\n\nR code\n\n# extract train and tube stations\nlondon_stations_osm &lt;- london_stations_osm |&gt;\n    filter(railway == \"station\")\n\n\nWe have now cleaned our london_stations_osm spatial dataframe to remove all those points within our dataset that are not tagged as railway == \"station\". Our london_stations spatial dataframe is of course an authoritative dataset from TfL, so we hope that this data is accurate, albeit incomplete, and at very least does not contain stations that do not exist. Therefore, it would be helpful if we could compare our two datasets to one another spatially to double-check that our london_stations_osm spatial dataframe contains all the data found within our london_stations spatial dataframe. We can first look at this by comparing their distributions visually on a map.\n\n\n\nR code\n\n# add London outline\ntm_shape(london_outline) +\n  tm_fill() +\n  # add OSM station data\n  tm_shape(london_stations_osm) +\n  tm_dots(\n    col = \"black\"\n  ) +\n  # add TfL station data\n  tm_shape(london_stations) +\n  tm_dots(\n    col = \"red\"\n  ) +\n  # add north arrow\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"bottom\")\n  ) +\n  # add scale bar\n  tm_scale_bar(\n    breaks = c(0, 5, 10, 15, 20),\n    position = c(\"left\", \"bottom\")\n  ) +\n  # add credits\n  tm_credits(\"© OpenStreetMap contributors\")\n\n\n\n\n\nFigure 5: Map of TfL and OSM stations.\n\n\n\n\nWhat we can see is that it looks like our OSM data actual does a much better job at covering all train and tube stations across London but still it is pretty hard to get a sense of comparison from a static map like this whether it contains all of the tube and train stations in our london_stations spatial dataframe. An interactive map would enable us to interrogate the spatial coverage of our two station spatial dataframes further. To do so, we use the tmap_mode() function and change it from its default plot() mode to a view() model:\n\n\n\nR code\n\n# change tmap mode to interactive\ntmap_mode(\"view\")\n\n# add London outline\ntm_shape(london_outline) +\n  tm_borders() +\n  # add OSM station data\n  tm_shape(london_stations_osm) +\n  tm_dots(\n    col = \"black\"\n  ) +\n  # add TfL station data\n  tm_shape(london_stations) +\n  tm_dots(\n    col = \"red\"\n  ) +\n  # add basemap\n  tm_basemap(c(StreetMap = \"OpenStreetMap\"))\n\n\n\n\n\n\nFigure 6: Interactive map of TfL and OSM stations.\n\n\n\nUsing the interactive map, what we can see is that whilst we do have overlap with our datasets, and more importantly, our london_stations_osm spatial dataframe seems to contain all of the data within the london_stations spatial dataframe, although there are definitely differences in their precise location. Now depending on what level of accuracy we are willing to accept with our assumption that our OSM data contains the same data as our Transport for London data, we could leave our comparison here and move forward with our analysis. There are, however, several more steps we could complete to validate this assumption. The easiest first step is to simply reverse the order of our datasets to check that each london_stations spatial dataframe point is covered by reversing the drawing order:\n\n\n\nR code\n\n# add London outline\ntm_shape(london_outline) +\n  tm_borders() +\n  # add TfL station data\n  tm_shape(london_stations) +\n  tm_dots(\n    col = \"red\"\n  ) +\n  # add OSM station data\n  tm_shape(london_stations_osm) +\n  tm_dots(\n    col = \"black\"\n  ) +\n  # add basemap\n  tm_basemap(c(StreetMap = \"OpenStreetMap\"))\n\n\n\n\n\n\nFigure 7: Interactive map of TfL and OSM stations.\n\n\n\n\n\n\nThe comparison looks pretty good but still the question is: can we be sure? Using geometric operations and spatial queries, we can look to find if any of our stations in our london_stations spatial dataframe are not present the london_stations_osm spatial dataframe. We can use specific geometric operations and/or queries that let us check whether or not all points within our london_stations spatial dataframe spatially intersect with our london_stations_osm spatial dataframe, i.e. we can complete the opposite of the clip/intersection that we conducted earlier. The issue we face, however is that, as we saw above, our points are slightly offset from one another as the datasets have ultimately given the same stations slightly different locations. This offset means we need to think a little about the geometric operation or spatial query that we want to use.\nWe will approach this question in two different ways to highlight the differences between geometric operations and spatial queries:\n\nWe will use geometric operations to generate geometries that highlight missing stations from our london_stations spatial dataframe (i.e. ones that are not present in thelondon_stations_osm spatial dataframe).\nWe will use spatial queries to provide us with a list of features in our london_stations spatial dataframe that do not meet our spatial requirements (i.e. are not present in thelondon_stations_osm spatial dataframe).\n\n\n\nAs highlighted above, the offset between our spatial dataframes adds a little complexity to our geometric operations code. To be able to make our direct spatial comparisons across our spatial dataframes, what we first need to do is try to snap the geometry of our london_stations spatial dataframe to our london_stations_osm spatial dataframe for points within a given distance threshold. This will mean that any points in the london_stations spatial dataframe that are within a specific distance of the london_stations_osm spatial dataframe will have their geometry changed to that of the london_stations_osm spatial dataframe.\n\n\n\n\n\nFigure 8: Snapping points to a line. In our case we snap our points to other points. [Enlarge image]\n\n\n\n\nBy placing a threshold on this snap, we stop too many points moving about if they are unlikely to be representing the same station (e.g. further than 150m or so away) but this still allows us to create more uniformity across our datasets’ geometries (and tries to reduce the uncertainty we add by completing this process).\nSnap our our london_stations spatial dataframe to our london_stations_osm spatial dataframe for points within a 150m distance threshold:\n\n\n\nR code\n\n# snap points\nlondon_stations_snap &lt;- london_stations |&gt;\n    st_snap(london_stations_osm, 150)\n\n\nNow we have out snapped geometry, we can look to compare our two datasets to calculate whether or not our london_stations_osm spatial dataframe is missing any data from our london_stations_snap spatial dataframe. To do so, we will use the st_difference() function which will return us the geometries of those points in our london_stations_snap spatial dataframe that are missing in our our london_stations_osm spatial dataframe. However, to use this function successfully we need to convert our our london_stations_osm spatial dataframe into a single geometry first. To simplify our london_stations_osm spatial dataframe into a single geometry, we simply use the st_union() code we used with our London outline above:\n\n\n\nR code\n\n# create a single geometry\nlondon_stations_osm_compare &lt;- london_stations_osm |&gt;\n    st_union()\n\n# compare the geometries\nmissing_stations &lt;- st_difference(london_stations_snap, london_stations_osm_compare)\n\n\nWarning: attribute variables are assumed to be spatially constant throughout all\ngeometries\n\n\nYou should now find that we apparently have 3 missing stations in our london_stations_osm spatial dataframe. We can plot these missing stations against our london_stations_osm spatial dataframe and confirm whether these stations are indeed missing or not.\n\n\n\nR code\n\n# add OSM station data\ntm_shape(london_stations_osm) +\n  tm_dots(\n    col = \"black\"\n  ) +\n  # add missing station data\n  tm_shape(missing_stations) +\n  tm_dots(\n    col = \"green\"\n  ) +\n  # add basemap\n  tm_basemap(c(StreetMap = \"OpenStreetMap\"))\n\n\n\n\n\n\nFigure 9: Interactive map of TfL and OSM stations after the snapping operation.\n\n\n\nWhen you investigate the missing stations, you can actually see that our london_stations_osm spatial dataframe dataset is actually more accurate than the TfL locations. All ‘missing’ stations are not in fact missing but simply at a greater offset than 150m. We can safely suggest that we can move forward with only using the london_stations_osm spatial dataframe and do not need to follow through with adding any more data to this dataset. Before we move on, let’s save the data as a GeoPackage.\n\n\n\nR code\n\n# write to GeoPackage\nst_write(london_stations_osm, \"data/data/LondonStations.gpkg\")\n\n\n\n\n\nBefore we go ahead and move forward with our analysis, we will have a look at how we can implement the above quantification using spatial queries instead of geometric operations. Usually, when we want to find out if two spatial dataframes have the same or similar geometries, we would use one of the following queries:\n\nst_equals()\nst_intersects()\nst_crosses()\nst_overlaps()\nst_touches()\n\nUltimately which query or spatial relationship conceptualisation you would choose would depend on the qualifications you are trying to place on your dataset. In our case, considering our london_stations spatial dataframe and our london_stations_osm spatial dataframe, we again have to consider the offset between our datasets. We could, of course, snap our spatial dataframe as above but wouldn’t it be great if we could skip this step?\nTo do so, instead of snapping the london_stations spatial dataframe to the london_stations_osm spatial dataframe, we can use the st_is_within_distance() spatial query to ask whether our points in our london_stations spatial dataframe are within 150m of our london_stations_osm spatial dataframe. This ultimately means we can skip the snapping and st_difference() steps and complete our processing in two simple steps.\n\n\n\n\n\n\nOne thing to be aware of when running spatial queries in R and sf is that whichever spatial dataframe is the comparison geometry (i.e. spatial dataframe y in our queries), this spatial dataframe must be a single geometry (as we saw above in our st_difference() geometric operation). If it is not a single geometry, then the query will be run x number of observations times y spatial dataframe number of observations times, which is not the output that we want. By converting our comparison spatial dataframe to a single geometry, the query is only run for the number of observations in x.\nYou should also be aware that any of our spatial queries will return one of two potential outputs: a list detailing the indexes of all those observation features in x that do intersect with y, or a matrix that contains a TRUE or FALSE statement about this relationship. To define whether we want a list or a matrix output, we set the sparse parameter within our query to TRUE or FALSE respectively.\n\n\n\nQuery whether the points in our london_stations spatial dataframe are within 150m of our london_stations_osm spatial dataframe:\n\n\n\nR code\n\n# create a single geometry\nlondon_stations_osm_compare &lt;- london_stations_osm |&gt;\n    st_union()\n\n# compare the two point geometries\nlondon_stations$in_osm_data &lt;- london_stations |&gt;\n    st_is_within_distance(london_stations_osm_compare, dist = 150, sparse = FALSE)\n\n\nWe can go ahead and count() the results of our query.\n\n\n\nR code\n\n# count\ncount(london_stations, in_osm_data)\n\n\nSimple feature collection with 2 features and 2 fields\nGeometry type: MULTIPOINT\nDimension:     XY\nBounding box:  xmin: 505625.9 ymin: 168579.9 xmax: 556144.8 ymax: 196387.1\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 2 × 3\n  in_osm_data[,1]     n                                                 geometry\n* &lt;lgl&gt;           &lt;int&gt;                                         &lt;MULTIPOINT [m]&gt;\n1 FALSE               3 ((533644.3 188926.1), (537592.5 179814.9), (538301.6 17…\n2 TRUE              283 ((505625.9 184164.2), (507565.2 185008.3), (507584.9 17…\n\n\nGreat - we can see we have 3 stations missing (FALSE observations), just like we had in our geometric operations approach. Now we have done our checks and we know that we can move forward with using our tube and train station file, we should save a copy for usage at a later stage. Save your london_stations_osm spatial dataframe under the transport folder in your raw directory as a shapefile:\n\n\n\nR code\n\n# write to GeoPackage\nst_write(london_stations_osm, \"data/raw/transport/osm_stations.gpkg\")\n\n\n\n\n\n\nWe now have our London bike theft and train stations ready for analysis and we just need to complete one last step of processing with this dataset to find out whether or not bike theft occurs more often near to a train station. As above, we can use both geometric operations or spatial queries to complete this analysis.\n\n\nOur first approach using geometric operations will involve the creation of a buffer around each train station to then identify which bike thefts occur within 400m of a train or tube station.When it comes to buffers, we need to consider two main things: what distance will we use (and are we in the right CRS to use a buffer) and whether we want individual buffers or a single buffer.\n\n\n\n\n\nFigure 10: A single versus multiple buffer. The single buffer represents a dissolved version of the multiple buffer option. [Enlarge image]\n\n\n\n\nIn terms of CRS, we want to make sure we use a CRS that defines its measurement units in metres. If our CRS does not use metres as its measurement unit, it might be in a base unit of an Arc Degree or something else that creates difficulties when converting between a required metre distance and the measurement unit of that CRS. In our case, we are using British National Grid and, luckily for us, the units of the CRS is metres, so we do not need to worry about this.\n\n\n\nR code\n\n# generate a 400m buffer\nstation_400m_buffer &lt;- london_stations_osm |&gt;\n    st_buffer(dist = 400) |&gt;\n    st_union()\n\n\nYou can then go ahead and plot our buffer to see the results, entering plot(station_400m_buffer) within the console:\n\n\n\nR code\n\nplot(station_400m_buffer)\n\n\n\n\n\nFigure 11: Quick plot of the stations with their 400m buffers\n\n\n\n\nTo find out which bike thefts have occurred within 400m of a station, we will use the st_intersects() function.\n\n\n\n\n\n\nOne thing to note is that there is a difference between st_intersects() and the st_intersections() function we have been used so far. Unlike the st_intersections() function which creates a ‘clip’ of our dataset, i.e. produces a new spatial dataframe containing the clipped geometry, the st_intersects() function simply identifies whether “x and y geometry share any space”. As explained above, as with all spatial queries, the st_intersects() function can produce two different outputs: either a list detailing the indexes of all those observation features in x that do intersect with y or a matrix that contains a TRUE or FALSE statement about this relationship. As with our previous spatial query, we will continue to use the matrix approach: this means for every single bike theft in London, we will know whether or not it occurred within our chosen distance of a train station. We can then join this as a new column to our bike_theft spatial dataframe.\n\n\n\nTo detect which bike thefts occur within 400m of a train or tube station, we can do:\n\n\n\nR code\n\n# intersect buffers with thefts\nbike_theft$d400 &lt;- bike_theft |&gt;\n    st_intersects(station_400m_buffer, sparse = FALSE)\n\n\nWe could go ahead and recode this to create a 1 or 0, or YES or NO after processing, but for now we will leave it as TRUE or FALSE. We can go ahead and now visualise our bike_theft based on this column, to see those occurring near to a train station:\n\n\n\nR code\n\n# set tmap back to plot\ntmap_mode(\"plot\")\n\n# add London outline\ntm_shape(london_outline) +\n  tm_borders() +\n  # add bike theft\n  tm_shape(bike_theft) +\n  tm_dots(\n    col = \"d400\",\n    palette = \"BuGn\"\n  ) +\n  # add train stations\n  tm_shape(london_stations_osm) +\n  tm_dots(\n    palette = \"gray\"\n  ) +\n  # add credits\n  tm_credits(\"© OpenStreetMap contributors\")\n\n\n\n\n\nFigure 12: Map of bike thefts in the vicinity of stations.\n\n\n\n\nIt should be of no surprise that visually we can of course see some defined clusters of our points around the various train stations. We can then utilise this resulting dataset to calculate the percentage of bike thefts have occurred at this distance, but first we will look at the spatial query approach to obtaining the same result.\n\n\n\nLike earlier, we can use the st_is_within_distance() function to identify those bike thefts that fall within 400m of a tube or train station in London.\nWe will again need to use the single geometry version of our london_stations_osm spatial dataframe for this comparison with sparse = FALSE to create a matrix that we assign to our bike_theft spatial dataframe as a new column:\n\n\n\nR code\n\n# compare the two point geometries\nbike_theft$d400_sq &lt;- bike_theft |&gt;\n    st_is_within_distance(london_stations_osm_compare, dist = 400, sparse = FALSE)\n\n\nWe can count or map the outputs of our two different approaches to check that we have the same output. If you were to do this you will see that we have achieved the exact same output with fewer lines of code and, as a result, quicker processing. However, unlike with the geometric operations, we do not have a buffer to visualise this distance around a train station, which we might want to do, for example, for maps in a report or presentation. Once again, it will be up to you to determine which approach you prefer to use. Some people prefer using the more visual techniques of geometric operations, whereas others might find spatial queries to answer the same questions.\n\n\n\nNow we have for each bike theft in our bike_theft spatial dataframe an attribute that contains information about whether the theft occurred within 400m of a train or tube station or not. We can use the count() function to find out just how many thefts fall in each of these categories.\n\n\n\nR code\n\n# count thefts within 400m of a station\ncount(bike_theft, d400_sq)\n\n\nSimple feature collection with 2 features and 2 fields\nGeometry type: MULTIPOINT\nDimension:     XY\nBounding box:  xmin: 504839.3 ymin: 158502.4 xmax: 557575.6 ymax: 199969.7\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 2 × 3\n  d400_sq[,1]     n                                                     geometry\n* &lt;lgl&gt;       &lt;int&gt;                                             &lt;MULTIPOINT [m]&gt;\n1 FALSE        9896 ((504937.2 184142.4), (505034.2 182358.6), (505048.2 184104…\n2 TRUE        10633 ((504839.3 175647.3), (504861.2 175885.4), (505339.2 184226…\n\n\nAlmost 50 per cent of bike thefts occur within 400m of a train station. Our main hypothesis that bike thefts occur primarily near train and tube stations is perhaps not quite proven, but so far we have managed to quantify that a substantial amount of bike thefts do occur within 400m of these areas.\nIn a final step, we will conduct a very familiar procedure: aggregating our data to the MSOA level. At the moment, we have now calculated for each bike theft whether or not it occurs within 400m of a tube or train station. We can use this to see if specific MSOAs are hotspots of bike crimes near stations across London. To do this, we will be using the same process we used in in previous weeks: counting the number of points in each of our polygons.\nTo create a point-in-polygon count within sf, we use the st_intersects() function again but instead of using the matrix output of TRUE or FALSE that we have used before, what we actually want to extract from our function is the total number of points it identifies as intersecting with our msoa_london spatial dataframe. To achieve this, we use the lengths() function from the base R package to count the number of wards returned within the index list its sparse output creates.\nRemember, this sparse output creates a list of the bike thefts (by their index) that intersect with each MSOA. The lengths() function will return the length of this list, i.e. how many bike thefts each MSOA contains or, in other words, a point-in-polygon count. This time around therefore we do not set the sparse function to FALSE but leave it as TRUE (its default) by not entering the parameter. As a result, we can calculate the number of bike thefts per MSOA and the number of bike thefts within 400m of a station per MSOA and use this to generate a theft rate for each MSOA of the number of bikes thefts that occur near a train station for identification of these hotspots.\n\n\n\nR code\n\n# point in polygon\nmsoa_london$total_bike_theft &lt;- lengths(st_intersects(msoa_london, bike_theft))\n\n# point in polygon, only thefts within 400m of a station\nmsoa_london$station_bike_theft &lt;- lengths(st_intersects(msoa_london, filter(bike_theft,\n    d400_sq == TRUE)))\n\n\n\n\n\n\n\n\nWe are looking specifically at the phenomena of whether bike theft occurs near to a train or tube station or not. By normalising by the total bike theft, we are creating a rate that shows specifically where there are hotspots of bike theft near train stations. This, however, will be of course influenced by the number of train stations within a MSOA, the size of the MSOA, and of course the number of bikes and potentially daytime and residential populations within an area.\n\n\n\nCalculate the rate of bike theft within 400m of a train or tube station out of all bike thefts for each MSOA:\n\n\n\nR code\n\n# theft rate\nmsoa_london &lt;- msoa_london |&gt;\n    mutate(rate_bike_theft = (station_bike_theft/total_bike_theft) * 100)"
  },
  {
    "objectID": "06-operations.html#assignment-w06",
    "href": "06-operations.html#assignment-w06",
    "title": "1 Analysing Spatial Patterns I: Geometric Operations and Spatial Queries",
    "section": "",
    "text": "Now we worked through all this, for this week’s assignment:\n\nCreate a proper map of the rate of bike thefts within 400m of a train or tube station.\nRe-run the above analysis at four more distances: 100m, 200m, 300m, 500m and calculate the percentage of bike theft at these different distances. You can choose whether you would like to use the geometric operations or spatial queries approach.\nThere are clear differences between using different buffer distances. What do you think could explain the substantial differences in counts as we increase the distance from 100 to 200m?"
  },
  {
    "objectID": "06-operations.html#wm-w06",
    "href": "06-operations.html#wm-w06",
    "title": "1 Analysing Spatial Patterns I: Geometric Operations and Spatial Queries",
    "section": "",
    "text": "The book Data Skills for Reproducible Research provides an excellent overview of skills needed for reproducible and open research using R and the tidyverse packages. To get started: have a look at Chapter 2: Reproducible Workflows, Chapter 7: Data Wrangling, and Chapter 8: Iterations and Functions."
  },
  {
    "objectID": "06-operations.html#byl-w06",
    "href": "06-operations.html#byl-w06",
    "title": "1 Analysing Spatial Patterns I: Geometric Operations and Spatial Queries",
    "section": "",
    "text": "And that is how you can conduct basic geometric operations and spatial queries using R and sf. More RGIS coming in the next few weeks, but this concludes the tutorial for this week. Time to check out that reading list?"
  },
  {
    "objectID": "07-point-pattern.html",
    "href": "07-point-pattern.html",
    "title": "1 Analysing Spatial Patterns II: Point Pattern Analysis",
    "section": "",
    "text": "This week, we will be looking at Point Pattern Analysis. With point pattern analysis, we look to detect clusters or patterns across a set of points, including measuring density, dispersion and homogeneity in our point structures. There are several approaches to calculating and detecting these clusters and today we explore several of these techniques using our bike theft dataset from last week.\n\n\nThe slides for this week’s lecture can be downloaded here: [Link]\n\n\n\n\n\n\nArribas-Bel, D., Garcia-López, M.-À., Viladecans-Marsal, E. 2021. Building(s and) cities: Delineating urban areas with a machine learning algorithm. Journal of Urban Economics 125: 103217. [Link]\nCheshire, J. and Longley, P. 2011. Identifying spatial concentrations of surnames. International Journal of Geographical Information Science 26(2), pp.309-325. [Link]\nLongley, P. et al. 2015. Geographic Information Science & systems, Chapter 12: Geovisualization. [Link]\n\n\n\n\n\nVan Dijk, J. and Longley, P. 2020. Interactive display of surnames distributions in historic and contemporary Great Britain. Journal of Maps 16, pp.58-76. [Link]\nShi, X. 2010. Selection of bandwidth type and adjustment side in kernel density estimation over inhomogeneous backgrounds. International Journal of Geographical Information Science 24(5), pp.643-660. [Link]\nYin, P. 2020. Kernels and density estimation. The Geographic Information Science & Technology Body of Knowledge. [Link]\n\n\n\n\n\nThis week, we again investigate bike theft in London in 2021 as we continue to look to confirm our very simple hypothesis: that bike theft cluster in space. This week, instead of looking at the distance of individual bike thefts from train stations, we will look to analyse the distribution of clusters and identify hotspots of bike theft.\n\n\nOpen a new script within your GEOG0030 project and save this script as wk7-bike-theft-ppa.r. At the top of your script, add the following metadata:\n\n\n\nR code\n\n# Analysing bike theft in London using point pattern analysis\n# Date: January 2024\n\n\nAll of the geometric operations and spatial queries we will use are contained within the sf library. For our Point Pattern Analysis, we will be using the spatstat library (“spatial statistics”). The spatstat library contains the different Point Pattern Analysis techniques we will want to use in this practical. We will also need the terra library, which provides classes and functions to manipulate geographic (spatial) data in raster format. We will use this package briefly today, but look into it in more detail in Week 9. Lastly, you will also need to load the dbscan package. Now let us add all of the libraries we will be using today to the top of our script:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\nlibrary(spatstat)\nlibrary(terra)\nlibrary(dbscan)\n\n\n\n\n\nThis week, we will continue to use the data we extracted from OpenStreetMap last week as well as the 2021 bike theft data that we prepared. Let’s go ahead and load all of our data at once. We will also load the MSOA\n\n\n\nR code\n\n# read in our MSOA GeoPackage, create outline\nmsoa_london &lt;- st_read(\"data/raw/boundaries/MSOA2021_London.gpkg\")\n\n\nReading layer `MSOA2021_London' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/raw/boundaries/MSOA2021_London.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1002 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n\noutline_london &lt;- msoa_london |&gt;\n    st_union()\n\n# read in OSM tube and trains stations\nstations_london &lt;- st_read(\"data/data/LondonStations.gpkg\")\n\nReading layer `LondonStations' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/data/LondonStations.gpkg' \n  using driver `GPKG'\nSimple feature collection with 608 features and 6 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 505078.2 ymin: 159027.2 xmax: 556185.7 ymax: 200138.6\nProjected CRS: OSGB36 / British National Grid\n\n# read in bike theft\nbike_theft_london &lt;- st_read(\"data/data/LondonBikeTheft2021.gpkg\")\n\nReading layer `LondonBikeTheft2021' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/data/LondonBikeTheft2021.gpkg' \n  using driver `GPKG'\nSimple feature collection with 20768 features and 0 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 458978.5 ymin: 99463.06 xmax: 559591.3 ymax: 300738.7\nProjected CRS: OSGB36 / British National Grid\n\n\nLet’s create a quick map of our data to check it loaded correctly:\n\n\n\nR code\n\n# plot our London MSOAs\ntm_shape(outline_london) +\n  tm_fill() +\n  # then add bike crime as blue\n  tm_shape(bike_theft_london) +\n  tm_dots(\n    col = \"blue\"\n  ) +\n  # then add our stations as red\n  tm_shape(stations_london) +\n  tm_dots(\n    col = \"red\"\n  ) +\n  # then add a north arrow\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"bottom\")\n  ) +\n  # then add a scale bar\n  tm_scale_bar(\n    breaks = c(0, 5, 10, 15, 20),\n    position = c(\"left\", \"bottom\")\n  )\n\n\n\n\n\nFigure 1: Quick plot to see whether all data loaded correctly.\n\n\n\n\nGreat - that looks familiar! This means we can move forward with our data analysis and theoretical content for this week.\n\n\n\n\nPoint pattern analysis (PPA) studies the spatial distribution of points. PPA uses the density, dispersion and homogeneity in our point datasets to assess, quantify and characterise its distribution. Over the last fifty years, various methods and measurements have been developed to analyze, model, visualise, and interpret these properties of point patterns (Qiang et al, 2020).\nThere are three main categories of PPA techniques:\n\nDescriptive statistics: The use of descriptive statistics will provide a summary of the basic characteristics of a point pattern, such as its central tendency and dispersion. Descriptive statistics provide a simple way of visualising a dataset as a whole, from plotting the median or mean centre, or, often preferably, a standard deviational ellipse for those datasets that display a directional pattern.\nDensity-based methods: Density-based methods focus on the first-order properties of a dataset, i.e. the variation in the individual locations of the points in the dataset across the area of interest, and will characterise our dataset’s distribution accordingly in terms of density.\nDistanced-based methods: Distanced-based methods focus on the second-order properties of a dataset, i.e. the interactions between points within our data and whether they appear to have influence on one another and form clusters, and will characterise our dataset’s distribution accordingly in terms of dispersion.\n\nThe main library to use when it comes to point pattern analysis in R is the spatstat library, developed by Baddeley, Rubak and Turner since 2005. As their documentation states:\n\n“Spatstat is a package for the statistical analysis of spatial data. Its main focus is the analysis of spatial patterns of points in two-dimensional space”.\n\n\nAccording to the Get Started with spatstat documentation spatstat supports a very wide range of popular techniques for statistical analysis for spatial point patterns, including:\n\nKernel estimation of density/intensity\n\nQuadrat counting and clustering indices\nDetection of clustering using Ripley’s K-function\nModel-fitting\nMonte Carlo tests\n\nWe will only cover a small amount of the functionality the package offers - it has almost 1,800 pages of documentation and over 1,000 functions, so it would be near impossible to cover everything even if we had a full module dedicated just to PPA.\n\nBefore we get started with our analysis, you need to know one critical piece of information in order to use spatstat: we need our data to be in the format of a ppp object. There are some spatial packages in R that require us to convert our data from an sf simple features object (e.g. for point data, a SpatialPoints object) into a different spatial object class. spatstat is one of them.\n\n\n\n\n\n\nThe ppp format is specific to spatstat, but you may find it used in other spatial libraries. An object of the class ppp represents a two-dimensional point dataset within a pre-defined area, known as the window of observation, a class in its own right, known as owin in spatstat. We can either directly create a ppp object from a list of coordinates (as long as they are supplied with a window of observation) or convert from another data type.\n\n\n\nLet’s turn our bike_theft_london dataframe into a ppp object:\n\n\n\nR code\n\n# clip bike theft to the London outline\nbike_theft_london &lt;- bike_theft_london |&gt;\n    st_intersection(outline_london)\n\n# sf to ppp\nwindow = as.owin(outline_london)\nbike_theft_ppp &lt;- ppp(st_coordinates(bike_theft_london)[, 1], st_coordinates(bike_theft_london)[,\n    2], window = window)\n\n\nWarning: data contain duplicated points\n\n# inspect\nplot(bike_theft_ppp)\n\n\n\n\nFigure 2: Bike theft in London represented as ppp object.\n\n\n\n\nOur plot shows us our bike_theft_ppp object, which includes both the coordinate points of our bike theft data and our observation window. You should also see your bike_theft_ppp object variable appear in your Environment window. You should also see a message stating that our data contains duplicated points.\nOne of the key assumptions underlying many analytical methods is that all events are unique. In fact, some statistical procedures actually may return very wrong results if duplicate points are found within the data. In terms of our bike theft data, it is likely that it contains duplicates. The Police service use snapping points, to which crimes are snapped to in order to preserve the anonymity and privacy of those involved. This is an issue in spatial point pattern analysis as we need our events, i.e. each record of a theft and its respective location, to be unique in order for our analysis to be accurate. Let’s investigate by how many duplicated points we have:\n\n\n\nR code\n\n# check for any duplicates\nanyDuplicated(bike_theft_ppp)\n\n\n[1] TRUE\n\n# count number of duplicated points\nsum(multiplicity(bike_theft_ppp) &gt; 1)\n\n[1] 13730\n\n\n\n\n\n\n\n\nWe clearly have a large number of duplicated points. To account for these issues within our dataset, we have three options:\n\nWe can remove the duplicates and pretend they simply are not there. However, this is feasible only when your research problem allows for this, i.e. the number of points at each location is not as important as the locations themselves or the number of duplicated points is incredibly small and will not affect any analysis.\nCreate and assign a weighting schema to our points, where each point will have an attribute that details the number of events that occur in that location and utilise this weight within our PPA techniques. Weights, however, can only be used with certain PPA techniques.\nForce all points to be unique by utilising a function that offsets our points randomly from their current location. If the precise location is not important for your analysis or, for example, you are dealing with data that in our case is already slightly offset, we can introduce a “jitter” to our dataset that slightly adjusts all coordinates so that the event locations do not exactly coincide anymore.\n\n\n\n\nEach approach will have a specific compromise, which you will have to decide upon depending on the type of analysis you are completing. In our case, we will choose the jitter approach to keep all of our bike theft events. We know that already the location of our bike thefts are not precise locations of the original theft, therefore adding minimal (~5 metres) additional offset will not affect our analysis.\n\n\n\nR code\n\n# add an offset to our points\nbike_theft_ppp_jitter &lt;- rjitter(bike_theft_ppp, radius = 5, retry = TRUE, nsim = 1,\n    drop = TRUE)\n\n# check for any duplicates\nanyDuplicated(bike_theft_ppp_jitter)\n\n\n[1] FALSE\n\n# count number of duplicated points\nsum(multiplicity(bike_theft_ppp_jitter) &gt; 1)\n\n[1] 0\n\n\nGreat, we now have our bike theft data in a format ready to be analysed with our different PPA techniques using the spatstat library.\n\n\n\n\n\n\nOne additional thing to note about the ppp data object is that a ppp object does not necessarily have to have any attributes associated with the events each point our point data represents. If your data does have attributes, these attributes are referred to as marks within the spatstat environment. Be aware that some functions do require these marks to be present.\n\n\n\n\n\n\nDensity-based techniques are used to characterise the pattern of a point dataset utilising its general distribution. We can calculate densities at both the global and local scale. However, global densities do not tell us much about the distribution of our data. This is where local density techniques such as Quadrat Analysis and Kernel Density Estimation can help us.\n\n\nWe can create a simple understanding of our data’s distribution by first understanding its global density. This is simply the ratio of the observed number of points \\((n\\) to the study region’s surface area \\(a\\):\n\\[\\widehat{\\lambda} =\\frac{n}{a}\\]\nCalculate the global density of our bike theft point data relative to London:\n\n\n\nR code\n\n# global density\nlength(bike_theft_london$geom)/sum(st_area(msoa_london)) * 1000\n\n\n0.01304807 [1/m^2]\n\n\nWe can see that we have a global density of ~13 bike thefts per square kilometre. This simple density analysis could be supported with further descriptive statistics, however, we still would know little about the local density of our points. The most basic approach to understanding a point layer’s local density is to simply measure the density at different locations within the study area. This approach helps us assess if the density is constant across the study area. The simplest approach to this measurement is through Quadrat Analysis, where the study area is divided into sub-regions. The point density is then computed for each quadrat, by dividing the number of points in each quadrat by the quadrat’s area.\n\n\n\n\n\n\nQuadrats can take on many different shapes such as hexagons or rectangles. The standard approach, however, is a grid of squares The choice of quadrat numbers and quadrat shape can influence the measure of local density and therefore must be chosen with care.\n\n\n\nWe will start with a simple quadrat count by dividing the observation window into 10 x 10 sections and then counting the number of bicycle thefts:\n\n\n\nR code\n\n# quadratcount\nbiketheft_quadrat &lt;- quadratcount(bike_theft_ppp_jitter, nx = 10, ny = 10)\n\n# inspect\nplot(biketheft_quadrat)\n\n\n\n\n\nFigure 3: Quadratcount for our bike theft point dataset.\n\n\n\n\nOur resulting quadrat count shows total counts of bike theft. We can see quite quickly that the quadrats in central London are likely to have a higher local density as their count is much higher than those on the outskirts of London. If we divided our count by the area covered by each quadrat, we would also be able to calculate a precise local density. We will not do this for now, as realistically quadrat analysis is not used very often. The reason why we look at this technique is that it provides us with an easy way to think about how to compare our data distribution and how this relates to the Poisson distribution of Complete Spatial Randomness (CSR).\n\n\n\n\n\n\nWhen looking at the distribution of our points and the respective patterns they show, the key question we often want to answer as geographers is: are our points clustered, randomly distributed, uniform or dispersed? Whilst we can visually assess this distribution, to be able to statistically quantify our data’s distribution, we can compare its distribution to that of the Poisson distribution. The Poisson distribution describes the probability or rate of an event happening over a fixed interval of time or space.\nThe Poisson Distribution can be used when:\n\nThe events are discrete and can be counted in integers\nEvents are independent of one another\nThe average number of events over space or time is known\n\nPoint data that contains a random distribution of points is said to follow a Poisson distribution. The Poisson distribution is very useful as it allows us to compare a random expected model to our observations. Essentially, if our data’s distribution does not fit the Poisson distribution, then we can infer that something interesting might be going on and our events might not actually be independent of each other. Instead, they might be clustered or dispersed and there is likely to be underlying processes influencing these patterns.\n\n\n\nWe can use a Poisson distribution to generate a randomly generated point pattern dataset with the same number of points and the same observation window. We then can compare our quadrat analysis with the theoretical quadrat analysis using a chi-squared test, with the null hypotheses that our point data are randomly distributed.\n\n\n\nR code\n\n# quadrat test\nquadrat.test(bike_theft_ppp_jitter, nx = 10, ny = 10)\n\n\nWarning: Some expected counts are small; chi^2 approximation may be inaccurate\n\n\n\n    Chi-squared test of CSR using quadrat counts\n\ndata:  bike_theft_ppp_jitter\nX2 = 55562, df = 80, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\nQuadrats: 81 tiles (irregular windows)\n\n\nOur \\(p\\) value is well below 0.01, which means there is a statistically significant difference between the expected random distribution as drawn from a Poisson distribution and the observed distribution.\n\n\n\nWe now have a basic confirmation that our data is not randomly distributed. Instead of looking at the distribution of our bike theft with the boundaries of our quadrats, we can analyse our points using a Kernel Density Estimation (KDE). KDE is a statistical technique to generate a smooth continuous distribution between data points that represent the density of the underlying pattern.\n\n\n\n\n\n\nA KDE will produce a raster surface that details the estimated distribution of our event point data over space. Each cell within our raster contains a value that is this estimated density at that location; when visualised in its entirety as the whole raster, we can quickly identify areas of high and low density, i.e. where are clusters are located in our dataset. To create this surface, a KDE computes a localised density for small subsets of our study area but unlike quadrat analysis, these subsets overlap one another to create a moving sub-region window, defined by a kernel. A kernel defines the shape and size of the window and can also weight the points, using a defined kernel function. The simplest kernel function is a basic kernel where each point in the kernel window is assigned equal weight. The kernel density approach generates a grid of density values whose cell size is smaller than that of the kernel window. Each cell is assigned the density value computed for the kernel window centered on that cell. The resulting surface is created from these individually, locally calculated density values.\n\n\n\nProducing a KDE in R is very straight-forward in spatstat, using your ppp object and the density.ppp() function. However, you will need to consider both the bandwidth or diameter of your Kernel (sigma) and whether you want to apply a weighting to your points using a function. First, let’s go ahead and create a simple KDE of bike theft with our bandwidth set to 100m:\n\n\n\nR code\n\n# kernel density estimation\nplot(density.ppp(bike_theft_ppp_jitter, sigma = 100))\n\n\n\n\n\nFigure 4: Kernel density estimation - bandwidth 100m.\n\n\n\n\nWe can see from just our KDE that there are visible clusters present within our bike theft data, particularly in and around central London. We can go ahead and vary our bandwidth to to see how that affects the density estimate:\n\n\n\nR code\n\n# kernel density estimation\nplot(density.ppp(bike_theft_ppp_jitter, sigma = 500))\n\n\n\n\n\nFigure 5: Kernel density estimation - bandwidth 500m.\n\n\n\n\nOur clusters now appear brighter and larger than our KDE with a 100m bandwidth. This is because changing the bandwidth enables your KDE to take into account more points within its calculation, resulting in a smoother surface. However, there are issues with oversmoothing your data - as you can see above, our clusters are not as well defined and therefore we may attribute high levels of bike theft to areas where there actually is not that much. Smaller bandwidths will lead to a more irregular shaped surface, where we have more precision in our defined clusters but, once again, there are issues of undersmoothing.\n\n\n\n\n\n\nWhilst there are automated functions (e.g. based on maximum-likelihood estimations) that can help you with selecting an appropriate bandwidth, in the end you will have to make a decision on what is most appropriate for your dataset.\n\n\n\nAlthough bandwidth typically has a more pronounced effect upon the density estimation than the type of kernel used, kernel types can affect the result too. When we use a different kernel type, we are looking to weight the points within our kernel differently:\n\n\n\n\n\nFigure 6: Kernel types and their distributions. [Enlarge image]\n\n\n\n\nEach function will result in [a slightly different estimation. Deciding which function is most suitable for your analysis will all depend on what you are trying to capture. We can compare and see the impact of different functions on our current dataset looking at the default kernel in density.ppp(), which gaussian, alongside the epanechnikov, quartic or disc kernels.\n\n\n\n\n\n\nTo change the kernel within your KDE, you simply need to add the kernel parameter and set it to one of the kernels available, denoted as a string, e.g. epanechnikov, quartic, disc. Ultimately, however, bandwidth will have a more marked effect upon the density estimation than kernel type.\n\n\n\n\n\n\nR code\n\n# kernel density estimation\nplot(density.ppp(bike_theft_ppp_jitter, sigma = 400, kernel = \"gaussian\"), main = \"Gaussian\")\n\n\n\n\n\nFigure 7: Kernel density estimation - bandwidth 400m, Gaussian kernel.\n\n\n\n\n\n\n\nR code\n\n# kernel density estimation\nplot(density.ppp(bike_theft_ppp_jitter, sigma = 400, kernel = \"epanechnikov\"), main = \"Epanechnikov\")\n\n\n\n\n\nFigure 8: Kernel density estimation - bandwidth 400m, Epanechnikov kernel.\n\n\n\n\n\n\n\nR code\n\n# kernel density estimation\nplot(density.ppp(bike_theft_ppp_jitter, sigma = 400, kernel = \"quartic\"), main = \"Quartic\")\n\n\n\n\n\nFigure 9: Kernel density estimation - bandwidth 400m, Quartic kernel.\n\n\n\n\n\n\n\nR code\n\n# kernel density estimation\nplot(density.ppp(bike_theft_ppp_jitter, sigma = 400, kernel = \"disc\"), main = \"Disc\")\n\n\n\n\n\nFigure 10: Kernel density estimation - bandwidth 400m, Disc kernel.\n\n\n\n\nWe can be quite confident in stating that bike theft in London in 2021 is not a spatially random process and we can clearly see the areas where bicycle theft is most concentrated. How can we use this new data in our original analysis that looks to find out whether bike theft primarily occurs near tube and train stations? KDEs are primarily used for visual analysis of point data distribution. What we can do, however, is improve our visualisation and making it into a proper map using tmap.\n\n\n\nR code\n\n# to raster, clip by London outline\nkde_400g_raster &lt;- density.ppp(bike_theft_ppp_jitter, sigma = 400) |&gt;\n    rast()\n\n\nWe now have a standalone raster we can use with any function in the tmap library. Before we go ahead, one issue we will face is that our resulting raster does not have a Coordinate Reference System, so we need to manually add this information to the raster object:\n\n\n\nR code\n\n# check CRS\nst_crs(kde_400g_raster)\n\n\nCoordinate Reference System: NA\n\n\nYou should see an NA appear within our CRS arguments, so we need to fix this:\n\n\n\nR code\n\n# add CRS\ncrs(kde_400g_raster) &lt;- \"epsg:27700\"\n\n# check CRS\nst_crs(kde_400g_raster)\n\n\nCoordinate Reference System:\n  User input: OSGB36 / British National Grid \n  wkt:\nPROJCRS[\"OSGB36 / British National Grid\",\n    BASEGEOGCRS[\"OSGB36\",\n        DATUM[\"Ordnance Survey of Great Britain 1936\",\n            ELLIPSOID[\"Airy 1830\",6377563.396,299.3249646,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4277]],\n    CONVERSION[\"British National Grid\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",49,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",-2,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996012717,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",400000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",-100000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"United Kingdom (UK) - offshore to boundary of UKCS within 49°45'N to 61°N and 9°W to 2°E; onshore Great Britain (England, Wales and Scotland). Isle of Man onshore.\"],\n        BBOX[49.75,-9,61.01,2.01]],\n    ID[\"EPSG\",27700]]\n\n\nNow we have our raster data ready to map:\n\n\n\nR code\n\n# map kde raster\ntm_shape(kde_400g_raster) +\n  tm_raster(\n    col = \"lyr.1\",\n    palette = \"Blues\"\n  )\n\n\n\n\n\nFigure 11: Basic map using raster data using the tmap library.\n\n\n\n\nWith our kde_400g_raster using tmap, we can go ahead and customise our map as we would do with our vector mapping such as changing the legend, adding a title, north arrow, etc. — and, of course, our stations_london spatial layer.\n\n\n\n\nWe have spent a good amount of time looking at using density-based methods to quantify whether our point data is randomly distributed and visualise and identify high and low areas of density, showing if our data is clustered and where. Distance-based methods, on the other hands, allow us to quantify the second order properties of our data, i.e. the influence the location of these points have on one another.\n\n\n\n\n\n\nDistance-based measures analyse the spatial distribution of points using distances between point pairs, with the majority using Euclidean distance, to determine quantitatively whether our data is, again, randomly distributed or shows sign of clustering or dispersion. These methods are a more rigorous alternative to using the Quadrat Analysis approach, and enables us to assess clustering within our point data at both a global and local scale. For the remainder of this practical, we look at one distance-based measure (average nearest neighbour) as well as a well-known hybrid technique (DBSCAN)\n\n\n\n\n\nAverage Nearest Neighbour (ANN) is the average distance between all points within a dataset and their individual nearest point. ANN is used as a global indicator to measure the overall pattern of a point set. The ANN of a given point collection can be compared with the expected ANN from points following a random distribution to test whether our point data is clustered or dispersed. The approach is similar to that of the Quadrat Analysis simulation we saw above, but by using distance rather than density grouped to arbitrary quadrats, ANN is likely to be a more robust quantification of our point distribution.\nWe can calculate the ANN for our dataset by using the nndist() function from the spatstat library:\n\n\n\nR code\n\n# calculate the average distance to nearest neighbour\nmean(nndist(bike_theft_ppp_jitter, k = 1))\n\n\n[1] 53.4103\n\n\nWe can see that the average nearest neighbour for all points is 53.5 metres. To get an insight into the spatial ordering of all our points relative to one another we can plot the ANN values for different order neighbours (i.e. first closest point, second closest point, etc. For point patterns that are highly clustered, we would expect the average distances between points to be very small.\nCalculate the average nearest neighbour to the \\(k\\) nearest neighbours for our bike theft data:\n\n\n\nR code\n\n# calculate the average distance to the nearest 100 neighbours\nbike_theft_ann &lt;- apply(nndist(bike_theft_ppp_jitter, k = 1:100), 2, FUN = mean)\n\n# plot\nplot(bike_theft_ann ~ seq(1:100))\n\n\n\n\n\nFigure 12: Average distances to the nearest 100 neighbours.\n\n\n\n\nIn our case, the plot does not reveal anything interesting in particular except that higher order points seem to be slightly closer than lower order points. Overall, the ANN is a good approach to conduct statistical tests on large datasets (e.g. if we were to compare the result to a theoretical distribution), but visually it does not tell us a huge amount about our dataset!\n\n\n\nThe techniques above are useful exploratory techniques for telling us if we have spatial clusters present in our point data, but they are not able to tell us precisely where in our area of interest the clusters are occurring. One popular technique for discovering precise clusters in space is an algorithm known as DBSCAN, an algorithm that incorporates both distance and density.\n\n\n\n\n\n\nFor the complete overview of the DBSCAN algorithm, you can refer to the original paper by Ester et al. (1996). Whilst DBSCAN is a relatively old algorithm, there has been a substantial resurgence in its use within spatial data science (e.g. see Arribas-Bel et al. 2019).\n\n\n\nWe can use DBSCAN to detect clusters within our bike theft dataset and then use the clusters to further answer our original research question. DBSCAN takes two parameters:\n\nThe minimum number of points: MinPts.\nThe distance: epsilon.\n\nAcross a set of points, DBSCAN will group together points that are close to each other based on a distance measurement and a minimum number of points. It also marks as outliers the points that are in low-density regions. The algorithm can be used to find associations and structures in data that are hard to find through visual observation alone, but that can be relevant and useful to find patterns and predict trends. However, DBSCAN will only work well if you are able to successfully define the distance and minimum points parameters and your clusters do not vary considerably in density.\nWe can conduct a DBSCAN analysis using the dbscan library. For our analysis, we will set our epsilon to 200m and then set our minimum cluster size to 20 bike thefts.\n\n\n\nR code\n\n# run dbscan\nbike_theft_dbscan &lt;- bike_theft_london |&gt;\n    st_coordinates() |&gt;\n    dbscan(eps = 200, minPts = 20)\n\n\n\n\n\n\n\n\nThe dbscan() function only accepts a data matrix or dataframe of points as input rather than a spatial dataframe. This is why in the above code we pass the st_coordinates() function to extract the projected coordinates.\n\n\n\nThe DBSCAN output contains three objects, including a vector detailing the cluster for each of our bike theft observations. To be able to work with our DBSCAN output effectively and, for example, plot the clusters as individual polygons, we need to add our cluster groups back to into our original point dataset. Because the DBSCAN output does not change the order of points, we can simply stick the cluster output on the bike_theft_london spatial dataframe.\n\n\n\nR code\n\n# add the cluster numbers as column\nbike_theft_london &lt;- bike_theft_london |&gt;\n    mutate(dbcluster = bike_theft_dbscan$cluster)\n\n\nNow we have each of our bike theft points in London associated with a specific cluster, we can generate a polygon that represents these clusters in space, as we can sort of see in our above plot. To do so, we will utilise a geometric operation from the sf package: the st_convex_hull() function. The st_convex_hull() function can be used to create a polygon that represents the minimum coverage of our individual clusters. We can use this function to create a polygon that represents the geometry of all points within each cluster. To enable this, we will use something called a for loop. A for loop is used to repeat a specific block of code a known number of times.\n\n\n\n\n\n\nThe code below is a bit complex, but essentially we repeat the following steps for each cluster that the DBSCAN algorithm identified:\n\nFilter the bike_theft_london spatial dataframe by cluster number.\nCombine all points belonging to the same cluster into a single set of geometry observations.\nCalculate the convex hull of that single set of geometry observations.\nStore the resulting polygon in a list.\n\nOnce we have looped through all clusters, we end up with a final list containing all the geometries of our cluster polygons which we can convert into a spatial dataframe.\n\n\n\nRun a for loop to generate a polygon dataset that represents our bike theft clusters:\n\n\n\nR code\n\n# create an empty list to store the resulting convex hull geometries set the\n# length of this list to the total number of clusters found\ngeometry_list &lt;- vector(mode = \"list\", length = max(bike_theft_london$dbcluster))\n\n# create a counter to keep track\ncounter &lt;- 0\n\n# begin loop\nfor (cluster_index in seq(0, max(bike_theft_london$dbcluster))) {\n\n    # filter to only return points for belonging to cluster n\n    biketheft_cluster_subset &lt;- bike_theft_london |&gt;\n        filter(dbcluster == cluster_index)\n\n    # union points, calculate convex hull\n    cluster_polygon &lt;- biketheft_cluster_subset |&gt;\n        st_union() |&gt;\n        st_convex_hull()\n\n    # add the geometry of the polygon to our list\n    geometry_list[counter] &lt;- (cluster_polygon)\n\n    # update the counter\n    counter &lt;- counter + 1\n}\n\n# combine the list\nbike_theft_clusters &lt;- st_sfc(geometry_list, crs = 27700)\n\n\nWe now have a polygon spatial dataframe, bike_theft_clusters, that show the general location and distribution of bike theft clusters in London. Let’s put them on a map:\n\n\n\nR code\n\n# add London outline\ntm_shape(outline_london) +\n  tm_borders() +\n  # add bike theft clusters\n  tm_shape(bike_theft_clusters) +\n  tm_polygons() +\n  # add basemap\n  tm_basemap(c(StreetMap = \"OpenStreetMap\"))\n\n\n\n\n\nFigure 13: Basic map of identified DBSCAN bike theft clusters.\n\n\n\n\n\n\n\n\nWe have conducted a lot of analysis today. What we have failed to do, however, is to make proper maps. For this week’s assignment, create two publishable maps with all the trimmings using the main outputs from today’s tutorial:\n\nA Kernel Density Map of bike theft in London.\nA cluster map of bike theft in London, using the DBSCAN output.\n\n\n\n\n\n\n\n\nBecause the thefts concentrate in central London why not consider zooming into a specific area?\nWhy not contextualise the densities and clusters by including the stations_london layer?\n\n\n\n\n\n\n\n\n\nLast week you might have had a look at the book Data Skills for Reproducible Research. A great tool that can be used to create reproducible workflows is found in the targets package. The targets package is a Make-like pipeline tool for Statistics and data science in R. With targets, you can maintain a reproducible workflow without repeating yourself. To get started: have a look at the Walkthrough chapter to see targets in action.\n\n\n\n\nAs geographers we are keen to understand our point data’s distribution and understand whether are our points clustered, randomly distributed, uniform or dispersed. We have looked at various techniques that enable us to statistically and visually assess our data’s distribution and understand whether our data is randomly distributed or clustered in space. And that is us done for this week. Reading list anyone?"
  },
  {
    "objectID": "07-point-pattern.html#slides-w07",
    "href": "07-point-pattern.html#slides-w07",
    "title": "1 Analysing Spatial Patterns II: Point Pattern Analysis",
    "section": "",
    "text": "The slides for this week’s lecture can be downloaded here: [Link]"
  },
  {
    "objectID": "07-point-pattern.html#reading-w07",
    "href": "07-point-pattern.html#reading-w07",
    "title": "1 Analysing Spatial Patterns II: Point Pattern Analysis",
    "section": "",
    "text": "Arribas-Bel, D., Garcia-López, M.-À., Viladecans-Marsal, E. 2021. Building(s and) cities: Delineating urban areas with a machine learning algorithm. Journal of Urban Economics 125: 103217. [Link]\nCheshire, J. and Longley, P. 2011. Identifying spatial concentrations of surnames. International Journal of Geographical Information Science 26(2), pp.309-325. [Link]\nLongley, P. et al. 2015. Geographic Information Science & systems, Chapter 12: Geovisualization. [Link]\n\n\n\n\n\nVan Dijk, J. and Longley, P. 2020. Interactive display of surnames distributions in historic and contemporary Great Britain. Journal of Maps 16, pp.58-76. [Link]\nShi, X. 2010. Selection of bandwidth type and adjustment side in kernel density estimation over inhomogeneous backgrounds. International Journal of Geographical Information Science 24(5), pp.643-660. [Link]\nYin, P. 2020. Kernels and density estimation. The Geographic Information Science & Technology Body of Knowledge. [Link]"
  },
  {
    "objectID": "07-point-pattern.html#bike-theft-w07",
    "href": "07-point-pattern.html#bike-theft-w07",
    "title": "1 Analysing Spatial Patterns II: Point Pattern Analysis",
    "section": "",
    "text": "This week, we again investigate bike theft in London in 2021 as we continue to look to confirm our very simple hypothesis: that bike theft cluster in space. This week, instead of looking at the distance of individual bike thefts from train stations, we will look to analyse the distribution of clusters and identify hotspots of bike theft.\n\n\nOpen a new script within your GEOG0030 project and save this script as wk7-bike-theft-ppa.r. At the top of your script, add the following metadata:\n\n\n\nR code\n\n# Analysing bike theft in London using point pattern analysis\n# Date: January 2024\n\n\nAll of the geometric operations and spatial queries we will use are contained within the sf library. For our Point Pattern Analysis, we will be using the spatstat library (“spatial statistics”). The spatstat library contains the different Point Pattern Analysis techniques we will want to use in this practical. We will also need the terra library, which provides classes and functions to manipulate geographic (spatial) data in raster format. We will use this package briefly today, but look into it in more detail in Week 9. Lastly, you will also need to load the dbscan package. Now let us add all of the libraries we will be using today to the top of our script:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\nlibrary(spatstat)\nlibrary(terra)\nlibrary(dbscan)\n\n\n\n\n\nThis week, we will continue to use the data we extracted from OpenStreetMap last week as well as the 2021 bike theft data that we prepared. Let’s go ahead and load all of our data at once. We will also load the MSOA\n\n\n\nR code\n\n# read in our MSOA GeoPackage, create outline\nmsoa_london &lt;- st_read(\"data/raw/boundaries/MSOA2021_London.gpkg\")\n\n\nReading layer `MSOA2021_London' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/raw/boundaries/MSOA2021_London.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1002 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n\noutline_london &lt;- msoa_london |&gt;\n    st_union()\n\n# read in OSM tube and trains stations\nstations_london &lt;- st_read(\"data/data/LondonStations.gpkg\")\n\nReading layer `LondonStations' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/data/LondonStations.gpkg' \n  using driver `GPKG'\nSimple feature collection with 608 features and 6 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 505078.2 ymin: 159027.2 xmax: 556185.7 ymax: 200138.6\nProjected CRS: OSGB36 / British National Grid\n\n# read in bike theft\nbike_theft_london &lt;- st_read(\"data/data/LondonBikeTheft2021.gpkg\")\n\nReading layer `LondonBikeTheft2021' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/data/LondonBikeTheft2021.gpkg' \n  using driver `GPKG'\nSimple feature collection with 20768 features and 0 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 458978.5 ymin: 99463.06 xmax: 559591.3 ymax: 300738.7\nProjected CRS: OSGB36 / British National Grid\n\n\nLet’s create a quick map of our data to check it loaded correctly:\n\n\n\nR code\n\n# plot our London MSOAs\ntm_shape(outline_london) +\n  tm_fill() +\n  # then add bike crime as blue\n  tm_shape(bike_theft_london) +\n  tm_dots(\n    col = \"blue\"\n  ) +\n  # then add our stations as red\n  tm_shape(stations_london) +\n  tm_dots(\n    col = \"red\"\n  ) +\n  # then add a north arrow\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"bottom\")\n  ) +\n  # then add a scale bar\n  tm_scale_bar(\n    breaks = c(0, 5, 10, 15, 20),\n    position = c(\"left\", \"bottom\")\n  )\n\n\n\n\n\nFigure 1: Quick plot to see whether all data loaded correctly.\n\n\n\n\nGreat - that looks familiar! This means we can move forward with our data analysis and theoretical content for this week."
  },
  {
    "objectID": "07-point-pattern.html#point-pattern-analysis",
    "href": "07-point-pattern.html#point-pattern-analysis",
    "title": "1 Analysing Spatial Patterns II: Point Pattern Analysis",
    "section": "",
    "text": "Point pattern analysis (PPA) studies the spatial distribution of points. PPA uses the density, dispersion and homogeneity in our point datasets to assess, quantify and characterise its distribution. Over the last fifty years, various methods and measurements have been developed to analyze, model, visualise, and interpret these properties of point patterns (Qiang et al, 2020).\nThere are three main categories of PPA techniques:\n\nDescriptive statistics: The use of descriptive statistics will provide a summary of the basic characteristics of a point pattern, such as its central tendency and dispersion. Descriptive statistics provide a simple way of visualising a dataset as a whole, from plotting the median or mean centre, or, often preferably, a standard deviational ellipse for those datasets that display a directional pattern.\nDensity-based methods: Density-based methods focus on the first-order properties of a dataset, i.e. the variation in the individual locations of the points in the dataset across the area of interest, and will characterise our dataset’s distribution accordingly in terms of density.\nDistanced-based methods: Distanced-based methods focus on the second-order properties of a dataset, i.e. the interactions between points within our data and whether they appear to have influence on one another and form clusters, and will characterise our dataset’s distribution accordingly in terms of dispersion.\n\nThe main library to use when it comes to point pattern analysis in R is the spatstat library, developed by Baddeley, Rubak and Turner since 2005. As their documentation states:\n\n“Spatstat is a package for the statistical analysis of spatial data. Its main focus is the analysis of spatial patterns of points in two-dimensional space”.\n\n\nAccording to the Get Started with spatstat documentation spatstat supports a very wide range of popular techniques for statistical analysis for spatial point patterns, including:\n\nKernel estimation of density/intensity\n\nQuadrat counting and clustering indices\nDetection of clustering using Ripley’s K-function\nModel-fitting\nMonte Carlo tests\n\nWe will only cover a small amount of the functionality the package offers - it has almost 1,800 pages of documentation and over 1,000 functions, so it would be near impossible to cover everything even if we had a full module dedicated just to PPA.\n\nBefore we get started with our analysis, you need to know one critical piece of information in order to use spatstat: we need our data to be in the format of a ppp object. There are some spatial packages in R that require us to convert our data from an sf simple features object (e.g. for point data, a SpatialPoints object) into a different spatial object class. spatstat is one of them.\n\n\n\n\n\n\nThe ppp format is specific to spatstat, but you may find it used in other spatial libraries. An object of the class ppp represents a two-dimensional point dataset within a pre-defined area, known as the window of observation, a class in its own right, known as owin in spatstat. We can either directly create a ppp object from a list of coordinates (as long as they are supplied with a window of observation) or convert from another data type.\n\n\n\nLet’s turn our bike_theft_london dataframe into a ppp object:\n\n\n\nR code\n\n# clip bike theft to the London outline\nbike_theft_london &lt;- bike_theft_london |&gt;\n    st_intersection(outline_london)\n\n# sf to ppp\nwindow = as.owin(outline_london)\nbike_theft_ppp &lt;- ppp(st_coordinates(bike_theft_london)[, 1], st_coordinates(bike_theft_london)[,\n    2], window = window)\n\n\nWarning: data contain duplicated points\n\n# inspect\nplot(bike_theft_ppp)\n\n\n\n\nFigure 2: Bike theft in London represented as ppp object.\n\n\n\n\nOur plot shows us our bike_theft_ppp object, which includes both the coordinate points of our bike theft data and our observation window. You should also see your bike_theft_ppp object variable appear in your Environment window. You should also see a message stating that our data contains duplicated points.\nOne of the key assumptions underlying many analytical methods is that all events are unique. In fact, some statistical procedures actually may return very wrong results if duplicate points are found within the data. In terms of our bike theft data, it is likely that it contains duplicates. The Police service use snapping points, to which crimes are snapped to in order to preserve the anonymity and privacy of those involved. This is an issue in spatial point pattern analysis as we need our events, i.e. each record of a theft and its respective location, to be unique in order for our analysis to be accurate. Let’s investigate by how many duplicated points we have:\n\n\n\nR code\n\n# check for any duplicates\nanyDuplicated(bike_theft_ppp)\n\n\n[1] TRUE\n\n# count number of duplicated points\nsum(multiplicity(bike_theft_ppp) &gt; 1)\n\n[1] 13730\n\n\n\n\n\n\n\n\nWe clearly have a large number of duplicated points. To account for these issues within our dataset, we have three options:\n\nWe can remove the duplicates and pretend they simply are not there. However, this is feasible only when your research problem allows for this, i.e. the number of points at each location is not as important as the locations themselves or the number of duplicated points is incredibly small and will not affect any analysis.\nCreate and assign a weighting schema to our points, where each point will have an attribute that details the number of events that occur in that location and utilise this weight within our PPA techniques. Weights, however, can only be used with certain PPA techniques.\nForce all points to be unique by utilising a function that offsets our points randomly from their current location. If the precise location is not important for your analysis or, for example, you are dealing with data that in our case is already slightly offset, we can introduce a “jitter” to our dataset that slightly adjusts all coordinates so that the event locations do not exactly coincide anymore.\n\n\n\n\nEach approach will have a specific compromise, which you will have to decide upon depending on the type of analysis you are completing. In our case, we will choose the jitter approach to keep all of our bike theft events. We know that already the location of our bike thefts are not precise locations of the original theft, therefore adding minimal (~5 metres) additional offset will not affect our analysis.\n\n\n\nR code\n\n# add an offset to our points\nbike_theft_ppp_jitter &lt;- rjitter(bike_theft_ppp, radius = 5, retry = TRUE, nsim = 1,\n    drop = TRUE)\n\n# check for any duplicates\nanyDuplicated(bike_theft_ppp_jitter)\n\n\n[1] FALSE\n\n# count number of duplicated points\nsum(multiplicity(bike_theft_ppp_jitter) &gt; 1)\n\n[1] 0\n\n\nGreat, we now have our bike theft data in a format ready to be analysed with our different PPA techniques using the spatstat library.\n\n\n\n\n\n\nOne additional thing to note about the ppp data object is that a ppp object does not necessarily have to have any attributes associated with the events each point our point data represents. If your data does have attributes, these attributes are referred to as marks within the spatstat environment. Be aware that some functions do require these marks to be present."
  },
  {
    "objectID": "07-point-pattern.html#density-based-methods",
    "href": "07-point-pattern.html#density-based-methods",
    "title": "1 Analysing Spatial Patterns II: Point Pattern Analysis",
    "section": "",
    "text": "Density-based techniques are used to characterise the pattern of a point dataset utilising its general distribution. We can calculate densities at both the global and local scale. However, global densities do not tell us much about the distribution of our data. This is where local density techniques such as Quadrat Analysis and Kernel Density Estimation can help us.\n\n\nWe can create a simple understanding of our data’s distribution by first understanding its global density. This is simply the ratio of the observed number of points \\((n\\) to the study region’s surface area \\(a\\):\n\\[\\widehat{\\lambda} =\\frac{n}{a}\\]\nCalculate the global density of our bike theft point data relative to London:\n\n\n\nR code\n\n# global density\nlength(bike_theft_london$geom)/sum(st_area(msoa_london)) * 1000\n\n\n0.01304807 [1/m^2]\n\n\nWe can see that we have a global density of ~13 bike thefts per square kilometre. This simple density analysis could be supported with further descriptive statistics, however, we still would know little about the local density of our points. The most basic approach to understanding a point layer’s local density is to simply measure the density at different locations within the study area. This approach helps us assess if the density is constant across the study area. The simplest approach to this measurement is through Quadrat Analysis, where the study area is divided into sub-regions. The point density is then computed for each quadrat, by dividing the number of points in each quadrat by the quadrat’s area.\n\n\n\n\n\n\nQuadrats can take on many different shapes such as hexagons or rectangles. The standard approach, however, is a grid of squares The choice of quadrat numbers and quadrat shape can influence the measure of local density and therefore must be chosen with care.\n\n\n\nWe will start with a simple quadrat count by dividing the observation window into 10 x 10 sections and then counting the number of bicycle thefts:\n\n\n\nR code\n\n# quadratcount\nbiketheft_quadrat &lt;- quadratcount(bike_theft_ppp_jitter, nx = 10, ny = 10)\n\n# inspect\nplot(biketheft_quadrat)\n\n\n\n\n\nFigure 3: Quadratcount for our bike theft point dataset.\n\n\n\n\nOur resulting quadrat count shows total counts of bike theft. We can see quite quickly that the quadrats in central London are likely to have a higher local density as their count is much higher than those on the outskirts of London. If we divided our count by the area covered by each quadrat, we would also be able to calculate a precise local density. We will not do this for now, as realistically quadrat analysis is not used very often. The reason why we look at this technique is that it provides us with an easy way to think about how to compare our data distribution and how this relates to the Poisson distribution of Complete Spatial Randomness (CSR).\n\n\n\n\n\n\nWhen looking at the distribution of our points and the respective patterns they show, the key question we often want to answer as geographers is: are our points clustered, randomly distributed, uniform or dispersed? Whilst we can visually assess this distribution, to be able to statistically quantify our data’s distribution, we can compare its distribution to that of the Poisson distribution. The Poisson distribution describes the probability or rate of an event happening over a fixed interval of time or space.\nThe Poisson Distribution can be used when:\n\nThe events are discrete and can be counted in integers\nEvents are independent of one another\nThe average number of events over space or time is known\n\nPoint data that contains a random distribution of points is said to follow a Poisson distribution. The Poisson distribution is very useful as it allows us to compare a random expected model to our observations. Essentially, if our data’s distribution does not fit the Poisson distribution, then we can infer that something interesting might be going on and our events might not actually be independent of each other. Instead, they might be clustered or dispersed and there is likely to be underlying processes influencing these patterns.\n\n\n\nWe can use a Poisson distribution to generate a randomly generated point pattern dataset with the same number of points and the same observation window. We then can compare our quadrat analysis with the theoretical quadrat analysis using a chi-squared test, with the null hypotheses that our point data are randomly distributed.\n\n\n\nR code\n\n# quadrat test\nquadrat.test(bike_theft_ppp_jitter, nx = 10, ny = 10)\n\n\nWarning: Some expected counts are small; chi^2 approximation may be inaccurate\n\n\n\n    Chi-squared test of CSR using quadrat counts\n\ndata:  bike_theft_ppp_jitter\nX2 = 55562, df = 80, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\nQuadrats: 81 tiles (irregular windows)\n\n\nOur \\(p\\) value is well below 0.01, which means there is a statistically significant difference between the expected random distribution as drawn from a Poisson distribution and the observed distribution.\n\n\n\nWe now have a basic confirmation that our data is not randomly distributed. Instead of looking at the distribution of our bike theft with the boundaries of our quadrats, we can analyse our points using a Kernel Density Estimation (KDE). KDE is a statistical technique to generate a smooth continuous distribution between data points that represent the density of the underlying pattern.\n\n\n\n\n\n\nA KDE will produce a raster surface that details the estimated distribution of our event point data over space. Each cell within our raster contains a value that is this estimated density at that location; when visualised in its entirety as the whole raster, we can quickly identify areas of high and low density, i.e. where are clusters are located in our dataset. To create this surface, a KDE computes a localised density for small subsets of our study area but unlike quadrat analysis, these subsets overlap one another to create a moving sub-region window, defined by a kernel. A kernel defines the shape and size of the window and can also weight the points, using a defined kernel function. The simplest kernel function is a basic kernel where each point in the kernel window is assigned equal weight. The kernel density approach generates a grid of density values whose cell size is smaller than that of the kernel window. Each cell is assigned the density value computed for the kernel window centered on that cell. The resulting surface is created from these individually, locally calculated density values.\n\n\n\nProducing a KDE in R is very straight-forward in spatstat, using your ppp object and the density.ppp() function. However, you will need to consider both the bandwidth or diameter of your Kernel (sigma) and whether you want to apply a weighting to your points using a function. First, let’s go ahead and create a simple KDE of bike theft with our bandwidth set to 100m:\n\n\n\nR code\n\n# kernel density estimation\nplot(density.ppp(bike_theft_ppp_jitter, sigma = 100))\n\n\n\n\n\nFigure 4: Kernel density estimation - bandwidth 100m.\n\n\n\n\nWe can see from just our KDE that there are visible clusters present within our bike theft data, particularly in and around central London. We can go ahead and vary our bandwidth to to see how that affects the density estimate:\n\n\n\nR code\n\n# kernel density estimation\nplot(density.ppp(bike_theft_ppp_jitter, sigma = 500))\n\n\n\n\n\nFigure 5: Kernel density estimation - bandwidth 500m.\n\n\n\n\nOur clusters now appear brighter and larger than our KDE with a 100m bandwidth. This is because changing the bandwidth enables your KDE to take into account more points within its calculation, resulting in a smoother surface. However, there are issues with oversmoothing your data - as you can see above, our clusters are not as well defined and therefore we may attribute high levels of bike theft to areas where there actually is not that much. Smaller bandwidths will lead to a more irregular shaped surface, where we have more precision in our defined clusters but, once again, there are issues of undersmoothing.\n\n\n\n\n\n\nWhilst there are automated functions (e.g. based on maximum-likelihood estimations) that can help you with selecting an appropriate bandwidth, in the end you will have to make a decision on what is most appropriate for your dataset.\n\n\n\nAlthough bandwidth typically has a more pronounced effect upon the density estimation than the type of kernel used, kernel types can affect the result too. When we use a different kernel type, we are looking to weight the points within our kernel differently:\n\n\n\n\n\nFigure 6: Kernel types and their distributions. [Enlarge image]\n\n\n\n\nEach function will result in [a slightly different estimation. Deciding which function is most suitable for your analysis will all depend on what you are trying to capture. We can compare and see the impact of different functions on our current dataset looking at the default kernel in density.ppp(), which gaussian, alongside the epanechnikov, quartic or disc kernels.\n\n\n\n\n\n\nTo change the kernel within your KDE, you simply need to add the kernel parameter and set it to one of the kernels available, denoted as a string, e.g. epanechnikov, quartic, disc. Ultimately, however, bandwidth will have a more marked effect upon the density estimation than kernel type.\n\n\n\n\n\n\nR code\n\n# kernel density estimation\nplot(density.ppp(bike_theft_ppp_jitter, sigma = 400, kernel = \"gaussian\"), main = \"Gaussian\")\n\n\n\n\n\nFigure 7: Kernel density estimation - bandwidth 400m, Gaussian kernel.\n\n\n\n\n\n\n\nR code\n\n# kernel density estimation\nplot(density.ppp(bike_theft_ppp_jitter, sigma = 400, kernel = \"epanechnikov\"), main = \"Epanechnikov\")\n\n\n\n\n\nFigure 8: Kernel density estimation - bandwidth 400m, Epanechnikov kernel.\n\n\n\n\n\n\n\nR code\n\n# kernel density estimation\nplot(density.ppp(bike_theft_ppp_jitter, sigma = 400, kernel = \"quartic\"), main = \"Quartic\")\n\n\n\n\n\nFigure 9: Kernel density estimation - bandwidth 400m, Quartic kernel.\n\n\n\n\n\n\n\nR code\n\n# kernel density estimation\nplot(density.ppp(bike_theft_ppp_jitter, sigma = 400, kernel = \"disc\"), main = \"Disc\")\n\n\n\n\n\nFigure 10: Kernel density estimation - bandwidth 400m, Disc kernel.\n\n\n\n\nWe can be quite confident in stating that bike theft in London in 2021 is not a spatially random process and we can clearly see the areas where bicycle theft is most concentrated. How can we use this new data in our original analysis that looks to find out whether bike theft primarily occurs near tube and train stations? KDEs are primarily used for visual analysis of point data distribution. What we can do, however, is improve our visualisation and making it into a proper map using tmap.\n\n\n\nR code\n\n# to raster, clip by London outline\nkde_400g_raster &lt;- density.ppp(bike_theft_ppp_jitter, sigma = 400) |&gt;\n    rast()\n\n\nWe now have a standalone raster we can use with any function in the tmap library. Before we go ahead, one issue we will face is that our resulting raster does not have a Coordinate Reference System, so we need to manually add this information to the raster object:\n\n\n\nR code\n\n# check CRS\nst_crs(kde_400g_raster)\n\n\nCoordinate Reference System: NA\n\n\nYou should see an NA appear within our CRS arguments, so we need to fix this:\n\n\n\nR code\n\n# add CRS\ncrs(kde_400g_raster) &lt;- \"epsg:27700\"\n\n# check CRS\nst_crs(kde_400g_raster)\n\n\nCoordinate Reference System:\n  User input: OSGB36 / British National Grid \n  wkt:\nPROJCRS[\"OSGB36 / British National Grid\",\n    BASEGEOGCRS[\"OSGB36\",\n        DATUM[\"Ordnance Survey of Great Britain 1936\",\n            ELLIPSOID[\"Airy 1830\",6377563.396,299.3249646,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4277]],\n    CONVERSION[\"British National Grid\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",49,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",-2,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996012717,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",400000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",-100000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"United Kingdom (UK) - offshore to boundary of UKCS within 49°45'N to 61°N and 9°W to 2°E; onshore Great Britain (England, Wales and Scotland). Isle of Man onshore.\"],\n        BBOX[49.75,-9,61.01,2.01]],\n    ID[\"EPSG\",27700]]\n\n\nNow we have our raster data ready to map:\n\n\n\nR code\n\n# map kde raster\ntm_shape(kde_400g_raster) +\n  tm_raster(\n    col = \"lyr.1\",\n    palette = \"Blues\"\n  )\n\n\n\n\n\nFigure 11: Basic map using raster data using the tmap library.\n\n\n\n\nWith our kde_400g_raster using tmap, we can go ahead and customise our map as we would do with our vector mapping such as changing the legend, adding a title, north arrow, etc. — and, of course, our stations_london spatial layer."
  },
  {
    "objectID": "07-point-pattern.html#distance-based-methods",
    "href": "07-point-pattern.html#distance-based-methods",
    "title": "1 Analysing Spatial Patterns II: Point Pattern Analysis",
    "section": "",
    "text": "We have spent a good amount of time looking at using density-based methods to quantify whether our point data is randomly distributed and visualise and identify high and low areas of density, showing if our data is clustered and where. Distance-based methods, on the other hands, allow us to quantify the second order properties of our data, i.e. the influence the location of these points have on one another.\n\n\n\n\n\n\nDistance-based measures analyse the spatial distribution of points using distances between point pairs, with the majority using Euclidean distance, to determine quantitatively whether our data is, again, randomly distributed or shows sign of clustering or dispersion. These methods are a more rigorous alternative to using the Quadrat Analysis approach, and enables us to assess clustering within our point data at both a global and local scale. For the remainder of this practical, we look at one distance-based measure (average nearest neighbour) as well as a well-known hybrid technique (DBSCAN)\n\n\n\n\n\nAverage Nearest Neighbour (ANN) is the average distance between all points within a dataset and their individual nearest point. ANN is used as a global indicator to measure the overall pattern of a point set. The ANN of a given point collection can be compared with the expected ANN from points following a random distribution to test whether our point data is clustered or dispersed. The approach is similar to that of the Quadrat Analysis simulation we saw above, but by using distance rather than density grouped to arbitrary quadrats, ANN is likely to be a more robust quantification of our point distribution.\nWe can calculate the ANN for our dataset by using the nndist() function from the spatstat library:\n\n\n\nR code\n\n# calculate the average distance to nearest neighbour\nmean(nndist(bike_theft_ppp_jitter, k = 1))\n\n\n[1] 53.4103\n\n\nWe can see that the average nearest neighbour for all points is 53.5 metres. To get an insight into the spatial ordering of all our points relative to one another we can plot the ANN values for different order neighbours (i.e. first closest point, second closest point, etc. For point patterns that are highly clustered, we would expect the average distances between points to be very small.\nCalculate the average nearest neighbour to the \\(k\\) nearest neighbours for our bike theft data:\n\n\n\nR code\n\n# calculate the average distance to the nearest 100 neighbours\nbike_theft_ann &lt;- apply(nndist(bike_theft_ppp_jitter, k = 1:100), 2, FUN = mean)\n\n# plot\nplot(bike_theft_ann ~ seq(1:100))\n\n\n\n\n\nFigure 12: Average distances to the nearest 100 neighbours.\n\n\n\n\nIn our case, the plot does not reveal anything interesting in particular except that higher order points seem to be slightly closer than lower order points. Overall, the ANN is a good approach to conduct statistical tests on large datasets (e.g. if we were to compare the result to a theoretical distribution), but visually it does not tell us a huge amount about our dataset!\n\n\n\nThe techniques above are useful exploratory techniques for telling us if we have spatial clusters present in our point data, but they are not able to tell us precisely where in our area of interest the clusters are occurring. One popular technique for discovering precise clusters in space is an algorithm known as DBSCAN, an algorithm that incorporates both distance and density.\n\n\n\n\n\n\nFor the complete overview of the DBSCAN algorithm, you can refer to the original paper by Ester et al. (1996). Whilst DBSCAN is a relatively old algorithm, there has been a substantial resurgence in its use within spatial data science (e.g. see Arribas-Bel et al. 2019).\n\n\n\nWe can use DBSCAN to detect clusters within our bike theft dataset and then use the clusters to further answer our original research question. DBSCAN takes two parameters:\n\nThe minimum number of points: MinPts.\nThe distance: epsilon.\n\nAcross a set of points, DBSCAN will group together points that are close to each other based on a distance measurement and a minimum number of points. It also marks as outliers the points that are in low-density regions. The algorithm can be used to find associations and structures in data that are hard to find through visual observation alone, but that can be relevant and useful to find patterns and predict trends. However, DBSCAN will only work well if you are able to successfully define the distance and minimum points parameters and your clusters do not vary considerably in density.\nWe can conduct a DBSCAN analysis using the dbscan library. For our analysis, we will set our epsilon to 200m and then set our minimum cluster size to 20 bike thefts.\n\n\n\nR code\n\n# run dbscan\nbike_theft_dbscan &lt;- bike_theft_london |&gt;\n    st_coordinates() |&gt;\n    dbscan(eps = 200, minPts = 20)\n\n\n\n\n\n\n\n\nThe dbscan() function only accepts a data matrix or dataframe of points as input rather than a spatial dataframe. This is why in the above code we pass the st_coordinates() function to extract the projected coordinates.\n\n\n\nThe DBSCAN output contains three objects, including a vector detailing the cluster for each of our bike theft observations. To be able to work with our DBSCAN output effectively and, for example, plot the clusters as individual polygons, we need to add our cluster groups back to into our original point dataset. Because the DBSCAN output does not change the order of points, we can simply stick the cluster output on the bike_theft_london spatial dataframe.\n\n\n\nR code\n\n# add the cluster numbers as column\nbike_theft_london &lt;- bike_theft_london |&gt;\n    mutate(dbcluster = bike_theft_dbscan$cluster)\n\n\nNow we have each of our bike theft points in London associated with a specific cluster, we can generate a polygon that represents these clusters in space, as we can sort of see in our above plot. To do so, we will utilise a geometric operation from the sf package: the st_convex_hull() function. The st_convex_hull() function can be used to create a polygon that represents the minimum coverage of our individual clusters. We can use this function to create a polygon that represents the geometry of all points within each cluster. To enable this, we will use something called a for loop. A for loop is used to repeat a specific block of code a known number of times.\n\n\n\n\n\n\nThe code below is a bit complex, but essentially we repeat the following steps for each cluster that the DBSCAN algorithm identified:\n\nFilter the bike_theft_london spatial dataframe by cluster number.\nCombine all points belonging to the same cluster into a single set of geometry observations.\nCalculate the convex hull of that single set of geometry observations.\nStore the resulting polygon in a list.\n\nOnce we have looped through all clusters, we end up with a final list containing all the geometries of our cluster polygons which we can convert into a spatial dataframe.\n\n\n\nRun a for loop to generate a polygon dataset that represents our bike theft clusters:\n\n\n\nR code\n\n# create an empty list to store the resulting convex hull geometries set the\n# length of this list to the total number of clusters found\ngeometry_list &lt;- vector(mode = \"list\", length = max(bike_theft_london$dbcluster))\n\n# create a counter to keep track\ncounter &lt;- 0\n\n# begin loop\nfor (cluster_index in seq(0, max(bike_theft_london$dbcluster))) {\n\n    # filter to only return points for belonging to cluster n\n    biketheft_cluster_subset &lt;- bike_theft_london |&gt;\n        filter(dbcluster == cluster_index)\n\n    # union points, calculate convex hull\n    cluster_polygon &lt;- biketheft_cluster_subset |&gt;\n        st_union() |&gt;\n        st_convex_hull()\n\n    # add the geometry of the polygon to our list\n    geometry_list[counter] &lt;- (cluster_polygon)\n\n    # update the counter\n    counter &lt;- counter + 1\n}\n\n# combine the list\nbike_theft_clusters &lt;- st_sfc(geometry_list, crs = 27700)\n\n\nWe now have a polygon spatial dataframe, bike_theft_clusters, that show the general location and distribution of bike theft clusters in London. Let’s put them on a map:\n\n\n\nR code\n\n# add London outline\ntm_shape(outline_london) +\n  tm_borders() +\n  # add bike theft clusters\n  tm_shape(bike_theft_clusters) +\n  tm_polygons() +\n  # add basemap\n  tm_basemap(c(StreetMap = \"OpenStreetMap\"))\n\n\n\n\n\nFigure 13: Basic map of identified DBSCAN bike theft clusters."
  },
  {
    "objectID": "07-point-pattern.html#assignment-w07",
    "href": "07-point-pattern.html#assignment-w07",
    "title": "1 Analysing Spatial Patterns II: Point Pattern Analysis",
    "section": "",
    "text": "We have conducted a lot of analysis today. What we have failed to do, however, is to make proper maps. For this week’s assignment, create two publishable maps with all the trimmings using the main outputs from today’s tutorial:\n\nA Kernel Density Map of bike theft in London.\nA cluster map of bike theft in London, using the DBSCAN output.\n\n\n\n\n\n\n\n\nBecause the thefts concentrate in central London why not consider zooming into a specific area?\nWhy not contextualise the densities and clusters by including the stations_london layer?"
  },
  {
    "objectID": "07-point-pattern.html#wm-w07",
    "href": "07-point-pattern.html#wm-w07",
    "title": "1 Analysing Spatial Patterns II: Point Pattern Analysis",
    "section": "",
    "text": "Last week you might have had a look at the book Data Skills for Reproducible Research. A great tool that can be used to create reproducible workflows is found in the targets package. The targets package is a Make-like pipeline tool for Statistics and data science in R. With targets, you can maintain a reproducible workflow without repeating yourself. To get started: have a look at the Walkthrough chapter to see targets in action."
  },
  {
    "objectID": "07-point-pattern.html#byl-w07",
    "href": "07-point-pattern.html#byl-w07",
    "title": "1 Analysing Spatial Patterns II: Point Pattern Analysis",
    "section": "",
    "text": "As geographers we are keen to understand our point data’s distribution and understand whether are our points clustered, randomly distributed, uniform or dispersed. We have looked at various techniques that enable us to statistically and visually assess our data’s distribution and understand whether our data is randomly distributed or clustered in space. And that is us done for this week. Reading list anyone?"
  },
  {
    "objectID": "08-autocorrelation.html",
    "href": "08-autocorrelation.html",
    "title": "1 Analysing Spatial Patterns III: Spatial Autocorrelation",
    "section": "",
    "text": "This week, we will be looking at measuring spatial dependence. Spatial dependence is the idea that an observed value of a variable in one location is to some degree dependent on the observed value of the same value in a nearby location. For spatial analysis, this dependence can be assessed and measured statistically by considering the level of spatial autocorrelation between values of a specific variable, observed in either different locations or between pairs of variables observed at the same location. Spatial autocorrelation occurs when these values are not independent of one another and instead cluster together in space.\n\n\nThe slides for this week’s lecture can be downloaded here: [Link]\n\n\n\n\n\n\nGriffith, D. 2017. Spatial Autocorrelation. The Geographic Information Science & Technology Body of Knowledge. [Link]\nGimond, M. 2023. Intro to GIS and spatial analysis. Chapter 13: Spatial autocorrelation. [Link]\nLivings, M. and Wu, A-M. 2020. Local Measures of Spatial Association. The Geographic Information Science & Technology Body of Knowledge. [Link]\n\n\n\n\n\nLee, S. 2019. Uncertainty in the effects of the modifiable areal unit problem under different levels of spatial autocorrelation: a simulation study. International Journal of Geographical Information Science 33: 1135-1154. [Link]\nHarris, R. 2020. Exploring the neighbourhood-level correlates of Covid-19 deaths in London using a difference across spatial boundaries method. Health & Place 66: 102446. [Link]\n\n\n\n\n\nThis week, we are using a completely new dataset and we will investigate to what extent people in London who self-identified as Asian-Bangladeshi in the 2021 Census are clustered in London at the Ward-level. To complete this analysis, we will be using a data download from the London Datastore, which we will need to clean and join to a spatial layer containing the relevant Ward boundaries.\n\n\n\n\n\n\nThe Wards and electoral divisions in the United Kingdom are electoral districts at sub-national level. These differ from the Census geographies (LSOAs, MSOAs) we have been using so far.\n\n\n\n\n\nOpen a new script within your GEOG0030 project and save this script as wk8-population-analysis.r. At the top of your script, add the following metadata:\n\n\n\nR code\n\n# Analysing population clusters in London \n# Date: January 2024\n\n\nNow let us add all of the libraries we will be using today to the top of our script:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\nlibrary(spdep)\n\n\nWe will start by downloading the 2022 Ward boundaries for Great Britain:\n\nNavigate to the Open Geography Portal: [Link]\nIn the main menu go to Boundaries -&gt; Administrative Boundaries  -&gt; Wards / Electoral Divisions -&gt; 2022 Boundaries.\nClick on Wards (December 2022) Boundaries GB GBC.\nClick on Download -&gt; Download GeoPackage.\nSave the file as WARDS2022.gpkg in your boundaries folder.\n\nFor the data on ethnic groups we turn to the London Datastore again. They have prepared just the dataset that we want to use from the 2021 Census.\n\nNavigate to the London Datastore: [Link].\nClick on Data in the navigation menu.\nType 2021 census Wards ethnicity into the search field.\nDownload the Ethnic group.xlsx file containing Ward codes and counts of number of individuals who self-identify with a particular population group.\n\nOpen de file in Excel, and look at the first tab (Front Page). You will notice that under 2022 Wards are listed as the administrative geography the data have been aggregated to. Coincidentally we just downloaded the 2022 Ward boundaries. The actual data that we want to use can be found in the 2021 tab, which contains the 2021 Census results on ethnicity.\n\n\n\n\n\nFigure 1: The Excel file containing the number of people that self-identify as a particular group by Ward. [Enlarge image]\n\n\n\n\nLooking at the Excel file, we clearly need to extract the data and save this as a separate csv file before we can import the data into R.\n\nOpen a new Excel spreadsheet.\nFrom the 2021 tab of the Ethnic group.xlsx spreadsheet, cut (Edit -&gt; Cut) all cells from columns A to X and rows 1 to 681 and paste these into this new spreadsheet.\nSave the file as csv into your data folder as WARD2021_ethnic_group.csv.\n\nAfter this, let’s load our London Ward file:\n\n\n\nR code\n\n# read in our Ward GeoPackage\nward_gb &lt;- st_read(\"data/raw/boundaries/WARDS2022.gpkg\")\n\n\nReading layer `WD_DEC_22_GB_BGC' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/raw/boundaries/WARDS2022.gpkg' \n  using driver `GPKG'\nSimple feature collection with 8021 features and 11 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 5512.998 ymin: 5352.6 xmax: 655653.8 ymax: 1220299\nProjected CRS: OSGB36 / British National Grid\n\n\nCheck the CRS of our ward_gb spatial dataframe:\n\n\n\nR code\n\n# inspect CRS\nst_crs(ward_gb)\n\n\nCoordinate Reference System:\n  User input: OSGB36 / British National Grid \n  wkt:\nPROJCRS[\"OSGB36 / British National Grid\",\n    BASEGEOGCRS[\"OSGB36\",\n        DATUM[\"Ordnance Survey of Great Britain 1936\",\n            ELLIPSOID[\"Airy 1830\",6377563.396,299.3249646,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4277]],\n    CONVERSION[\"British National Grid\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",49,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",-2,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996012717,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",400000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",-100000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"United Kingdom (UK) - offshore to boundary of UKCS within 49°45'N to 61°N and 9°W to 2°E; onshore Great Britain (England, Wales and Scotland). Isle of Man onshore.\"],\n        BBOX[49.75,-9,61.01,2.01]],\n    ID[\"EPSG\",27700]]\n\n\nThis all looks good, so we can move to also load the csv we just created:\n\n\n\nR code\n\n# read csv\nethnicity_london &lt;- read_csv(\"data/data/WARD2021_ethnic_group.csv\")\n\n\nNew names:\nRows: 680 Columns: 24\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(4): ward code, ward name, local authority code, local authority name dbl (19):\nAll usual residents, White British, White Irish, White Gypsy/Irish... lgl (1):\n...9\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...9`\n\n\nInspect the file by using the View() function. First thing you will notice is that the column names are rather long. Second thing you will notice is that one of the columns does not contain any information but NA values. It also does not have a meaningful name (...9). It seems that in the process of converting the Excel file into a csv an extra column was added in the process. Let’s drop this column, and any other columns we do not need, and then rename the remaining columns for easier reference.\n\n\n\n\n\n\nIf your conversion from Excel to csv did not result in an extra column, update the code below to reflect this so to avoid dropping a column that contains information.\n\n\n\n\n\n\nR code\n\n# drop columns by index\nethnicity_london &lt;- ethnicity_london |&gt;\n    select(-3, -4, -9)\n\n# rename columns\nnames(ethnicity_london) &lt;- c(\"ward22cd\", \"ward22nm\", \"all_pop\", \"white_british\",\n    \"white_irish\", \"white_gypsy\", \"white_other\", \"mixed_white_asian\", \"mixed_white_african\",\n    \"mixed_white_caribbean\", \"mixed_other\", \"asian_bangladeshi\", \"asian_chinese\",\n    \"asian_indian\", \"asian_pakistani\", \"asian_other\", \"black_african\", \"black_caribbean\",\n    \"black_other\", \"other_arab\", \"other\")\n\n\nIf you like, you can also write out the final csv using the write_csv() function.\n\n\n\nWe now need to join our population group dataset to our Ward spatial layer. Because the Ward dataset contains every the geometry of every single Ward in Great Britain, we can use an inner_join so that only the geometries of those Wards that also appear in the ethnicity_london object are retained.\n\n\n\nR code\n\n# inner join\nethnicity_london_sdf &lt;- ward_gb |&gt;\n  inner_join(ethnicity_london, by = c(\"WD22CD\" = \"ward22cd\"))\n\n\nHave a look at your newly created Ward dataframe using the plot() function.\n\n\n\nR code\n\n# inspect\nplot(ethnicity_london_sdf, max.plot = 1)\n\n\n\n\n\nFigure 2: Quick plot of the filtered Ward spatial dataframe.\n\n\n\n\nThis is looking reasonably okay, however, it is clear that there are some data missing from the City of London. The reason for this is that the dataset we downloaded only contains an overall value for the City of London rather than a value for every Ward. If we only were interested in plotting the data, we could simply add a layer with no data to colour in the empty City of London area, but for measuring spatial autocorrelation it is essential that there are no holes in the spatial data that should not be there. We will fix this by extracting the Wards pertaining to the City of London and adding these to our spatial dataframe.\n\n\n\nR code\n\n# filter Wards pertaining to City of London\ncity_of_london_sdf &lt;- ward_gb |&gt;\n    filter(LAD22NM == \"City of London\")\n\n\nWe have now effectively filtered out the 25 2022 Wards that fall with in the City of London Local Authority District. We now need to assign our ethnic group data to this. We will first add the data and subsequently divide the overall counts equally across the Wards.\n\n\n\n\n\n\nThis is a bit of a lazy approach. Much better would be to actually try and find the actual Ward counts from the 2021 Census.\n\n\n\nTo join the ethnicity_london dataset to the City of London wards is a bit tricky — because the ethnicity_london dataset does not contain the actual Wards codes for the City of London Wards. Fortunately for us, the London Data Store was a bit cheeky and in the ethnic group file they created they put the Local Authority District code of the City of London into the Ward column. We can use this to our advantage and join our datasets together with a left_join.\n\n\n\nR code\n\n# join\ncity_of_london_sdf &lt;- city_of_london_sdf |&gt;\n  left_join(ethnicity_london, by = c(\"LAD22CD\" = \"ward22cd\"))\n\n# inspect\nhead(city_of_london_sdf)\n\n\nSimple feature collection with 6 features and 31 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 531866.1 ymin: 180540.4 xmax: 533617.7 ymax: 182084.5\nProjected CRS: OSGB36 / British National Grid\n     WD22CD       WD22NM WD22NMW   LAD22CD        LAD22NM  BNG_E  BNG_N\n1 E05009288   Aldersgate         E09000001 City of London 532169 181721\n2 E05009289      Aldgate         E09000001 City of London 533397 181175\n3 E05009290    Bassishaw         E09000001 City of London 532438 181495\n4 E05009291 Billingsgate         E09000001 City of London 533151 180754\n5 E05009292  Bishopsgate         E09000001 City of London 533207 181664\n6 E05009293 Bread Street         E09000001 City of London 532224 181151\n      LONG     LAT Shape_Leng                               GlobalID\n1 -0.09645 51.5190   1726.380 {E218C4C6-CC77-423C-9312-010363F61625}\n2 -0.07896 51.5138   1895.285 {E968A26F-9AF6-460A-A075-7F8AB975CB16}\n3 -0.09266 51.5169   1428.356 {8B433738-7452-4891-A3B0-17498C13F8AD}\n4 -0.08267 51.5100   1670.199 {7FF90094-80A5-466D-AE07-B30EC9966CCC}\n5 -0.08152 51.5182   2748.846 {DBE8B6D5-0C08-4FB5-9C36-D2F7483B4792}\n6 -0.09587 51.5138   2082.841 {1EC7A9B3-EAB0-4CC5-8627-354BA7865D6C}\n                     ward22nm all_pop white_british white_irish white_gypsy\n1 City of London (aggregated)    7375          4243         180           3\n2 City of London (aggregated)    7375          4243         180           3\n3 City of London (aggregated)    7375          4243         180           3\n4 City of London (aggregated)    7375          4243         180           3\n5 City of London (aggregated)    7375          4243         180           3\n6 City of London (aggregated)    7375          4243         180           3\n  white_other mixed_white_asian mixed_white_african mixed_white_caribbean\n1        1373               111                  37                    38\n2        1373               111                  37                    38\n3        1373               111                  37                    38\n4        1373               111                  37                    38\n5        1373               111                  37                    38\n6        1373               111                  37                    38\n  mixed_other asian_bangladeshi asian_chinese asian_indian asian_pakistani\n1         103               232           263          216              16\n2         103               232           263          216              16\n3         103               232           263          216              16\n4         103               232           263          216              16\n5         103               232           263          216              16\n6         103               232           263          216              16\n  asian_other black_african black_caribbean black_other other_arab other\n1         213            98              46          49         69    85\n2         213            98              46          49         69    85\n3         213            98              46          49         69    85\n4         213            98              46          49         69    85\n5         213            98              46          49         69    85\n6         213            98              46          49         69    85\n                           SHAPE\n1 MULTIPOLYGON (((532248.7 18...\n2 MULTIPOLYGON (((533466.1 18...\n3 MULTIPOLYGON (((532536.2 18...\n4 MULTIPOLYGON (((533320.1 18...\n5 MULTIPOLYGON (((533404.6 18...\n6 MULTIPOLYGON (((532025 1813...\n\n\nThis seems to have worked quite nicely, except that now all our data points have been duplicated. We need to fix this by dividing all counts by 25 (the number of wards) to equally divide the counts across the City of London. If we have to do this for every column individually there is a lot of typing involved, but fortunately we can use the mutate_at() function from the dplyr library to do this for the columns in one go.\n\n\n\n\n\n\nCheck the documentation of the mutate_at() function to see how it works in detail, but essentially you provide a function, in this case “divide a number by the total number of Wards” and do this for all columns that are specified.\n\n\n\n\n\n\nR code\n\n# divide counts by number of Wards\ncity_of_london_sdf &lt;- city_of_london_sdf |&gt;\n    mutate_at(c(13:31), function(x) as.integer(x/nrow(city_of_london_sdf)))\n\n# inspect\nhead(city_of_london_sdf)\n\n\nSimple feature collection with 6 features and 31 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 531866.1 ymin: 180540.4 xmax: 533617.7 ymax: 182084.5\nProjected CRS: OSGB36 / British National Grid\n     WD22CD       WD22NM WD22NMW   LAD22CD        LAD22NM  BNG_E  BNG_N\n1 E05009288   Aldersgate         E09000001 City of London 532169 181721\n2 E05009289      Aldgate         E09000001 City of London 533397 181175\n3 E05009290    Bassishaw         E09000001 City of London 532438 181495\n4 E05009291 Billingsgate         E09000001 City of London 533151 180754\n5 E05009292  Bishopsgate         E09000001 City of London 533207 181664\n6 E05009293 Bread Street         E09000001 City of London 532224 181151\n      LONG     LAT Shape_Leng                               GlobalID\n1 -0.09645 51.5190   1726.380 {E218C4C6-CC77-423C-9312-010363F61625}\n2 -0.07896 51.5138   1895.285 {E968A26F-9AF6-460A-A075-7F8AB975CB16}\n3 -0.09266 51.5169   1428.356 {8B433738-7452-4891-A3B0-17498C13F8AD}\n4 -0.08267 51.5100   1670.199 {7FF90094-80A5-466D-AE07-B30EC9966CCC}\n5 -0.08152 51.5182   2748.846 {DBE8B6D5-0C08-4FB5-9C36-D2F7483B4792}\n6 -0.09587 51.5138   2082.841 {1EC7A9B3-EAB0-4CC5-8627-354BA7865D6C}\n                     ward22nm all_pop white_british white_irish white_gypsy\n1 City of London (aggregated)     295           169           7           0\n2 City of London (aggregated)     295           169           7           0\n3 City of London (aggregated)     295           169           7           0\n4 City of London (aggregated)     295           169           7           0\n5 City of London (aggregated)     295           169           7           0\n6 City of London (aggregated)     295           169           7           0\n  white_other mixed_white_asian mixed_white_african mixed_white_caribbean\n1          54                 4                   1                     1\n2          54                 4                   1                     1\n3          54                 4                   1                     1\n4          54                 4                   1                     1\n5          54                 4                   1                     1\n6          54                 4                   1                     1\n  mixed_other asian_bangladeshi asian_chinese asian_indian asian_pakistani\n1           4                 9            10            8               0\n2           4                 9            10            8               0\n3           4                 9            10            8               0\n4           4                 9            10            8               0\n5           4                 9            10            8               0\n6           4                 9            10            8               0\n  asian_other black_african black_caribbean black_other other_arab other\n1           8             3               1           1          2     3\n2           8             3               1           1          2     3\n3           8             3               1           1          2     3\n4           8             3               1           1          2     3\n5           8             3               1           1          2     3\n6           8             3               1           1          2     3\n                           SHAPE\n1 MULTIPOLYGON (((532248.7 18...\n2 MULTIPOLYGON (((533466.1 18...\n3 MULTIPOLYGON (((532536.2 18...\n4 MULTIPOLYGON (((533320.1 18...\n5 MULTIPOLYGON (((533404.6 18...\n6 MULTIPOLYGON (((532025 1813...\n\n\nThat is much better. The last thing we now need to do is to combine our city_of_london_sdf with our ethnicity_london_sdf. Because both spatial dataframe contain the exact same data and column names, we can simply bind them together.\n\n\n\nR code\n\n# bind spatial dataframes\nethnicity_london_sdf &lt;- ethnicity_london_sdf |&gt;\n    rbind(city_of_london_sdf)\n\n\nLet’s check whether our approach worked by plotting the updated spatial dataframe:\n\n\n\nR code\n\n# inspect\nplot(ethnicity_london_sdf, max.plot = 1)\n\n\n\n\n\nFigure 3: Quick plot of the full Ward spatial dataframe.\n\n\n\n\nThis looks much better: there are no more obvious holes in our spatial dataframe and we can move on.\n\n\n\n\nToday, we are interested in looking at spatial autocorrelation: the effect of spatial processes on distributions. We will be using our newly created ethnicity_london_sdf to look at this in action. Before we do this, however, let’s start by looking at the data distribution.\n\n\n\n\n\n\nAnalysing the distribution of your data and summarising the main characteristics of its distribution is known as Exploratory Data Analysis (EDA). EDA was promoted by prominent statistician John Tukey to encourage data analysts to explore their data outside of traditional formal modelling and come up with new areas of investigation and hypotheses. Tukey promoted the use of five summary statistics: the maximum, the minimum, the median, and the quartiles, which, in comparison to the mean and standard deviation, provide a more robust understanding of a dataset’s distribution, particularly if the data is skewed.\n\n\n\nWe looked at how we can use R to extract some of these summary statistics briefly in Week 4’s computer tutorial, but let’s have a look at how we can add further to this EDA. A simple and straightforward way to extract the main characteristics of a dataset is by using the summary() function.\n\n\n\n\n\n\nThe summary() function can be called on a dataset as a whole and will generate summary statistics for each numeric variable.\n\n\n\n\n\n\nR code\n\n# summarise dataframe, but exclude geometry column\nsummary(ethnicity_london_sdf |&gt;\n    st_drop_geometry())\n\n\n    WD22CD             WD22NM            WD22NMW            LAD22CD         \n Length:704         Length:704         Length:704         Length:704        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n   LAD22NM              BNG_E            BNG_N             LONG         \n Length:704         Min.   :505661   Min.   :157756   Min.   :-0.47645  \n Class :character   1st Qu.:523789   1st Qu.:174575   1st Qu.:-0.21930  \n Mode  :character   Median :531040   Median :181022   Median :-0.11311  \n                    Mean   :530499   Mean   :180206   Mean   :-0.12106  \n                    3rd Qu.:537483   3rd Qu.:186252   3rd Qu.:-0.02262  \n                    Max.   :557943   Max.   :199291   Max.   : 0.27621  \n      LAT          Shape_Leng        GlobalID           ward22nm        \n Min.   :51.30   Min.   :  965.1   Length:704         Length:704        \n 1st Qu.:51.46   1st Qu.: 4888.3   Class :character   Class :character  \n Median :51.51   Median : 6496.1   Mode  :character   Mode  :character  \n Mean   :51.51   Mean   : 7213.4                                        \n 3rd Qu.:51.56   3rd Qu.: 8619.1                                        \n Max.   :51.68   Max.   :35083.0                                        \n    all_pop      white_british    white_irish    white_gypsy    \n Min.   :   51   Min.   :   10   Min.   :   1   Min.   :  0.00  \n 1st Qu.: 9641   1st Qu.: 3102   1st Qu.: 148   1st Qu.:  3.00  \n Median :11712   Median : 4854   Median : 227   Median :  8.00  \n Mean   :11611   Mean   : 5212   Mean   : 250   Mean   : 11.63  \n 3rd Qu.:14467   3rd Qu.: 7077   3rd Qu.: 323   3rd Qu.: 14.00  \n Max.   :21292   Max.   :14727   Max.   :1345   Max.   :216.00  \n  white_other     mixed_white_asian mixed_white_african mixed_white_caribbean\n Min.   :  11.0   Min.   :  0.0     Min.   :  1.0       Min.   :  1.00       \n 1st Qu.: 767.5   1st Qu.:101.0     1st Qu.: 48.0       1st Qu.: 85.75       \n Median :1299.5   Median :142.0     Median : 79.0       Median :140.00       \n Mean   :1468.7   Mean   :144.2     Mean   : 93.0       Mean   :169.62       \n 3rd Qu.:2057.2   3rd Qu.:185.2     3rd Qu.:125.2       3rd Qu.:223.00       \n Max.   :4708.0   Max.   :442.0     Max.   :403.0       Max.   :800.00       \n  mixed_other    asian_bangladeshi asian_chinese     asian_indian   \n Min.   :  1.0   Min.   :   3.0    Min.   :   0.0   Min.   :   3.0  \n 1st Qu.:109.0   1st Qu.:  42.0    1st Qu.:  83.0   1st Qu.: 191.0  \n Median :158.5   Median :  92.5    Median : 140.5   Median : 323.5  \n Mean   :168.8   Mean   : 315.5    Mean   : 176.5   Mean   : 771.1  \n 3rd Qu.:225.2   3rd Qu.: 201.0    3rd Qu.: 214.2   3rd Qu.: 765.2  \n Max.   :469.0   Max.   :6852.0    Max.   :1344.0   Max.   :7763.0  \n asian_pakistani   asian_other     black_african    black_caribbean \n Min.   :   0.0   Min.   :   1.0   Min.   :   3.0   Min.   :   1.0  \n 1st Qu.:  60.0   1st Qu.: 251.8   1st Qu.: 278.8   1st Qu.: 132.5  \n Median : 120.5   Median : 430.5   Median : 569.0   Median : 286.0  \n Mean   : 317.9   Mean   : 566.0   Mean   : 815.2   Mean   : 489.5  \n 3rd Qu.: 331.8   3rd Qu.: 712.8   3rd Qu.:1123.8   3rd Qu.: 676.2  \n Max.   :3633.0   Max.   :3225.0   Max.   :5297.0   Max.   :3523.0  \n  black_other       other_arab          other       \n Min.   :   1.0   Min.   :   1.00   Min.   :   0.0  \n 1st Qu.:  65.0   1st Qu.:  46.75   1st Qu.: 112.0  \n Median : 174.0   Median :  91.00   Median : 199.0  \n Mean   : 241.6   Mean   : 150.59   Mean   : 248.6  \n 3rd Qu.: 339.0   3rd Qu.: 169.00   3rd Qu.: 339.8  \n Max.   :1563.0   Max.   :1693.00   Max.   :1411.0  \n\n\nThis gives us an overview of all variables, but let’s have a look at our population group of interest for today’s practical by creating a histogram.\n\n\n\nR code\n\n# histogram\nhist(ethnicity_london_sdf$asian_bangladeshi)\n\n\n\n\n\nFigure 4: Histogram of the distribution of the self-identified Asian-Bangladeshi population group.\n\n\n\n\nWe can actually see our data has a very strong negative skew with the majority of Wards having a relatively low number of individual self-identifying as Asian-Bangladeshi, but there are also some Wards where a large number of self-identified Asian-Bangladeshis are residing.\nAnother type of chart we can create just using the base R library is a boxplot. A boxplot shows the core characteristics of the distributions within a dataset, including the interquartile range.\n\n\n\n\n\nFigure 5: Simple boxplot. [Enlarge image]\n\n\n\n\nPlot the boxplot of our asian_bangladeshi variable:\n\n\n\nR code\n\n# histogram\nboxplot(ethnicity_london_sdf$asian_bangladeshi, horizontal = TRUE)\n\n\n\n\n\nFigure 6: Boxplot of the distribution of the self-identified Asian-Bangladeshi population group.\n\n\n\n\nAgain, it is clear that the majority of Wards have a relatively low number of individuals that self-identify as Asian-Bangladeshi, but there is a good number of outliers. This raises the question whether these outliers are random or also clustered in space — and this brings us to spatial autocorrelation.\n\n\n\n\n\n\nThere is actually a lot more we can do in terms of visualising our data’s distribution and the best way forward would be to become more familiar with the ggplot2 library. ggplot2 is the main visualisation for both statistical and, increasingly, spatial graphs, charts and maps. Refer back to Week 4’s optional suggestions on how to get started with ggplot2.\n\n\n\n\n\n\nWhilst statistical analysis of distributions focus on tests and charts, when we want to understand the spatial distribution of our phenomena, we have a very simple solution: we make a map. In our case, we are looking at areal unit data and therefore we can use a choropleth map to study our data across the Wards. In fact, we can actually create a sequence of maps not only covering the self-identified Asian-Bangladeshis but also other population groups.\n\n\n\nR code\n\n# store variables of interest as separate variable\nvar_fields &lt;- names(ethnicity_london_sdf)[14:31]\n\n# add Ward boundaries\ntm_shape(ethnicity_london_sdf) +\n  tm_polygons(\n    col = \"gray\"\n  ) +\n  # map all variables\n  tm_shape(ethnicity_london_sdf) +\n  # select variables\n  tm_polygons(\n    col = var_fields\n  ) +\n  # add layout options\n  tm_layout(\n    legend.show = FALSE\n  ) +\n  # add 4 columns\n  tm_facets(\n    ncol = 4\n  )\n\n\n\n\n\nFigure 7: Facet map of all our population groups.\n\n\n\n\nDespite these maps being a little small and not containing any labels, it seems that different population groups indeed concentrate in different parts of London. Let’s zoom into the Asian-Bangladeshi population group and add a legend:\n\n\n\nR code\n\n# map a specific variable\ntm_shape(ethnicity_london_sdf) +\n  tm_polygons(\n    col = \"gray\"\n  ) +\n  tm_shape(ethnicity_london_sdf) +\n  tm_polygons(\n    col = \"asian_bangladeshi\",\n    n = 5,\n    style = \"jenks\"\n  ) +\n  tm_layout(\n    legend.outside = TRUE,\n    legend.outside.position = \"right\"\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 5, 10, 15, 20),\n    position = c(\"left\", \"bottom\")\n  )\n\n\n\n\n\nFigure 8: Quick map of the self-identified Asian-Bangladeshi population group.\n\n\n\n\n\n\n\n\n\n\nPlease remember, whereas the above map is fine for a quick inspection, it is technically incorrect because we are showing absolute numbers on a choropleth. This is something we should never do, unless the spatial units are identical in size (e.g. a hexagonal tessellation of an area), because larger areas will draw attention and affect the visualisation. To be fair, even for a quick inspection, it would be much better to normalise these counts by the total number of people living within each of the Wards to get a more honoust picture of the distribution.\n\n\n\nThe thing with spatial distributions is that we can quickly pick up on spatial patterns present within our data just by looking at the data. For example, we can clearly see some concentrations of people that self-identify as Asian-Bangladeshis in East London. The question now is whether these clusters are significant from a statistical point of view. This brings us to measuring spatial correlation.\nBefore we move on, let’s normalise the asian_banglades variable by creating a new variable that contains the proportion of individuals that self-identify as Asian-Bangladeshi:\n\n\n\nR code\n\n# calculate proportions\nethnicity_london_sdf &lt;- ethnicity_london_sdf |&gt;\n    mutate(asian_bangladeshi_prop = asian_bangladeshi/all_pop)\n\n\n\n\n\nWe can assess the distribution of our data using what is known as spatial autocorrelation tests, which can be conducted on both a global (identify if the data is clustered) and local (identify the precise clusters) scales. Whilst these different tests quantify how clustered, how random, or how dispersed, these distributions are through various approaches, ultimately they provide us with statistical and spatial information that can be used to create quantifiable descriptions of a variable’s distribution and how it vary over space.\nAs discussed in this week’s lecture, we have several types of tests that look to quantify spatial autocorrelation. Of these tests, there are two main categories:\n\nMeasures of global spatial aucorrelation: tests that provide us with a statistic to tell whether spatial autocorrelation is present in our dataset.\nMeasures of local spatial autocorrelation: tests that break down the global patterns and essentially tell us where we can find clusters and outliers.\n\nThree of the most frequently used tests are the Global Moran’s I, the Local Moran’s I, and the Getis-Ord Gi*:\n\n\n\n\n\n\n\n\n\nTest\nScale\nTest\nOutput\n\n\n\n\nGlobal Moran’s I\nGlobal\nTests how “random” the spatial distribution of values are, producing a correlation coefficient for the relationship between a variable and its surrounding values.\nMetric between \\(-1\\) and \\(1\\).\n\n\nLocal Moran’s I\nLocal\nTests the difference between a unit of analysis and its neighbour(s).\nCan be used alongside the mean of values to generate cluster type generations.\n\n\nGetis-Ord Gi*\nLocal\nIdentifies statistically significant hot spots and cold spots using the local Getis-Ord \\(Gi*\\) statistic.\nThe returned \\(z\\)-scores can be used to identify statistically significant clusters.\n\n\n\n\n\n\n\n\n\nIn each of these cases, our \\(p\\)-values are pseudo \\(p\\)-values, generated through simulations such as those outlined in the lecture. Our pseudo \\(p\\)-values allow us to interpret our relationships with a level of confidence. If we find that our relationships do not have any significance, then we cannot be confident in presenting them as true results.\n\n\n\n\n\nUnderlying our global Moran’s I test is the concept of a spatial lag model. A spatial lag model plots each value against the mean of its neighbours’ values, defined by our selected approach. This creates a scatter plot, from which our Moran’s I statistic can be derived.\nAn Ordinary Least Squares (OLS) regression is used to fit the data and produce a slope, which determines the Moran’s I statistic:\n\n\n\n\n\nFigure 9: A spatial lag model - plotting value against the mean of its neighbours. Source: Manuel Gimond. [Enlarge image]\n\n\n\n\nTo determine a \\(p\\)-value from our model for global Moran’s I, this spatial lag model is computed multiple times (think hundreds, thousands) but uses a random distribution of neighbouring values to determine different slopes for multiple ways our data could be distributed if our data was distributed randomly. The output of this test is a sampling distribution of Moran’s I values that would confirm a null hypothesis that our values are randomly distributed. These slopes are then compared to compare our observed slope versus our random slopes and identify whether the slope is within the main distribution of these values or an outlier:\n\n\n\n\n\nFigure 10: Determining significance using a Monte Carlo simulation. Source: Manuel Gimond. [Enlarge image]\n\n\n\n\nIf our slope is an outlier, i.e. not a value we would expect to compute if the data were randomly distributed, we are more confidently able to confirm our slope is reflective of our data’s clustering and is significant. Our pseudo-\\(p\\)-values are then computed from our simulation results:\n\\[\n\\frac{N_{extreme} + 1}{N + 1}\n\\]\nWhere \\({N_{extreme}}\\) is the number of simulated Moran’s I values that were more extreme that our observed statistic and \\({N}\\) is the total number of simulations. In the example above, from Manuel Gimond, only 1 out the 199 simulations was more extreme than the observed local Moran’s I statistic. Therefore \\({N_{extreme}}\\) = 1 , so \\(p\\) is equal to \\((1+1) / (199 + 1) = 0.01\\). This means that “there is a 1% probability that we would be wrong in rejecting the null hypothesis”. This approach is known as a Monte Carlo simulation or permutation bootstrap test.\n\n\n\nFor any spatial autocorrelation test that you want to conduct, you will always need one critical piece of information: how do we define ‘neighbours’ in our dataset to enable the value comparison. Every observation in a dataset will need to have a set of neighbours to which its value is compared. To enable this, we need to determine how many or what type of neighbours should be taken into account for each observation when conducting a spatial autocorrelation test. These ‘neighbouring’ observations can be defined in a multitude of ways, based either on geometry or proximity, and include:\n\nContiguity: Queen [nodes have to touch] or Rook [edges have to touch]\nFixed Distance: Euclidean Distance [specified distance]\n(K) Nearest Neighbours: \\(n\\) closest neighbours\n\n\n\n\n\n\nFigure 11: Different approaches of conceptualising neighbours for spatial autocorrelation measurement: contiguity, fixed distance and nearest neighbours. Source: Manuel Gimond. [Enlarge image]\n\n\n\n\nDepending on the variable you are measuring, the appropriateness of these different types of neighbourhood calculation techniques can change. As a result, how you define neighbours within your dataset will have an impact on the validity and accuracy of spatial analysis. Whatever approach you choose therefore needs to be grounded in particular theory that aims to represent the process and variable investigated.\n\n\n\n\n\n\nHave a look at Esri’s Help Documentation on Selecting a conceptualization of spatial relationships: Best practices when you come to need to define neighbours yourself for your own analysis.\n\n\n\nFor our analysis into the clustering of population groups, we will primarily use the Queen contiguity. This approach is “effective when polygons are similar in size and distribution, and when spatial relationships are a function of polygon proximity (the idea that if two polygons share a boundary, spatial interaction between them increases)” (Esri, 2024).\n\n\n\nBefore we can calculate Moran’s I and any similar statistics, we need to first define our spatial weights matrix. This is known mathematically as \\(W_{ij}\\) and this will tell our code which unit neighbours which, according to our neighbour definition. For each neighbour definition, there is a different approach to implementing code to calculate the \\(W_{ij}\\) spatial weights matrix. We will look at three approaches:\n\nCreating a Queen \\(W_{ij}\\) spatial weights matrix\nCreating a Rook \\(W_{ij}\\) spatial weights matrix\nCreating a Fixed Distance \\(W_{ij}\\) spatial weights matrix\n\nFor either approach, we use a single line of code to create the relevant \\(W_{ij}\\) spatial weights matrix:\n\n\n\nR code\n\n# Queens neighbours\nward_neighbours_queen &lt;- ethnicity_london_sdf |&gt;\n    poly2nb(queen = T)\n\n# Rook neighbours\nward_neighbours_rook &lt;- ethnicity_london_sdf |&gt;\n    poly2nb(queen = F)\n\n# Fixed distance neighbours\nward_neighbours_fd &lt;- dnearneigh(st_geometry(st_centroid(ethnicity_london_sdf)),\n    0, 4000)\n\n\nWarning in st_centroid.sf(ethnicity_london_sdf): st_centroid assumes attributes\nare constant over geometries of x\n\n\nCreating our neighbours list through a single line of code, as above, does not really tell us much about the differences between these different definitions. It would be useful to the links between neighbours for our three definitions and visualise their distribution across space. To be able to do this, we will use a few lines of code to generate a visualisation based on mapping the defined connections between the centroids of our Wards.\n\n\n\n\n\n\nA centroid in its most simplest form is the central point of an areal unit. How this central point is defined can be weighted by different approaches to understanding geometries or by using an additional variable. In our case, our centroids will reflect in the geometric middle point of our Wards.\n\n\n\nWe can calculate the centroids of our Wards using one of the geometric tools from the sf library: sf_centroid().\n\n\n\nR code\n\n# calculate the centroids of all of the Wards in London\nward_centroid &lt;- ethnicity_london_sdf |&gt;\n    st_centroid()\n\n\nWarning in st_centroid.sf(ethnicity_london_sdf): st_centroid assumes attributes\nare constant over geometries of x\n\n\n\n\n\n\n\n\nWe actually already used this function above as the creation of the fixed distance spatial weights matrix requires a point geometry and then calculates which other point geometries are within the specified distance.\n\n\n\nNow we have our Ward centroids, we can go ahead and plot the centroids and the defined neighbour connections between them from each of our neighbour definitions. To do so, we will use the plot() function, provide the relationships via our ward_neighbours_* lists and then the geometry associated with these lists from our ward_centroid object:\n\n\n\nR code\n\n# plot neighbours: Queen\nplot(ward_neighbours_queen, st_geometry(ward_centroid), col = \"red\", pch = 20, cex = 0.5)\n\n\n\n\n\nFigure 12: 2022 Wards with a Queen neighbourhood definition.\n\n\n\n\n\n\n\nR code\n\n# plot neighbours: Rook\nplot(ward_neighbours_rook, st_geometry(ward_centroid), col = \"blue\", pch = 20, cex = 0.5)\n\n\n\n\n\nFigure 13: 2022 Wards with a Rook neighbourhood definition.\n\n\n\n\n\n\n\nR code\n\n# plot neighbours: Fixed distance\nplot(ward_neighbours_fd, st_geometry(ward_centroid), col = \"red\", pch = 20, cex = 0.5)\n\n\n\n\n\nFigure 14: 2022 Wards with a Fixed distance neighbourhood definition.\n\n\n\n\n\n\n\n\n\n\nIn this example we use four kilometres as a distance threshold (i.e. centroids are considered neighbours if they are within three kilometres from one another, however, this is an arbitrary distance. When using a fixed distance make sure you have a good reason to select the distance (e.g. used in a paper that used the same administrative geographies).\n\n\n\nWhen comparing these different maps, we can see that there is definitely a difference in the number of neighbours when we use our different approaches. It seems our fixed distance neighbour conceptualisation has many connections in the centre of London versus areas on the outskirts. We can see that our contiguity approaches provide a more equally distributed connection map, with our Queen conceptualisation having a few more links that our Rook conceptualisation.\n\n\n\n\n\n\nWhichever form of neighbours you are using, always check the results, especially those of the contiguity based approaches. If the spatial file that you are using is not in good shape (e.g. polygons seem to touch upon visual inspection but actually do not), your results will be compromised.\n\n\n\nWe can also type the different neighbours objects into the console to find out the total number of non-zero links (i.e. total number of connections) present within the conceptualisation. You should see that Queen has 4022 non-zero links, Rook has 3854 and Fixed Difference has 12462 Whilst this code simply explores these conceptualisations it helps us understand further how our different neighbourhood conceptualisations can ultimately impact our overall analysis.\n\n\n\n\n\n\nRemember: the number of links essentially determines the value of the spatial lag variable — i.e. the value of a certain variable in comparison with its neighbours.\n\n\n\nWith our neighbours now defined, we will go ahead and create our final spatial weights objects that will be needed for our spatial autocorrelation code. At the moment, we have our neighbours defined as a list but we need to convert it to a neighbours object using the nb2listw() function:\n\n\n\nR code\n\n# create a neighbours list - Queen\nward_spatial_weights_queen &lt;- ward_neighbours_queen |&gt;\n    nb2listw(style = \"W\")\n\n# create a neighbours list - Rook\nward_spatial_weights_queen &lt;- ward_neighbours_rook |&gt;\n    nb2listw(style = \"W\")\n\n# create a neighbours list\nward_spatial_weights_fd &lt;- ward_neighbours_fd |&gt;\n    nb2listw(style = \"W\", zero.policy = TRUE)\n\n\n\n\n\n\n\n\nThe style that we specify in the code above determines how neighbours are weighted. style = 'W' row-standardises the values, e.g. if a Ward has five neighbours the value of the spatially lagged variable of interest will be the average of the variable of interest of these five neighbours — every neighbour has an equal weight. See ?nb2listw for more information on the different styles.\n\n\n\n\n\n\n\n\n\nFor the fixed distance measure we need to specify an additional parameter (zero.policy). This is because it is possible that there are Wards without any neighbour (i.e. there is a Ward with a centroid that is not within our specified distance). This is not ideal, because you would want every unit to have at least one neighbour. We do not need this parameter in case of the contiguity based measures today because every Ward polygon touches at least one other Ward polygon — provided our spatial data are in order.\n\n\n\n\n\n\nWith a Global Moran’s I we test how random the spatial distribution of our values are, producing a global Moran’s statistic from the lag approach explained earlier.\nThe global Moran’s I statistic is a metric between \\(-1\\) and \\(1\\):\n\n\\(-1\\) suggests a completely even spatial distribution of values\n\\(0\\) suggests a random distribution\n\\(1\\) suggests a non-random distribution of clearly defined clusters\n\nBefore we run our global Moran’s I test, we will first create a spatial lag model plot which looks at each of the values plotted against their spatially lagged values. The graph will show quickly whether we are likely to expect our test to return a positive, zero, or negative statistic:\n\n\n\nR code\n\n# Moran's plot\nmoran.plot(ethnicity_london_sdf$asian_bangladeshi_prop, listw = ward_spatial_weights_queen)\n\n\n\n\n\nFigure 15: Plot of lagged values versus polygon values using the Queen neigbhourhood definition.\n\n\n\n\nWe can see that there is a positive relationship between our asian_bangladeshi_prop variable and the spatially lagged asian_bangladeshi_prop variable, therefore we are expecting our global Moran’s I test to produce a statistic reflective of the slope visible in our scatter plot. Now we can run the global Moran’s I spatial autocorrelation test:\n\n\n\nR code\n\n# Moran's I\nmoran_queen &lt;- ethnicity_london_sdf |&gt;\n    pull(asian_bangladeshi_prop) |&gt;\n    as.vector() |&gt;\n    moran.test(ward_spatial_weights_queen)\n\n# inspect result\nmoran_queen\n\n\n\n    Moran I test under randomisation\n\ndata:  as.vector(pull(ethnicity_london_sdf, asian_bangladeshi_prop))  \nweights: ward_spatial_weights_queen    \n\nMoran I statistic standard deviate = 34.376, p-value &lt; 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.777712178      -0.001422475       0.000513713 \n\n\nThe Moran’s I statistic calculated should be 0.73. With \\(1\\) = clustered, \\(0\\) = no pattern, \\(-1\\) = dispersed, this means we can confirm that the population in London that self-identifies as Asian-Bangladeshi is strongly positively autocorrelated. In other words, self-identified Asian-Bangladeshis in London tend to reside in similar areas. We can also consider the pseudo \\(p\\)-value as a measure of the statistical significance of the model - at &lt; 2.2e-16, which confirm our result is statistically significant.\n\n\n\n\n\n\nBefore we run our local spatial autocorrelation tests, let’s just take a second to think through what our results have shown. From our global statistical tests, we can confirm that:\n\nThere is clustering in our dataset.\nSimilar values are clustering.\nHigh values are clustering.\n\nWe can conclude already that Wards with a relatively large proportion of people that self-identify as Asian-Bangladeshis tend to cluster in the same area. What we do not know yet is where these clusters are occurring. To help with this, we need to run our local models to identify where these clusters are located.\n\n\n\n\n\n\nA local Moran’s I test deconstructs the global Moran’s I down to its components and then constructs a localised measure of autocorrelation, which can show different cluster types. To run a local Moran’s I test, the code is similar to above:\n\n\n\nR code\n\n# Local Moran's I\nlocal_moran_queen &lt;- ethnicity_london_sdf |&gt;\n    pull(asian_bangladeshi_prop) |&gt;\n    as.vector() |&gt;\n    localmoran(ward_spatial_weights_queen)\n\n# inspect result\nhead(local_moran_queen)\n\n\n         Ii         E.Ii     Var.Ii      Z.Ii Pr(z != E(Ii))\n1 28.400091 -0.033787002  4.5702873 13.300389   2.302674e-40\n2  6.410822 -0.005144343  1.1975739  5.862879   4.549109e-09\n3  4.547442 -0.007928661  0.9163467  4.758760   1.947861e-06\n4 12.767600 -0.013075467  1.5033436 10.423767   1.931491e-25\n5 21.632012 -0.058105443  6.3758219  8.590013   8.695953e-18\n6 33.215970 -0.065154746 10.6742781 10.186588   2.276128e-24\n\n\nAs you should see, we are not given a single statistic as we did with our global test, but rather a table of different statistics that are all related back to each of the Wards in our dataset. If we look at the help page for the localmoran function we can find out what each of these statistics mean:\n\n\n\nName\nDescription\n\n\n\n\nIi\nLocal Moran’s I statistic\n\n\nE.Ii\nExpectation of local Moran’s I statistic\n\n\nVar.Ii\nVariance of local Moran’s I statistic\n\n\nZ.Ii\nStandard deviation of local Moran’s I statistic\n\n\nPr()\n\\(p\\)-value of local Moran’s I statistic\n\n\n\nWe therefore have a Moran’s I statistic for each of our Wards, as well as a significance value plus a few other pieces of information that can help us create some maps showing our clusters. To be able to do this, we need to join our local Moran’s I output back into our ethnicity_london_sdf spatial dataframe, which will then allow us to map these results.\nTo create this join, we first coerce our local Moran’s I output into a dataframe that we then join to our ethnicity_london_sdf spatial dataframe using the familiar mutate() function from the dplyr library. In our case, we do not need to provide an attribute to join these two dataframes together as we use the computer’s logic to join the data in the order in which it was created:\n\n\n\nR code\n\n# coerce to dataframe\nlocal_moran_queen &lt;- as.data.frame(local_moran_queen)\n\n# update the names for easier reference\nnames(local_moran_queen) &lt;- c(\"LMI_Ii\", \"LMI_eIi\", \"LMI_varIi\", \"LMI_zIi\", \"LMI_sigP\")\n\n# join\nethnicity_london_sdf &lt;- ethnicity_london_sdf |&gt;\n    mutate(local_moran_queen)\n\n\nWe now have the data we need to plot our local spatial autocorrelation maps. We will first plot the most simple maps to do with our local Moran’s I test: the local Moran’s I statistic.\n\n\n\nR code\n\n# map the local Moran's I statistic\ntm_shape(ethnicity_london_sdf) +\n  tm_polygons(\n    col = \"LMI_Ii\",\n    style = \"pretty\",\n    midpoint = 0,\n    title = \"Local Moran's I statistic\"\n  ) +\n  tm_layout(\n    main.title = \"Local Moran's I statistic\",\n    main.title.fontface = 2,\n    fontfamily = \"Helvetica\",\n    legend.outside = TRUE,\n    legend.outside.position = \"right\",\n    legend.title.size = 1,\n    legend.title.fontface = 2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"bottom\")\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 5, 10, 15, 20),\n    position = c(\"left\", \"bottom\")\n  )\n\n\n\n\n\nFigure 16: Cluster map of the self-identified Asian-Bangladeshi population group.\n\n\n\n\nFrom the map, it is possible to observe the variations in autocorrelation across space. We can interpret that there seems to be a geographic pattern to the autocorrelation. However, it is not possible to understand if these are clusters of high or low values or which ones are statistically significant. To be able to interpret this confidently, we also need to know the significance of the patterns we see in our map and therefore need to map the \\(p\\)-value of local Moran’s I statistic.\n\n\n\nR code\n\n# significance breaks\nbreaks &lt;- c(0, 0.05, 0.1, 1)\n\n# colour palette\ncolours &lt;- c(\"#ffffff\", \"#a6bddb\", \"#2b8cbe\")\n\n# map the local Moran's I statistic / significance only\ntm_shape(ethnicity_london_sdf) +\n  tm_polygons(\n    col = \"LMI_sigP\",\n    style = \"fixed\",\n    breaks = breaks,\n    palette = rev(colours),\n    title = \"p-value of Local Moran's I stat\"\n  ) +\n  tm_layout(\n    main.title = \"Significant clusters\",\n    main.title.fontface = 2,\n    fontfamily = \"Helvetica\",\n    legend.outside = TRUE,\n    legend.outside.position = \"right\",\n    legend.title.size = 1,\n    legend.title.fontface = 2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"bottom\")\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 5, 10, 15, 20),\n    position = c(\"left\", \"bottom\")\n  )\n\n\n\n\n\nFigure 17: Significant cluster map of the self-identified Asian-Bangladeshi population group.\n\n\n\n\nUsing our significance map, we can interpret the above clusters present in our local Moran’s I statistic more confidently. As evident, we do have several clusters that are statistically significant to the \\(p\\)-value &lt; 0.05.\nIdeally we would combine these two outputs to see what values cluster together as well as which clusters are significant. We can do this with a cluster map of our local Moran’s I statistic (Ii) which will show areas of different types of clusters, including:\n\nHIGH-HIGH: A Ward with a relatively high proportion of residents that self-identify as Asian-Bangladeshi that is also surrounded by other Wards with a relatively high proportion of residents that self-identify as Asian-Bangladeshi.\nHIGH-LOW: A Ward with a relatively high proportion of residents that self-identify as Asian-Bangladeshi that is surrounded by Wards with a relatively low proportion of residents that self-identify as Asian-Bangladeshi.\nLOW-HIGH: A Ward with a relatively low proportion of residents that self-identify as Asian-Bangladeshi that is surrounded by Wards with a relatively high proportion of residents that self-identify as Asian-Bangladeshi.\nLOW-LOW: A Ward with a relatively low proportion of residents that self-identify as Asian-Bangladeshi that is also surrounded by other Wards with a relatively low proportion of residents that self-identify as Asian-Bangladeshi.\n\nOur HIGH-HIGH and LOW-LOW will show our clusters, whereas the other two cluster types reveal anomalies in our variable. To create a map that shows this, we need to quantify the relationship each of our Wards have with the Wards around them to determine their cluster type. We do this using their observed value and their local Moran’s I statistic and their deviation around their respective means:\n\nIf a Ward’s observed value is higher than the observed mean and it’s local Moran’s I statistic is higher than the LMI mean, it is designated as HIGH-HIGH.\nIf a Ward’s observed value is lower than the observed mean and it’s local Moran’s I statistic is lower than the LMI mean, it is designated as LOW-LOW.\nIf a Ward’s observed value is lower than the observed mean but it’s local Moran’s I statistic is higher than the LMI mean, it is designated as LOW-HIGH.\nIf a Ward’s observed value is higher than the observed mean but it’s local Moran’s I statistic is lower than the LMI mean, it is designated as HIGH-LOW.\nIf a Ward’s LMI was found not to be significant, the Ward will be mapped as not significant.\n\nTo create this cluster map, we need to take several additional steps. We firstly need, for each Ward, whether its observed value is higher or lower than the mean observed. We then need to know, for each Ward, whether its LMI value is higher or lower than the mean LMI. Finally, we can use the values of these two columns to assign each Ward with a cluster type.\n\n\n\nR code\n\n# significance breaks\nethnicity_london_sdf &lt;- ethnicity_london_sdf |&gt;\n  mutate(obs_diff = (asian_bangladeshi_prop - mean(ethnicity_london_sdf$asian_bangladeshi_prop)))\n\n# compare local LMI value with mean LMI value\nethnicity_london_sdf &lt;- ethnicity_london_sdf |&gt;\n  mutate(LMI_diff = (ethnicity_london_sdf$LMI_Ii - mean(ethnicity_london_sdf$LMI_Ii)))\n\n# set significance threshold\nsignif &lt;- 0.05\n\n# generate column with cluster type, using values above\nethnicity_london_sdf &lt;- ethnicity_london_sdf |&gt;\n  mutate(cluster_type = case_when(\n    obs_diff &gt; 0 & LMI_diff &gt; 0 & LMI_sigP &lt; signif ~ \"High-High\",\n    obs_diff &lt; 0 & LMI_diff &lt; 0 & LMI_sigP &lt; signif ~ \"Low-Low\",\n    obs_diff &lt; 0 & LMI_diff &gt; 0 & LMI_sigP &lt; signif ~ \"Low-High\",\n    obs_diff &gt; 0 & LMI_diff &lt; 0 & LMI_sigP &lt; signif ~ \"High-Low\",\n    LMI_sigP &gt; signif ~ \"No Significance\"\n  ))\n\n\nNow we have a column detailing our cluster types, we can create a cluster map that details our four cluster types as well as those that are not significant. Creating a categorical map in R and using tmap is a little tricky and we will need to do some preparing of our colour palettes to ensure our data is mapped correctly. To do this, we first need to figure out how many cluster types we have in our cluster_type field:\n\n\n\nR code\n\n# count the cluster types\ncount(ethnicity_london_sdf, cluster_type)\n\n\nSimple feature collection with 4 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503575 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n     cluster_type   n                          SHAPE\n1       High-High  44 MULTIPOLYGON (((535466.2 18...\n2        High-Low  10 MULTIPOLYGON (((533648.3 18...\n3         Low-Low   1 MULTIPOLYGON (((533386.7 18...\n4 No Significance 649 MULTIPOLYGON (((520238.1 16...\n\n\nWe can see that we have three out of the four possible cluster types in our dataset — alongside the No Significance value. We therefore need to ensure our palette includes three colours for these cluster types, plus a white colour for No Significance:\n\n\n\nR code\n\n# create palette\npal &lt;- c(\"#d7191c\", \"#fdae61\", \"#2c7bb6\", \"#F5F5F5\")\n\n\nNow we can finally map our clusters:\n\n\n\nR code\n\n# plot the different clusters\ntm_shape(ethnicity_london_sdf) +\n  tm_polygons(\n    col = \"cluster_type\",\n    palette = pal,\n    title = \"Cluster Type\"\n  ) +\n  tm_layout(\n    main.title = \"Cluster Map Asian Bangladeshis in London\",\n    main.title.fontface = 2,\n    fontfamily = \"Helvetica\",\n    legend.outside = TRUE,\n    legend.outside.position = \"right\",\n    legend.title.size = 1,\n    legend.title.fontface = 2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"bottom\")\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 5, 10, 15, 20),\n    position = c(\"left\", \"bottom\")\n  )\n\n\n\n\n\nFigure 18: Cluster types map of the self-identified Asian-Bangladeshi population group.\n\n\n\n\nAnd there we have it: within one map we can visualise both the relationship of our Wards to their respective neighbourhoods and the significance of this relationship from our local Moran’s I test. This type of map is called a LISA map and is a great way of showing how a variable is actually clustering.\n\n\n\nThe final test we will run today is the local Getis-Ord, which will produce the \\(Gi*\\) statistic. This statistic will identify hot- and coldspots by looking at the neighbours within a defined proximity to identify where either high or low values cluster spatially and recognising statistically significant hotspots as those areas of high values where other areas within a neighbourhood range also share high values too (and vice versa for coldspots).\n\n\n\nR code\n\n# Gi* test\ngi_queen &lt;- ethnicity_london_sdf |&gt;\n    pull(asian_bangladeshi_prop) |&gt;\n    as.vector() |&gt;\n    localG(ward_spatial_weights_queen)\n\n# join\nethnicity_london_sdf &lt;- ethnicity_london_sdf |&gt;\n    mutate(asian_bangladeshi_gi = as.numeric(gi_queen))\n\n# inspect\nethnicity_london_sdf\n\n\nSimple feature collection with 704 features and 41 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503575 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\nFirst 10 features:\n      WD22CD                  WD22NM WD22NMW   LAD22CD       LAD22NM  BNG_E\n1  E05009317           Bethnal Green         E09000030 Tower Hamlets 535607\n2  E05009318 Blackwall & Cubitt Town         E09000030 Tower Hamlets 538100\n3  E05009319                Bow East         E09000030 Tower Hamlets 536928\n4  E05009320                Bow West         E09000030 Tower Hamlets 536261\n5  E05009321           Bromley North         E09000030 Tower Hamlets 537617\n6  E05009322           Bromley South         E09000030 Tower Hamlets 537535\n7  E05009323            Canary Wharf         E09000030 Tower Hamlets 537488\n8  E05009324          Island Gardens         E09000030 Tower Hamlets 537960\n9  E05009325                Lansbury         E09000030 Tower Hamlets 538149\n10 E05009326               Limehouse         E09000030 Tower Hamlets 536542\n    BNG_N     LONG     LAT Shape_Leng                               GlobalID\n1  182735 -0.04653 51.5272   4596.711 {ECA3FE65-ECBD-416D-8ACD-06CFCE763148}\n2  180056 -0.01167 51.5026   8655.302 {FC5D2F6D-B45D-425F-9FC8-AC9302A49D63}\n3  183769 -0.02710 51.5362   6838.744 {F7195489-A08A-4CDB-9AD3-589F9E8D5720}\n4  183181 -0.03694 51.5311   6114.871 {7FA851A2-53A2-4B25-A116-A8ED23B50B96}\n5  182704 -0.01759 51.5265   3892.041 {479BB007-2506-4F94-B4EF-9BED621B387A}\n6  182093 -0.01901 51.5210   3603.544 {BDAB15AA-F556-40FA-A5A4-77B8C1C898F2}\n7  179863 -0.02056 51.5010   5934.088 {F8FE1F74-465C-4409-9CA2-23FB59C0B870}\n8  178641 -0.01424 51.4899   4450.479 {A14A94A1-FF5B-46B6-9640-3AFAB6FAB091}\n9  181610 -0.01035 51.5165   5979.077 {89F93DA3-1DB3-420F-AC37-453F49482043}\n10 180835 -0.03380 51.5099   3275.725 {4CE013FC-4E13-4DFF-9CF8-153E4D4362EF}\n                  ward22nm all_pop white_british white_irish white_gypsy\n1            Bethnal Green   19703          6870         338           7\n2  Blackwall & Cubitt Town   13956          4365         208           2\n3                 Bow East   14781          7055         280          15\n4                 Bow West   12939          6167         260          12\n5            Bromley North    9329          2208         111          22\n6            Bromley South    8748          1979          75          16\n7             Canary Wharf   12831          3563         185           2\n8           Island Gardens   13409          5030         188           2\n9                 Lansbury   14993          4140         125          12\n10               Limehouse    6242          2354         135           1\n   white_other mixed_white_asian mixed_white_african mixed_white_caribbean\n1         2042               281                 124                   174\n2         2454               207                 103                   191\n3         1539               179                 105                   302\n4         1229               166                  73                   208\n5          688                82                  40                   144\n6          609                71                  65                   105\n7         2519               185                  96                   132\n8         2543               184                  68                   128\n9         1188                88                 107                   293\n10        1095                72                  46                    43\n   mixed_other asian_bangladeshi asian_chinese asian_indian asian_pakistani\n1          226              6406           466          364             169\n2          189              1995          1247          962             172\n3          222              2529           227          302              85\n4          185              2746           232          225              69\n5          131              3901           226          165              80\n6           71              3860           214          123              94\n7          190              2017          1232          847             123\n8          146              1854          1000          858             124\n9          115              5865           352          201             132\n10          82              1113           369          273              39\n   asian_other black_african black_caribbean black_other other_arab other\n1          436           705             320         315        202   257\n2          499           588             246         147        169   213\n3          233           572             578         264        122   172\n4          214           380             362         195         80   136\n5          169           536             357         224         97   149\n6          226           487             308         240         98   106\n7          452           435             219         174        212   247\n8          404           311             203          84        117   166\n9          318           891             477         375        126   188\n10         156           203              89          59         54    60\n                            SHAPE asian_bangladeshi_prop    LMI_Ii      LMI_eIi\n1  MULTIPOLYGON (((536275 1824...              0.3251282 28.400091 -0.033787002\n2  MULTIPOLYGON (((538731.4 17...              0.1429493  6.410822 -0.005144343\n3  MULTIPOLYGON (((537638.7 18...              0.1710980  4.547442 -0.007928661\n4  MULTIPOLYGON (((537375.7 18...              0.2122266 12.767600 -0.013075467\n5  MULTIPOLYGON (((538299.6 18...              0.4181584 21.632012 -0.058105443\n6  MULTIPOLYGON (((538284.3 18...              0.4412437 33.215970 -0.065154746\n7  MULTIPOLYGON (((538157 1805...              0.1571974  6.594147 -0.006478715\n8  MULTIPOLYGON (((538731.4 17...              0.1382653  3.683294 -0.004739251\n9  MULTIPOLYGON (((538302.5 18...              0.3911826 30.069710 -0.050379322\n10 MULTIPOLYGON (((536349.2 18...              0.1783082 13.291473 -0.008738364\n    LMI_varIi   LMI_zIi     LMI_sigP  obs_diff  LMI_diff cluster_type\n1   4.5702873 13.300389 2.302674e-40 0.2987531 27.622378    High-High\n2   1.1975739  5.862879 4.549109e-09 0.1165742  5.633110    High-High\n3   0.9163467  4.758760 1.947861e-06 0.1447230  3.769730    High-High\n4   1.5033436 10.423767 1.931491e-25 0.1858516 11.989888    High-High\n5   6.3758219  8.590013 8.695953e-18 0.3917834 20.854300    High-High\n6  10.6742781 10.186588 2.276128e-24 0.4148687 32.438258    High-High\n7   1.1280251  6.214780 5.139675e-10 0.1308224  5.816435    High-High\n8   1.6579452  2.864242 4.180087e-03 0.1118903  2.905581    High-High\n9   8.3840761 10.402283 2.420639e-25 0.3648075 29.291998    High-High\n10  1.0091030 13.240085 5.149045e-40 0.1519332 12.513761    High-High\n   asian_bangladeshi_gi\n1             13.300389\n2              5.862879\n3              4.758760\n4             10.423767\n5              8.590013\n6             10.186588\n7              6.214780\n8              2.864242\n9             10.402283\n10            13.240085\n\n\nBy printing the results of our test, we can see that the local Getis-Ord test is a bit different from a local Moran’s I test as it only contains a single value: the z-score. The z-score is a standardised value relating to whether high values or low values are clustering together, which we call the \\(Gi*\\) statistic. Now we have joined this output, a list of \\(Gi*\\) values, to our ethnicity_london_sdf spatial dataframe, we can map the result:\n\n\n\nR code\n\n# plot the different clusters\ntm_shape(ethnicity_london_sdf) +\n  tm_polygons(\n    col = \"asian_bangladeshi_gi\",\n    style = \"pretty\",\n    midpoint = 0,\n    title = \"Local Gi* statistic\",\n    palette = \"-RdYlBu\"\n  ) +\n  tm_layout(\n    main.title = \"Cluster Map Asian-Bangladeshis in London\",\n    main.title.fontface = 2,\n    fontfamily = \"Helvetica\",\n    legend.outside = TRUE,\n    legend.outside.position = \"right\",\n    legend.title.size = 1,\n    legend.title.fontface = 2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"bottom\")\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 5, 10, 15, 20),\n    position = c(\"left\", \"bottom\")\n  )\n\n\n\n\n\nFigure 19: Local \\(Gi*\\) map of the self-identified Asian-Bangladeshi population group.\n\n\n\n\nOur map shows quite clear hot spots of self-identified Asian-Bangladeshis across London: the higher the z-score, the stronger the clustering.\n\n\n\n\n\n\nIf we want to only map the statistically significant clusters, we need to use these z-scores to filter out only statistically significant clusters. When using a 95 percent confidence level your values should be between -1.96 and +1.96 standard deviations.\n\n\n\n\n\n\n\nThrough conducting our spatial autocorrelation tests, we can confirm the presence of clusters in our asian_bangladeshi_prop variable and assign a significance value to these clusters.\nNow you have the code, you will be able to repeat this analysis on any variable in the future. For this week’s assignment, we therefore want you to find out whether also our self-identified white_british ethnic group shows signs of spatial clustering.\nIn order to do this, you have to:\n\nCreate a choropleth map showing the distribution of the variable’s values.\nNormalise the variable by dividing it by the total population.\nCalculate a global spatial autocorrelation statistic and explain what it shows.\nCreate a local Moran’s I map showing the cluster types.\nCreate a local Getis-Ord hotspot map.\nCompare these results to the output of our asian_bangladeshi_prop values. Do both variables have clusters? Do we see similar clusters in similar locations?\nRun the analysis using a different neighbour definition. What happens? Do the results change?\n\n\n\n\n\n\nIf you are interested in moving beyond spatial autocorrelation, for instance how to account for spatial autocorrelation in statistical models (e.g. with Geographically Weighted Regression or Spatial Regression), have a look at the workbook Mapping and Modelling Geographic Data in R by Bristol-based Professor Richard Harris. Start with The Spatial Variable section, move to the Measuring spatial autocorrelation section before looking at the Geographically Weighted Statistics and Spatial Regression sections.\n\n\n\n\nAnd that is how you can measure spatial dependence in your dataset through different spatial autocorrelation measures. Next week we will focus on the last topic within our set of core spatial analysis methods and techniques, but this week we have covered enough! Probably time to get back to that pesky reading list."
  },
  {
    "objectID": "08-autocorrelation.html#slides-w08",
    "href": "08-autocorrelation.html#slides-w08",
    "title": "1 Analysing Spatial Patterns III: Spatial Autocorrelation",
    "section": "",
    "text": "The slides for this week’s lecture can be downloaded here: [Link]"
  },
  {
    "objectID": "08-autocorrelation.html#reading-w08",
    "href": "08-autocorrelation.html#reading-w08",
    "title": "1 Analysing Spatial Patterns III: Spatial Autocorrelation",
    "section": "",
    "text": "Griffith, D. 2017. Spatial Autocorrelation. The Geographic Information Science & Technology Body of Knowledge. [Link]\nGimond, M. 2023. Intro to GIS and spatial analysis. Chapter 13: Spatial autocorrelation. [Link]\nLivings, M. and Wu, A-M. 2020. Local Measures of Spatial Association. The Geographic Information Science & Technology Body of Knowledge. [Link]\n\n\n\n\n\nLee, S. 2019. Uncertainty in the effects of the modifiable areal unit problem under different levels of spatial autocorrelation: a simulation study. International Journal of Geographical Information Science 33: 1135-1154. [Link]\nHarris, R. 2020. Exploring the neighbourhood-level correlates of Covid-19 deaths in London using a difference across spatial boundaries method. Health & Place 66: 102446. [Link]"
  },
  {
    "objectID": "08-autocorrelation.html#population-clusters",
    "href": "08-autocorrelation.html#population-clusters",
    "title": "1 Analysing Spatial Patterns III: Spatial Autocorrelation",
    "section": "",
    "text": "This week, we are using a completely new dataset and we will investigate to what extent people in London who self-identified as Asian-Bangladeshi in the 2021 Census are clustered in London at the Ward-level. To complete this analysis, we will be using a data download from the London Datastore, which we will need to clean and join to a spatial layer containing the relevant Ward boundaries.\n\n\n\n\n\n\nThe Wards and electoral divisions in the United Kingdom are electoral districts at sub-national level. These differ from the Census geographies (LSOAs, MSOAs) we have been using so far.\n\n\n\n\n\nOpen a new script within your GEOG0030 project and save this script as wk8-population-analysis.r. At the top of your script, add the following metadata:\n\n\n\nR code\n\n# Analysing population clusters in London \n# Date: January 2024\n\n\nNow let us add all of the libraries we will be using today to the top of our script:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\nlibrary(spdep)\n\n\nWe will start by downloading the 2022 Ward boundaries for Great Britain:\n\nNavigate to the Open Geography Portal: [Link]\nIn the main menu go to Boundaries -&gt; Administrative Boundaries  -&gt; Wards / Electoral Divisions -&gt; 2022 Boundaries.\nClick on Wards (December 2022) Boundaries GB GBC.\nClick on Download -&gt; Download GeoPackage.\nSave the file as WARDS2022.gpkg in your boundaries folder.\n\nFor the data on ethnic groups we turn to the London Datastore again. They have prepared just the dataset that we want to use from the 2021 Census.\n\nNavigate to the London Datastore: [Link].\nClick on Data in the navigation menu.\nType 2021 census Wards ethnicity into the search field.\nDownload the Ethnic group.xlsx file containing Ward codes and counts of number of individuals who self-identify with a particular population group.\n\nOpen de file in Excel, and look at the first tab (Front Page). You will notice that under 2022 Wards are listed as the administrative geography the data have been aggregated to. Coincidentally we just downloaded the 2022 Ward boundaries. The actual data that we want to use can be found in the 2021 tab, which contains the 2021 Census results on ethnicity.\n\n\n\n\n\nFigure 1: The Excel file containing the number of people that self-identify as a particular group by Ward. [Enlarge image]\n\n\n\n\nLooking at the Excel file, we clearly need to extract the data and save this as a separate csv file before we can import the data into R.\n\nOpen a new Excel spreadsheet.\nFrom the 2021 tab of the Ethnic group.xlsx spreadsheet, cut (Edit -&gt; Cut) all cells from columns A to X and rows 1 to 681 and paste these into this new spreadsheet.\nSave the file as csv into your data folder as WARD2021_ethnic_group.csv.\n\nAfter this, let’s load our London Ward file:\n\n\n\nR code\n\n# read in our Ward GeoPackage\nward_gb &lt;- st_read(\"data/raw/boundaries/WARDS2022.gpkg\")\n\n\nReading layer `WD_DEC_22_GB_BGC' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/raw/boundaries/WARDS2022.gpkg' \n  using driver `GPKG'\nSimple feature collection with 8021 features and 11 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 5512.998 ymin: 5352.6 xmax: 655653.8 ymax: 1220299\nProjected CRS: OSGB36 / British National Grid\n\n\nCheck the CRS of our ward_gb spatial dataframe:\n\n\n\nR code\n\n# inspect CRS\nst_crs(ward_gb)\n\n\nCoordinate Reference System:\n  User input: OSGB36 / British National Grid \n  wkt:\nPROJCRS[\"OSGB36 / British National Grid\",\n    BASEGEOGCRS[\"OSGB36\",\n        DATUM[\"Ordnance Survey of Great Britain 1936\",\n            ELLIPSOID[\"Airy 1830\",6377563.396,299.3249646,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4277]],\n    CONVERSION[\"British National Grid\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",49,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",-2,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996012717,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",400000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",-100000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"United Kingdom (UK) - offshore to boundary of UKCS within 49°45'N to 61°N and 9°W to 2°E; onshore Great Britain (England, Wales and Scotland). Isle of Man onshore.\"],\n        BBOX[49.75,-9,61.01,2.01]],\n    ID[\"EPSG\",27700]]\n\n\nThis all looks good, so we can move to also load the csv we just created:\n\n\n\nR code\n\n# read csv\nethnicity_london &lt;- read_csv(\"data/data/WARD2021_ethnic_group.csv\")\n\n\nNew names:\nRows: 680 Columns: 24\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(4): ward code, ward name, local authority code, local authority name dbl (19):\nAll usual residents, White British, White Irish, White Gypsy/Irish... lgl (1):\n...9\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...9`\n\n\nInspect the file by using the View() function. First thing you will notice is that the column names are rather long. Second thing you will notice is that one of the columns does not contain any information but NA values. It also does not have a meaningful name (...9). It seems that in the process of converting the Excel file into a csv an extra column was added in the process. Let’s drop this column, and any other columns we do not need, and then rename the remaining columns for easier reference.\n\n\n\n\n\n\nIf your conversion from Excel to csv did not result in an extra column, update the code below to reflect this so to avoid dropping a column that contains information.\n\n\n\n\n\n\nR code\n\n# drop columns by index\nethnicity_london &lt;- ethnicity_london |&gt;\n    select(-3, -4, -9)\n\n# rename columns\nnames(ethnicity_london) &lt;- c(\"ward22cd\", \"ward22nm\", \"all_pop\", \"white_british\",\n    \"white_irish\", \"white_gypsy\", \"white_other\", \"mixed_white_asian\", \"mixed_white_african\",\n    \"mixed_white_caribbean\", \"mixed_other\", \"asian_bangladeshi\", \"asian_chinese\",\n    \"asian_indian\", \"asian_pakistani\", \"asian_other\", \"black_african\", \"black_caribbean\",\n    \"black_other\", \"other_arab\", \"other\")\n\n\nIf you like, you can also write out the final csv using the write_csv() function.\n\n\n\nWe now need to join our population group dataset to our Ward spatial layer. Because the Ward dataset contains every the geometry of every single Ward in Great Britain, we can use an inner_join so that only the geometries of those Wards that also appear in the ethnicity_london object are retained.\n\n\n\nR code\n\n# inner join\nethnicity_london_sdf &lt;- ward_gb |&gt;\n  inner_join(ethnicity_london, by = c(\"WD22CD\" = \"ward22cd\"))\n\n\nHave a look at your newly created Ward dataframe using the plot() function.\n\n\n\nR code\n\n# inspect\nplot(ethnicity_london_sdf, max.plot = 1)\n\n\n\n\n\nFigure 2: Quick plot of the filtered Ward spatial dataframe.\n\n\n\n\nThis is looking reasonably okay, however, it is clear that there are some data missing from the City of London. The reason for this is that the dataset we downloaded only contains an overall value for the City of London rather than a value for every Ward. If we only were interested in plotting the data, we could simply add a layer with no data to colour in the empty City of London area, but for measuring spatial autocorrelation it is essential that there are no holes in the spatial data that should not be there. We will fix this by extracting the Wards pertaining to the City of London and adding these to our spatial dataframe.\n\n\n\nR code\n\n# filter Wards pertaining to City of London\ncity_of_london_sdf &lt;- ward_gb |&gt;\n    filter(LAD22NM == \"City of London\")\n\n\nWe have now effectively filtered out the 25 2022 Wards that fall with in the City of London Local Authority District. We now need to assign our ethnic group data to this. We will first add the data and subsequently divide the overall counts equally across the Wards.\n\n\n\n\n\n\nThis is a bit of a lazy approach. Much better would be to actually try and find the actual Ward counts from the 2021 Census.\n\n\n\nTo join the ethnicity_london dataset to the City of London wards is a bit tricky — because the ethnicity_london dataset does not contain the actual Wards codes for the City of London Wards. Fortunately for us, the London Data Store was a bit cheeky and in the ethnic group file they created they put the Local Authority District code of the City of London into the Ward column. We can use this to our advantage and join our datasets together with a left_join.\n\n\n\nR code\n\n# join\ncity_of_london_sdf &lt;- city_of_london_sdf |&gt;\n  left_join(ethnicity_london, by = c(\"LAD22CD\" = \"ward22cd\"))\n\n# inspect\nhead(city_of_london_sdf)\n\n\nSimple feature collection with 6 features and 31 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 531866.1 ymin: 180540.4 xmax: 533617.7 ymax: 182084.5\nProjected CRS: OSGB36 / British National Grid\n     WD22CD       WD22NM WD22NMW   LAD22CD        LAD22NM  BNG_E  BNG_N\n1 E05009288   Aldersgate         E09000001 City of London 532169 181721\n2 E05009289      Aldgate         E09000001 City of London 533397 181175\n3 E05009290    Bassishaw         E09000001 City of London 532438 181495\n4 E05009291 Billingsgate         E09000001 City of London 533151 180754\n5 E05009292  Bishopsgate         E09000001 City of London 533207 181664\n6 E05009293 Bread Street         E09000001 City of London 532224 181151\n      LONG     LAT Shape_Leng                               GlobalID\n1 -0.09645 51.5190   1726.380 {E218C4C6-CC77-423C-9312-010363F61625}\n2 -0.07896 51.5138   1895.285 {E968A26F-9AF6-460A-A075-7F8AB975CB16}\n3 -0.09266 51.5169   1428.356 {8B433738-7452-4891-A3B0-17498C13F8AD}\n4 -0.08267 51.5100   1670.199 {7FF90094-80A5-466D-AE07-B30EC9966CCC}\n5 -0.08152 51.5182   2748.846 {DBE8B6D5-0C08-4FB5-9C36-D2F7483B4792}\n6 -0.09587 51.5138   2082.841 {1EC7A9B3-EAB0-4CC5-8627-354BA7865D6C}\n                     ward22nm all_pop white_british white_irish white_gypsy\n1 City of London (aggregated)    7375          4243         180           3\n2 City of London (aggregated)    7375          4243         180           3\n3 City of London (aggregated)    7375          4243         180           3\n4 City of London (aggregated)    7375          4243         180           3\n5 City of London (aggregated)    7375          4243         180           3\n6 City of London (aggregated)    7375          4243         180           3\n  white_other mixed_white_asian mixed_white_african mixed_white_caribbean\n1        1373               111                  37                    38\n2        1373               111                  37                    38\n3        1373               111                  37                    38\n4        1373               111                  37                    38\n5        1373               111                  37                    38\n6        1373               111                  37                    38\n  mixed_other asian_bangladeshi asian_chinese asian_indian asian_pakistani\n1         103               232           263          216              16\n2         103               232           263          216              16\n3         103               232           263          216              16\n4         103               232           263          216              16\n5         103               232           263          216              16\n6         103               232           263          216              16\n  asian_other black_african black_caribbean black_other other_arab other\n1         213            98              46          49         69    85\n2         213            98              46          49         69    85\n3         213            98              46          49         69    85\n4         213            98              46          49         69    85\n5         213            98              46          49         69    85\n6         213            98              46          49         69    85\n                           SHAPE\n1 MULTIPOLYGON (((532248.7 18...\n2 MULTIPOLYGON (((533466.1 18...\n3 MULTIPOLYGON (((532536.2 18...\n4 MULTIPOLYGON (((533320.1 18...\n5 MULTIPOLYGON (((533404.6 18...\n6 MULTIPOLYGON (((532025 1813...\n\n\nThis seems to have worked quite nicely, except that now all our data points have been duplicated. We need to fix this by dividing all counts by 25 (the number of wards) to equally divide the counts across the City of London. If we have to do this for every column individually there is a lot of typing involved, but fortunately we can use the mutate_at() function from the dplyr library to do this for the columns in one go.\n\n\n\n\n\n\nCheck the documentation of the mutate_at() function to see how it works in detail, but essentially you provide a function, in this case “divide a number by the total number of Wards” and do this for all columns that are specified.\n\n\n\n\n\n\nR code\n\n# divide counts by number of Wards\ncity_of_london_sdf &lt;- city_of_london_sdf |&gt;\n    mutate_at(c(13:31), function(x) as.integer(x/nrow(city_of_london_sdf)))\n\n# inspect\nhead(city_of_london_sdf)\n\n\nSimple feature collection with 6 features and 31 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 531866.1 ymin: 180540.4 xmax: 533617.7 ymax: 182084.5\nProjected CRS: OSGB36 / British National Grid\n     WD22CD       WD22NM WD22NMW   LAD22CD        LAD22NM  BNG_E  BNG_N\n1 E05009288   Aldersgate         E09000001 City of London 532169 181721\n2 E05009289      Aldgate         E09000001 City of London 533397 181175\n3 E05009290    Bassishaw         E09000001 City of London 532438 181495\n4 E05009291 Billingsgate         E09000001 City of London 533151 180754\n5 E05009292  Bishopsgate         E09000001 City of London 533207 181664\n6 E05009293 Bread Street         E09000001 City of London 532224 181151\n      LONG     LAT Shape_Leng                               GlobalID\n1 -0.09645 51.5190   1726.380 {E218C4C6-CC77-423C-9312-010363F61625}\n2 -0.07896 51.5138   1895.285 {E968A26F-9AF6-460A-A075-7F8AB975CB16}\n3 -0.09266 51.5169   1428.356 {8B433738-7452-4891-A3B0-17498C13F8AD}\n4 -0.08267 51.5100   1670.199 {7FF90094-80A5-466D-AE07-B30EC9966CCC}\n5 -0.08152 51.5182   2748.846 {DBE8B6D5-0C08-4FB5-9C36-D2F7483B4792}\n6 -0.09587 51.5138   2082.841 {1EC7A9B3-EAB0-4CC5-8627-354BA7865D6C}\n                     ward22nm all_pop white_british white_irish white_gypsy\n1 City of London (aggregated)     295           169           7           0\n2 City of London (aggregated)     295           169           7           0\n3 City of London (aggregated)     295           169           7           0\n4 City of London (aggregated)     295           169           7           0\n5 City of London (aggregated)     295           169           7           0\n6 City of London (aggregated)     295           169           7           0\n  white_other mixed_white_asian mixed_white_african mixed_white_caribbean\n1          54                 4                   1                     1\n2          54                 4                   1                     1\n3          54                 4                   1                     1\n4          54                 4                   1                     1\n5          54                 4                   1                     1\n6          54                 4                   1                     1\n  mixed_other asian_bangladeshi asian_chinese asian_indian asian_pakistani\n1           4                 9            10            8               0\n2           4                 9            10            8               0\n3           4                 9            10            8               0\n4           4                 9            10            8               0\n5           4                 9            10            8               0\n6           4                 9            10            8               0\n  asian_other black_african black_caribbean black_other other_arab other\n1           8             3               1           1          2     3\n2           8             3               1           1          2     3\n3           8             3               1           1          2     3\n4           8             3               1           1          2     3\n5           8             3               1           1          2     3\n6           8             3               1           1          2     3\n                           SHAPE\n1 MULTIPOLYGON (((532248.7 18...\n2 MULTIPOLYGON (((533466.1 18...\n3 MULTIPOLYGON (((532536.2 18...\n4 MULTIPOLYGON (((533320.1 18...\n5 MULTIPOLYGON (((533404.6 18...\n6 MULTIPOLYGON (((532025 1813...\n\n\nThat is much better. The last thing we now need to do is to combine our city_of_london_sdf with our ethnicity_london_sdf. Because both spatial dataframe contain the exact same data and column names, we can simply bind them together.\n\n\n\nR code\n\n# bind spatial dataframes\nethnicity_london_sdf &lt;- ethnicity_london_sdf |&gt;\n    rbind(city_of_london_sdf)\n\n\nLet’s check whether our approach worked by plotting the updated spatial dataframe:\n\n\n\nR code\n\n# inspect\nplot(ethnicity_london_sdf, max.plot = 1)\n\n\n\n\n\nFigure 3: Quick plot of the full Ward spatial dataframe.\n\n\n\n\nThis looks much better: there are no more obvious holes in our spatial dataframe and we can move on."
  },
  {
    "objectID": "08-autocorrelation.html#statistical-distributions",
    "href": "08-autocorrelation.html#statistical-distributions",
    "title": "1 Analysing Spatial Patterns III: Spatial Autocorrelation",
    "section": "",
    "text": "Today, we are interested in looking at spatial autocorrelation: the effect of spatial processes on distributions. We will be using our newly created ethnicity_london_sdf to look at this in action. Before we do this, however, let’s start by looking at the data distribution.\n\n\n\n\n\n\nAnalysing the distribution of your data and summarising the main characteristics of its distribution is known as Exploratory Data Analysis (EDA). EDA was promoted by prominent statistician John Tukey to encourage data analysts to explore their data outside of traditional formal modelling and come up with new areas of investigation and hypotheses. Tukey promoted the use of five summary statistics: the maximum, the minimum, the median, and the quartiles, which, in comparison to the mean and standard deviation, provide a more robust understanding of a dataset’s distribution, particularly if the data is skewed.\n\n\n\nWe looked at how we can use R to extract some of these summary statistics briefly in Week 4’s computer tutorial, but let’s have a look at how we can add further to this EDA. A simple and straightforward way to extract the main characteristics of a dataset is by using the summary() function.\n\n\n\n\n\n\nThe summary() function can be called on a dataset as a whole and will generate summary statistics for each numeric variable.\n\n\n\n\n\n\nR code\n\n# summarise dataframe, but exclude geometry column\nsummary(ethnicity_london_sdf |&gt;\n    st_drop_geometry())\n\n\n    WD22CD             WD22NM            WD22NMW            LAD22CD         \n Length:704         Length:704         Length:704         Length:704        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n   LAD22NM              BNG_E            BNG_N             LONG         \n Length:704         Min.   :505661   Min.   :157756   Min.   :-0.47645  \n Class :character   1st Qu.:523789   1st Qu.:174575   1st Qu.:-0.21930  \n Mode  :character   Median :531040   Median :181022   Median :-0.11311  \n                    Mean   :530499   Mean   :180206   Mean   :-0.12106  \n                    3rd Qu.:537483   3rd Qu.:186252   3rd Qu.:-0.02262  \n                    Max.   :557943   Max.   :199291   Max.   : 0.27621  \n      LAT          Shape_Leng        GlobalID           ward22nm        \n Min.   :51.30   Min.   :  965.1   Length:704         Length:704        \n 1st Qu.:51.46   1st Qu.: 4888.3   Class :character   Class :character  \n Median :51.51   Median : 6496.1   Mode  :character   Mode  :character  \n Mean   :51.51   Mean   : 7213.4                                        \n 3rd Qu.:51.56   3rd Qu.: 8619.1                                        \n Max.   :51.68   Max.   :35083.0                                        \n    all_pop      white_british    white_irish    white_gypsy    \n Min.   :   51   Min.   :   10   Min.   :   1   Min.   :  0.00  \n 1st Qu.: 9641   1st Qu.: 3102   1st Qu.: 148   1st Qu.:  3.00  \n Median :11712   Median : 4854   Median : 227   Median :  8.00  \n Mean   :11611   Mean   : 5212   Mean   : 250   Mean   : 11.63  \n 3rd Qu.:14467   3rd Qu.: 7077   3rd Qu.: 323   3rd Qu.: 14.00  \n Max.   :21292   Max.   :14727   Max.   :1345   Max.   :216.00  \n  white_other     mixed_white_asian mixed_white_african mixed_white_caribbean\n Min.   :  11.0   Min.   :  0.0     Min.   :  1.0       Min.   :  1.00       \n 1st Qu.: 767.5   1st Qu.:101.0     1st Qu.: 48.0       1st Qu.: 85.75       \n Median :1299.5   Median :142.0     Median : 79.0       Median :140.00       \n Mean   :1468.7   Mean   :144.2     Mean   : 93.0       Mean   :169.62       \n 3rd Qu.:2057.2   3rd Qu.:185.2     3rd Qu.:125.2       3rd Qu.:223.00       \n Max.   :4708.0   Max.   :442.0     Max.   :403.0       Max.   :800.00       \n  mixed_other    asian_bangladeshi asian_chinese     asian_indian   \n Min.   :  1.0   Min.   :   3.0    Min.   :   0.0   Min.   :   3.0  \n 1st Qu.:109.0   1st Qu.:  42.0    1st Qu.:  83.0   1st Qu.: 191.0  \n Median :158.5   Median :  92.5    Median : 140.5   Median : 323.5  \n Mean   :168.8   Mean   : 315.5    Mean   : 176.5   Mean   : 771.1  \n 3rd Qu.:225.2   3rd Qu.: 201.0    3rd Qu.: 214.2   3rd Qu.: 765.2  \n Max.   :469.0   Max.   :6852.0    Max.   :1344.0   Max.   :7763.0  \n asian_pakistani   asian_other     black_african    black_caribbean \n Min.   :   0.0   Min.   :   1.0   Min.   :   3.0   Min.   :   1.0  \n 1st Qu.:  60.0   1st Qu.: 251.8   1st Qu.: 278.8   1st Qu.: 132.5  \n Median : 120.5   Median : 430.5   Median : 569.0   Median : 286.0  \n Mean   : 317.9   Mean   : 566.0   Mean   : 815.2   Mean   : 489.5  \n 3rd Qu.: 331.8   3rd Qu.: 712.8   3rd Qu.:1123.8   3rd Qu.: 676.2  \n Max.   :3633.0   Max.   :3225.0   Max.   :5297.0   Max.   :3523.0  \n  black_other       other_arab          other       \n Min.   :   1.0   Min.   :   1.00   Min.   :   0.0  \n 1st Qu.:  65.0   1st Qu.:  46.75   1st Qu.: 112.0  \n Median : 174.0   Median :  91.00   Median : 199.0  \n Mean   : 241.6   Mean   : 150.59   Mean   : 248.6  \n 3rd Qu.: 339.0   3rd Qu.: 169.00   3rd Qu.: 339.8  \n Max.   :1563.0   Max.   :1693.00   Max.   :1411.0  \n\n\nThis gives us an overview of all variables, but let’s have a look at our population group of interest for today’s practical by creating a histogram.\n\n\n\nR code\n\n# histogram\nhist(ethnicity_london_sdf$asian_bangladeshi)\n\n\n\n\n\nFigure 4: Histogram of the distribution of the self-identified Asian-Bangladeshi population group.\n\n\n\n\nWe can actually see our data has a very strong negative skew with the majority of Wards having a relatively low number of individual self-identifying as Asian-Bangladeshi, but there are also some Wards where a large number of self-identified Asian-Bangladeshis are residing.\nAnother type of chart we can create just using the base R library is a boxplot. A boxplot shows the core characteristics of the distributions within a dataset, including the interquartile range.\n\n\n\n\n\nFigure 5: Simple boxplot. [Enlarge image]\n\n\n\n\nPlot the boxplot of our asian_bangladeshi variable:\n\n\n\nR code\n\n# histogram\nboxplot(ethnicity_london_sdf$asian_bangladeshi, horizontal = TRUE)\n\n\n\n\n\nFigure 6: Boxplot of the distribution of the self-identified Asian-Bangladeshi population group.\n\n\n\n\nAgain, it is clear that the majority of Wards have a relatively low number of individuals that self-identify as Asian-Bangladeshi, but there is a good number of outliers. This raises the question whether these outliers are random or also clustered in space — and this brings us to spatial autocorrelation.\n\n\n\n\n\n\nThere is actually a lot more we can do in terms of visualising our data’s distribution and the best way forward would be to become more familiar with the ggplot2 library. ggplot2 is the main visualisation for both statistical and, increasingly, spatial graphs, charts and maps. Refer back to Week 4’s optional suggestions on how to get started with ggplot2."
  },
  {
    "objectID": "08-autocorrelation.html#spatial-distributions",
    "href": "08-autocorrelation.html#spatial-distributions",
    "title": "1 Analysing Spatial Patterns III: Spatial Autocorrelation",
    "section": "",
    "text": "Whilst statistical analysis of distributions focus on tests and charts, when we want to understand the spatial distribution of our phenomena, we have a very simple solution: we make a map. In our case, we are looking at areal unit data and therefore we can use a choropleth map to study our data across the Wards. In fact, we can actually create a sequence of maps not only covering the self-identified Asian-Bangladeshis but also other population groups.\n\n\n\nR code\n\n# store variables of interest as separate variable\nvar_fields &lt;- names(ethnicity_london_sdf)[14:31]\n\n# add Ward boundaries\ntm_shape(ethnicity_london_sdf) +\n  tm_polygons(\n    col = \"gray\"\n  ) +\n  # map all variables\n  tm_shape(ethnicity_london_sdf) +\n  # select variables\n  tm_polygons(\n    col = var_fields\n  ) +\n  # add layout options\n  tm_layout(\n    legend.show = FALSE\n  ) +\n  # add 4 columns\n  tm_facets(\n    ncol = 4\n  )\n\n\n\n\n\nFigure 7: Facet map of all our population groups.\n\n\n\n\nDespite these maps being a little small and not containing any labels, it seems that different population groups indeed concentrate in different parts of London. Let’s zoom into the Asian-Bangladeshi population group and add a legend:\n\n\n\nR code\n\n# map a specific variable\ntm_shape(ethnicity_london_sdf) +\n  tm_polygons(\n    col = \"gray\"\n  ) +\n  tm_shape(ethnicity_london_sdf) +\n  tm_polygons(\n    col = \"asian_bangladeshi\",\n    n = 5,\n    style = \"jenks\"\n  ) +\n  tm_layout(\n    legend.outside = TRUE,\n    legend.outside.position = \"right\"\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 5, 10, 15, 20),\n    position = c(\"left\", \"bottom\")\n  )\n\n\n\n\n\nFigure 8: Quick map of the self-identified Asian-Bangladeshi population group.\n\n\n\n\n\n\n\n\n\n\nPlease remember, whereas the above map is fine for a quick inspection, it is technically incorrect because we are showing absolute numbers on a choropleth. This is something we should never do, unless the spatial units are identical in size (e.g. a hexagonal tessellation of an area), because larger areas will draw attention and affect the visualisation. To be fair, even for a quick inspection, it would be much better to normalise these counts by the total number of people living within each of the Wards to get a more honoust picture of the distribution.\n\n\n\nThe thing with spatial distributions is that we can quickly pick up on spatial patterns present within our data just by looking at the data. For example, we can clearly see some concentrations of people that self-identify as Asian-Bangladeshis in East London. The question now is whether these clusters are significant from a statistical point of view. This brings us to measuring spatial correlation.\nBefore we move on, let’s normalise the asian_banglades variable by creating a new variable that contains the proportion of individuals that self-identify as Asian-Bangladeshi:\n\n\n\nR code\n\n# calculate proportions\nethnicity_london_sdf &lt;- ethnicity_london_sdf |&gt;\n    mutate(asian_bangladeshi_prop = asian_bangladeshi/all_pop)"
  },
  {
    "objectID": "08-autocorrelation.html#spatial-autocorrelation",
    "href": "08-autocorrelation.html#spatial-autocorrelation",
    "title": "1 Analysing Spatial Patterns III: Spatial Autocorrelation",
    "section": "",
    "text": "We can assess the distribution of our data using what is known as spatial autocorrelation tests, which can be conducted on both a global (identify if the data is clustered) and local (identify the precise clusters) scales. Whilst these different tests quantify how clustered, how random, or how dispersed, these distributions are through various approaches, ultimately they provide us with statistical and spatial information that can be used to create quantifiable descriptions of a variable’s distribution and how it vary over space.\nAs discussed in this week’s lecture, we have several types of tests that look to quantify spatial autocorrelation. Of these tests, there are two main categories:\n\nMeasures of global spatial aucorrelation: tests that provide us with a statistic to tell whether spatial autocorrelation is present in our dataset.\nMeasures of local spatial autocorrelation: tests that break down the global patterns and essentially tell us where we can find clusters and outliers.\n\nThree of the most frequently used tests are the Global Moran’s I, the Local Moran’s I, and the Getis-Ord Gi*:\n\n\n\n\n\n\n\n\n\nTest\nScale\nTest\nOutput\n\n\n\n\nGlobal Moran’s I\nGlobal\nTests how “random” the spatial distribution of values are, producing a correlation coefficient for the relationship between a variable and its surrounding values.\nMetric between \\(-1\\) and \\(1\\).\n\n\nLocal Moran’s I\nLocal\nTests the difference between a unit of analysis and its neighbour(s).\nCan be used alongside the mean of values to generate cluster type generations.\n\n\nGetis-Ord Gi*\nLocal\nIdentifies statistically significant hot spots and cold spots using the local Getis-Ord \\(Gi*\\) statistic.\nThe returned \\(z\\)-scores can be used to identify statistically significant clusters.\n\n\n\n\n\n\n\n\n\nIn each of these cases, our \\(p\\)-values are pseudo \\(p\\)-values, generated through simulations such as those outlined in the lecture. Our pseudo \\(p\\)-values allow us to interpret our relationships with a level of confidence. If we find that our relationships do not have any significance, then we cannot be confident in presenting them as true results.\n\n\n\n\n\nUnderlying our global Moran’s I test is the concept of a spatial lag model. A spatial lag model plots each value against the mean of its neighbours’ values, defined by our selected approach. This creates a scatter plot, from which our Moran’s I statistic can be derived.\nAn Ordinary Least Squares (OLS) regression is used to fit the data and produce a slope, which determines the Moran’s I statistic:\n\n\n\n\n\nFigure 9: A spatial lag model - plotting value against the mean of its neighbours. Source: Manuel Gimond. [Enlarge image]\n\n\n\n\nTo determine a \\(p\\)-value from our model for global Moran’s I, this spatial lag model is computed multiple times (think hundreds, thousands) but uses a random distribution of neighbouring values to determine different slopes for multiple ways our data could be distributed if our data was distributed randomly. The output of this test is a sampling distribution of Moran’s I values that would confirm a null hypothesis that our values are randomly distributed. These slopes are then compared to compare our observed slope versus our random slopes and identify whether the slope is within the main distribution of these values or an outlier:\n\n\n\n\n\nFigure 10: Determining significance using a Monte Carlo simulation. Source: Manuel Gimond. [Enlarge image]\n\n\n\n\nIf our slope is an outlier, i.e. not a value we would expect to compute if the data were randomly distributed, we are more confidently able to confirm our slope is reflective of our data’s clustering and is significant. Our pseudo-\\(p\\)-values are then computed from our simulation results:\n\\[\n\\frac{N_{extreme} + 1}{N + 1}\n\\]\nWhere \\({N_{extreme}}\\) is the number of simulated Moran’s I values that were more extreme that our observed statistic and \\({N}\\) is the total number of simulations. In the example above, from Manuel Gimond, only 1 out the 199 simulations was more extreme than the observed local Moran’s I statistic. Therefore \\({N_{extreme}}\\) = 1 , so \\(p\\) is equal to \\((1+1) / (199 + 1) = 0.01\\). This means that “there is a 1% probability that we would be wrong in rejecting the null hypothesis”. This approach is known as a Monte Carlo simulation or permutation bootstrap test.\n\n\n\nFor any spatial autocorrelation test that you want to conduct, you will always need one critical piece of information: how do we define ‘neighbours’ in our dataset to enable the value comparison. Every observation in a dataset will need to have a set of neighbours to which its value is compared. To enable this, we need to determine how many or what type of neighbours should be taken into account for each observation when conducting a spatial autocorrelation test. These ‘neighbouring’ observations can be defined in a multitude of ways, based either on geometry or proximity, and include:\n\nContiguity: Queen [nodes have to touch] or Rook [edges have to touch]\nFixed Distance: Euclidean Distance [specified distance]\n(K) Nearest Neighbours: \\(n\\) closest neighbours\n\n\n\n\n\n\nFigure 11: Different approaches of conceptualising neighbours for spatial autocorrelation measurement: contiguity, fixed distance and nearest neighbours. Source: Manuel Gimond. [Enlarge image]\n\n\n\n\nDepending on the variable you are measuring, the appropriateness of these different types of neighbourhood calculation techniques can change. As a result, how you define neighbours within your dataset will have an impact on the validity and accuracy of spatial analysis. Whatever approach you choose therefore needs to be grounded in particular theory that aims to represent the process and variable investigated.\n\n\n\n\n\n\nHave a look at Esri’s Help Documentation on Selecting a conceptualization of spatial relationships: Best practices when you come to need to define neighbours yourself for your own analysis.\n\n\n\nFor our analysis into the clustering of population groups, we will primarily use the Queen contiguity. This approach is “effective when polygons are similar in size and distribution, and when spatial relationships are a function of polygon proximity (the idea that if two polygons share a boundary, spatial interaction between them increases)” (Esri, 2024).\n\n\n\nBefore we can calculate Moran’s I and any similar statistics, we need to first define our spatial weights matrix. This is known mathematically as \\(W_{ij}\\) and this will tell our code which unit neighbours which, according to our neighbour definition. For each neighbour definition, there is a different approach to implementing code to calculate the \\(W_{ij}\\) spatial weights matrix. We will look at three approaches:\n\nCreating a Queen \\(W_{ij}\\) spatial weights matrix\nCreating a Rook \\(W_{ij}\\) spatial weights matrix\nCreating a Fixed Distance \\(W_{ij}\\) spatial weights matrix\n\nFor either approach, we use a single line of code to create the relevant \\(W_{ij}\\) spatial weights matrix:\n\n\n\nR code\n\n# Queens neighbours\nward_neighbours_queen &lt;- ethnicity_london_sdf |&gt;\n    poly2nb(queen = T)\n\n# Rook neighbours\nward_neighbours_rook &lt;- ethnicity_london_sdf |&gt;\n    poly2nb(queen = F)\n\n# Fixed distance neighbours\nward_neighbours_fd &lt;- dnearneigh(st_geometry(st_centroid(ethnicity_london_sdf)),\n    0, 4000)\n\n\nWarning in st_centroid.sf(ethnicity_london_sdf): st_centroid assumes attributes\nare constant over geometries of x\n\n\nCreating our neighbours list through a single line of code, as above, does not really tell us much about the differences between these different definitions. It would be useful to the links between neighbours for our three definitions and visualise their distribution across space. To be able to do this, we will use a few lines of code to generate a visualisation based on mapping the defined connections between the centroids of our Wards.\n\n\n\n\n\n\nA centroid in its most simplest form is the central point of an areal unit. How this central point is defined can be weighted by different approaches to understanding geometries or by using an additional variable. In our case, our centroids will reflect in the geometric middle point of our Wards.\n\n\n\nWe can calculate the centroids of our Wards using one of the geometric tools from the sf library: sf_centroid().\n\n\n\nR code\n\n# calculate the centroids of all of the Wards in London\nward_centroid &lt;- ethnicity_london_sdf |&gt;\n    st_centroid()\n\n\nWarning in st_centroid.sf(ethnicity_london_sdf): st_centroid assumes attributes\nare constant over geometries of x\n\n\n\n\n\n\n\n\nWe actually already used this function above as the creation of the fixed distance spatial weights matrix requires a point geometry and then calculates which other point geometries are within the specified distance.\n\n\n\nNow we have our Ward centroids, we can go ahead and plot the centroids and the defined neighbour connections between them from each of our neighbour definitions. To do so, we will use the plot() function, provide the relationships via our ward_neighbours_* lists and then the geometry associated with these lists from our ward_centroid object:\n\n\n\nR code\n\n# plot neighbours: Queen\nplot(ward_neighbours_queen, st_geometry(ward_centroid), col = \"red\", pch = 20, cex = 0.5)\n\n\n\n\n\nFigure 12: 2022 Wards with a Queen neighbourhood definition.\n\n\n\n\n\n\n\nR code\n\n# plot neighbours: Rook\nplot(ward_neighbours_rook, st_geometry(ward_centroid), col = \"blue\", pch = 20, cex = 0.5)\n\n\n\n\n\nFigure 13: 2022 Wards with a Rook neighbourhood definition.\n\n\n\n\n\n\n\nR code\n\n# plot neighbours: Fixed distance\nplot(ward_neighbours_fd, st_geometry(ward_centroid), col = \"red\", pch = 20, cex = 0.5)\n\n\n\n\n\nFigure 14: 2022 Wards with a Fixed distance neighbourhood definition.\n\n\n\n\n\n\n\n\n\n\nIn this example we use four kilometres as a distance threshold (i.e. centroids are considered neighbours if they are within three kilometres from one another, however, this is an arbitrary distance. When using a fixed distance make sure you have a good reason to select the distance (e.g. used in a paper that used the same administrative geographies).\n\n\n\nWhen comparing these different maps, we can see that there is definitely a difference in the number of neighbours when we use our different approaches. It seems our fixed distance neighbour conceptualisation has many connections in the centre of London versus areas on the outskirts. We can see that our contiguity approaches provide a more equally distributed connection map, with our Queen conceptualisation having a few more links that our Rook conceptualisation.\n\n\n\n\n\n\nWhichever form of neighbours you are using, always check the results, especially those of the contiguity based approaches. If the spatial file that you are using is not in good shape (e.g. polygons seem to touch upon visual inspection but actually do not), your results will be compromised.\n\n\n\nWe can also type the different neighbours objects into the console to find out the total number of non-zero links (i.e. total number of connections) present within the conceptualisation. You should see that Queen has 4022 non-zero links, Rook has 3854 and Fixed Difference has 12462 Whilst this code simply explores these conceptualisations it helps us understand further how our different neighbourhood conceptualisations can ultimately impact our overall analysis.\n\n\n\n\n\n\nRemember: the number of links essentially determines the value of the spatial lag variable — i.e. the value of a certain variable in comparison with its neighbours.\n\n\n\nWith our neighbours now defined, we will go ahead and create our final spatial weights objects that will be needed for our spatial autocorrelation code. At the moment, we have our neighbours defined as a list but we need to convert it to a neighbours object using the nb2listw() function:\n\n\n\nR code\n\n# create a neighbours list - Queen\nward_spatial_weights_queen &lt;- ward_neighbours_queen |&gt;\n    nb2listw(style = \"W\")\n\n# create a neighbours list - Rook\nward_spatial_weights_queen &lt;- ward_neighbours_rook |&gt;\n    nb2listw(style = \"W\")\n\n# create a neighbours list\nward_spatial_weights_fd &lt;- ward_neighbours_fd |&gt;\n    nb2listw(style = \"W\", zero.policy = TRUE)\n\n\n\n\n\n\n\n\nThe style that we specify in the code above determines how neighbours are weighted. style = 'W' row-standardises the values, e.g. if a Ward has five neighbours the value of the spatially lagged variable of interest will be the average of the variable of interest of these five neighbours — every neighbour has an equal weight. See ?nb2listw for more information on the different styles.\n\n\n\n\n\n\n\n\n\nFor the fixed distance measure we need to specify an additional parameter (zero.policy). This is because it is possible that there are Wards without any neighbour (i.e. there is a Ward with a centroid that is not within our specified distance). This is not ideal, because you would want every unit to have at least one neighbour. We do not need this parameter in case of the contiguity based measures today because every Ward polygon touches at least one other Ward polygon — provided our spatial data are in order.\n\n\n\n\n\n\nWith a Global Moran’s I we test how random the spatial distribution of our values are, producing a global Moran’s statistic from the lag approach explained earlier.\nThe global Moran’s I statistic is a metric between \\(-1\\) and \\(1\\):\n\n\\(-1\\) suggests a completely even spatial distribution of values\n\\(0\\) suggests a random distribution\n\\(1\\) suggests a non-random distribution of clearly defined clusters\n\nBefore we run our global Moran’s I test, we will first create a spatial lag model plot which looks at each of the values plotted against their spatially lagged values. The graph will show quickly whether we are likely to expect our test to return a positive, zero, or negative statistic:\n\n\n\nR code\n\n# Moran's plot\nmoran.plot(ethnicity_london_sdf$asian_bangladeshi_prop, listw = ward_spatial_weights_queen)\n\n\n\n\n\nFigure 15: Plot of lagged values versus polygon values using the Queen neigbhourhood definition.\n\n\n\n\nWe can see that there is a positive relationship between our asian_bangladeshi_prop variable and the spatially lagged asian_bangladeshi_prop variable, therefore we are expecting our global Moran’s I test to produce a statistic reflective of the slope visible in our scatter plot. Now we can run the global Moran’s I spatial autocorrelation test:\n\n\n\nR code\n\n# Moran's I\nmoran_queen &lt;- ethnicity_london_sdf |&gt;\n    pull(asian_bangladeshi_prop) |&gt;\n    as.vector() |&gt;\n    moran.test(ward_spatial_weights_queen)\n\n# inspect result\nmoran_queen\n\n\n\n    Moran I test under randomisation\n\ndata:  as.vector(pull(ethnicity_london_sdf, asian_bangladeshi_prop))  \nweights: ward_spatial_weights_queen    \n\nMoran I statistic standard deviate = 34.376, p-value &lt; 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.777712178      -0.001422475       0.000513713 \n\n\nThe Moran’s I statistic calculated should be 0.73. With \\(1\\) = clustered, \\(0\\) = no pattern, \\(-1\\) = dispersed, this means we can confirm that the population in London that self-identifies as Asian-Bangladeshi is strongly positively autocorrelated. In other words, self-identified Asian-Bangladeshis in London tend to reside in similar areas. We can also consider the pseudo \\(p\\)-value as a measure of the statistical significance of the model - at &lt; 2.2e-16, which confirm our result is statistically significant.\n\n\n\n\n\n\nBefore we run our local spatial autocorrelation tests, let’s just take a second to think through what our results have shown. From our global statistical tests, we can confirm that:\n\nThere is clustering in our dataset.\nSimilar values are clustering.\nHigh values are clustering.\n\nWe can conclude already that Wards with a relatively large proportion of people that self-identify as Asian-Bangladeshis tend to cluster in the same area. What we do not know yet is where these clusters are occurring. To help with this, we need to run our local models to identify where these clusters are located.\n\n\n\n\n\n\nA local Moran’s I test deconstructs the global Moran’s I down to its components and then constructs a localised measure of autocorrelation, which can show different cluster types. To run a local Moran’s I test, the code is similar to above:\n\n\n\nR code\n\n# Local Moran's I\nlocal_moran_queen &lt;- ethnicity_london_sdf |&gt;\n    pull(asian_bangladeshi_prop) |&gt;\n    as.vector() |&gt;\n    localmoran(ward_spatial_weights_queen)\n\n# inspect result\nhead(local_moran_queen)\n\n\n         Ii         E.Ii     Var.Ii      Z.Ii Pr(z != E(Ii))\n1 28.400091 -0.033787002  4.5702873 13.300389   2.302674e-40\n2  6.410822 -0.005144343  1.1975739  5.862879   4.549109e-09\n3  4.547442 -0.007928661  0.9163467  4.758760   1.947861e-06\n4 12.767600 -0.013075467  1.5033436 10.423767   1.931491e-25\n5 21.632012 -0.058105443  6.3758219  8.590013   8.695953e-18\n6 33.215970 -0.065154746 10.6742781 10.186588   2.276128e-24\n\n\nAs you should see, we are not given a single statistic as we did with our global test, but rather a table of different statistics that are all related back to each of the Wards in our dataset. If we look at the help page for the localmoran function we can find out what each of these statistics mean:\n\n\n\nName\nDescription\n\n\n\n\nIi\nLocal Moran’s I statistic\n\n\nE.Ii\nExpectation of local Moran’s I statistic\n\n\nVar.Ii\nVariance of local Moran’s I statistic\n\n\nZ.Ii\nStandard deviation of local Moran’s I statistic\n\n\nPr()\n\\(p\\)-value of local Moran’s I statistic\n\n\n\nWe therefore have a Moran’s I statistic for each of our Wards, as well as a significance value plus a few other pieces of information that can help us create some maps showing our clusters. To be able to do this, we need to join our local Moran’s I output back into our ethnicity_london_sdf spatial dataframe, which will then allow us to map these results.\nTo create this join, we first coerce our local Moran’s I output into a dataframe that we then join to our ethnicity_london_sdf spatial dataframe using the familiar mutate() function from the dplyr library. In our case, we do not need to provide an attribute to join these two dataframes together as we use the computer’s logic to join the data in the order in which it was created:\n\n\n\nR code\n\n# coerce to dataframe\nlocal_moran_queen &lt;- as.data.frame(local_moran_queen)\n\n# update the names for easier reference\nnames(local_moran_queen) &lt;- c(\"LMI_Ii\", \"LMI_eIi\", \"LMI_varIi\", \"LMI_zIi\", \"LMI_sigP\")\n\n# join\nethnicity_london_sdf &lt;- ethnicity_london_sdf |&gt;\n    mutate(local_moran_queen)\n\n\nWe now have the data we need to plot our local spatial autocorrelation maps. We will first plot the most simple maps to do with our local Moran’s I test: the local Moran’s I statistic.\n\n\n\nR code\n\n# map the local Moran's I statistic\ntm_shape(ethnicity_london_sdf) +\n  tm_polygons(\n    col = \"LMI_Ii\",\n    style = \"pretty\",\n    midpoint = 0,\n    title = \"Local Moran's I statistic\"\n  ) +\n  tm_layout(\n    main.title = \"Local Moran's I statistic\",\n    main.title.fontface = 2,\n    fontfamily = \"Helvetica\",\n    legend.outside = TRUE,\n    legend.outside.position = \"right\",\n    legend.title.size = 1,\n    legend.title.fontface = 2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"bottom\")\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 5, 10, 15, 20),\n    position = c(\"left\", \"bottom\")\n  )\n\n\n\n\n\nFigure 16: Cluster map of the self-identified Asian-Bangladeshi population group.\n\n\n\n\nFrom the map, it is possible to observe the variations in autocorrelation across space. We can interpret that there seems to be a geographic pattern to the autocorrelation. However, it is not possible to understand if these are clusters of high or low values or which ones are statistically significant. To be able to interpret this confidently, we also need to know the significance of the patterns we see in our map and therefore need to map the \\(p\\)-value of local Moran’s I statistic.\n\n\n\nR code\n\n# significance breaks\nbreaks &lt;- c(0, 0.05, 0.1, 1)\n\n# colour palette\ncolours &lt;- c(\"#ffffff\", \"#a6bddb\", \"#2b8cbe\")\n\n# map the local Moran's I statistic / significance only\ntm_shape(ethnicity_london_sdf) +\n  tm_polygons(\n    col = \"LMI_sigP\",\n    style = \"fixed\",\n    breaks = breaks,\n    palette = rev(colours),\n    title = \"p-value of Local Moran's I stat\"\n  ) +\n  tm_layout(\n    main.title = \"Significant clusters\",\n    main.title.fontface = 2,\n    fontfamily = \"Helvetica\",\n    legend.outside = TRUE,\n    legend.outside.position = \"right\",\n    legend.title.size = 1,\n    legend.title.fontface = 2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"bottom\")\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 5, 10, 15, 20),\n    position = c(\"left\", \"bottom\")\n  )\n\n\n\n\n\nFigure 17: Significant cluster map of the self-identified Asian-Bangladeshi population group.\n\n\n\n\nUsing our significance map, we can interpret the above clusters present in our local Moran’s I statistic more confidently. As evident, we do have several clusters that are statistically significant to the \\(p\\)-value &lt; 0.05.\nIdeally we would combine these two outputs to see what values cluster together as well as which clusters are significant. We can do this with a cluster map of our local Moran’s I statistic (Ii) which will show areas of different types of clusters, including:\n\nHIGH-HIGH: A Ward with a relatively high proportion of residents that self-identify as Asian-Bangladeshi that is also surrounded by other Wards with a relatively high proportion of residents that self-identify as Asian-Bangladeshi.\nHIGH-LOW: A Ward with a relatively high proportion of residents that self-identify as Asian-Bangladeshi that is surrounded by Wards with a relatively low proportion of residents that self-identify as Asian-Bangladeshi.\nLOW-HIGH: A Ward with a relatively low proportion of residents that self-identify as Asian-Bangladeshi that is surrounded by Wards with a relatively high proportion of residents that self-identify as Asian-Bangladeshi.\nLOW-LOW: A Ward with a relatively low proportion of residents that self-identify as Asian-Bangladeshi that is also surrounded by other Wards with a relatively low proportion of residents that self-identify as Asian-Bangladeshi.\n\nOur HIGH-HIGH and LOW-LOW will show our clusters, whereas the other two cluster types reveal anomalies in our variable. To create a map that shows this, we need to quantify the relationship each of our Wards have with the Wards around them to determine their cluster type. We do this using their observed value and their local Moran’s I statistic and their deviation around their respective means:\n\nIf a Ward’s observed value is higher than the observed mean and it’s local Moran’s I statistic is higher than the LMI mean, it is designated as HIGH-HIGH.\nIf a Ward’s observed value is lower than the observed mean and it’s local Moran’s I statistic is lower than the LMI mean, it is designated as LOW-LOW.\nIf a Ward’s observed value is lower than the observed mean but it’s local Moran’s I statistic is higher than the LMI mean, it is designated as LOW-HIGH.\nIf a Ward’s observed value is higher than the observed mean but it’s local Moran’s I statistic is lower than the LMI mean, it is designated as HIGH-LOW.\nIf a Ward’s LMI was found not to be significant, the Ward will be mapped as not significant.\n\nTo create this cluster map, we need to take several additional steps. We firstly need, for each Ward, whether its observed value is higher or lower than the mean observed. We then need to know, for each Ward, whether its LMI value is higher or lower than the mean LMI. Finally, we can use the values of these two columns to assign each Ward with a cluster type.\n\n\n\nR code\n\n# significance breaks\nethnicity_london_sdf &lt;- ethnicity_london_sdf |&gt;\n  mutate(obs_diff = (asian_bangladeshi_prop - mean(ethnicity_london_sdf$asian_bangladeshi_prop)))\n\n# compare local LMI value with mean LMI value\nethnicity_london_sdf &lt;- ethnicity_london_sdf |&gt;\n  mutate(LMI_diff = (ethnicity_london_sdf$LMI_Ii - mean(ethnicity_london_sdf$LMI_Ii)))\n\n# set significance threshold\nsignif &lt;- 0.05\n\n# generate column with cluster type, using values above\nethnicity_london_sdf &lt;- ethnicity_london_sdf |&gt;\n  mutate(cluster_type = case_when(\n    obs_diff &gt; 0 & LMI_diff &gt; 0 & LMI_sigP &lt; signif ~ \"High-High\",\n    obs_diff &lt; 0 & LMI_diff &lt; 0 & LMI_sigP &lt; signif ~ \"Low-Low\",\n    obs_diff &lt; 0 & LMI_diff &gt; 0 & LMI_sigP &lt; signif ~ \"Low-High\",\n    obs_diff &gt; 0 & LMI_diff &lt; 0 & LMI_sigP &lt; signif ~ \"High-Low\",\n    LMI_sigP &gt; signif ~ \"No Significance\"\n  ))\n\n\nNow we have a column detailing our cluster types, we can create a cluster map that details our four cluster types as well as those that are not significant. Creating a categorical map in R and using tmap is a little tricky and we will need to do some preparing of our colour palettes to ensure our data is mapped correctly. To do this, we first need to figure out how many cluster types we have in our cluster_type field:\n\n\n\nR code\n\n# count the cluster types\ncount(ethnicity_london_sdf, cluster_type)\n\n\nSimple feature collection with 4 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503575 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n     cluster_type   n                          SHAPE\n1       High-High  44 MULTIPOLYGON (((535466.2 18...\n2        High-Low  10 MULTIPOLYGON (((533648.3 18...\n3         Low-Low   1 MULTIPOLYGON (((533386.7 18...\n4 No Significance 649 MULTIPOLYGON (((520238.1 16...\n\n\nWe can see that we have three out of the four possible cluster types in our dataset — alongside the No Significance value. We therefore need to ensure our palette includes three colours for these cluster types, plus a white colour for No Significance:\n\n\n\nR code\n\n# create palette\npal &lt;- c(\"#d7191c\", \"#fdae61\", \"#2c7bb6\", \"#F5F5F5\")\n\n\nNow we can finally map our clusters:\n\n\n\nR code\n\n# plot the different clusters\ntm_shape(ethnicity_london_sdf) +\n  tm_polygons(\n    col = \"cluster_type\",\n    palette = pal,\n    title = \"Cluster Type\"\n  ) +\n  tm_layout(\n    main.title = \"Cluster Map Asian Bangladeshis in London\",\n    main.title.fontface = 2,\n    fontfamily = \"Helvetica\",\n    legend.outside = TRUE,\n    legend.outside.position = \"right\",\n    legend.title.size = 1,\n    legend.title.fontface = 2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"bottom\")\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 5, 10, 15, 20),\n    position = c(\"left\", \"bottom\")\n  )\n\n\n\n\n\nFigure 18: Cluster types map of the self-identified Asian-Bangladeshi population group.\n\n\n\n\nAnd there we have it: within one map we can visualise both the relationship of our Wards to their respective neighbourhoods and the significance of this relationship from our local Moran’s I test. This type of map is called a LISA map and is a great way of showing how a variable is actually clustering.\n\n\n\nThe final test we will run today is the local Getis-Ord, which will produce the \\(Gi*\\) statistic. This statistic will identify hot- and coldspots by looking at the neighbours within a defined proximity to identify where either high or low values cluster spatially and recognising statistically significant hotspots as those areas of high values where other areas within a neighbourhood range also share high values too (and vice versa for coldspots).\n\n\n\nR code\n\n# Gi* test\ngi_queen &lt;- ethnicity_london_sdf |&gt;\n    pull(asian_bangladeshi_prop) |&gt;\n    as.vector() |&gt;\n    localG(ward_spatial_weights_queen)\n\n# join\nethnicity_london_sdf &lt;- ethnicity_london_sdf |&gt;\n    mutate(asian_bangladeshi_gi = as.numeric(gi_queen))\n\n# inspect\nethnicity_london_sdf\n\n\nSimple feature collection with 704 features and 41 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503575 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\nFirst 10 features:\n      WD22CD                  WD22NM WD22NMW   LAD22CD       LAD22NM  BNG_E\n1  E05009317           Bethnal Green         E09000030 Tower Hamlets 535607\n2  E05009318 Blackwall & Cubitt Town         E09000030 Tower Hamlets 538100\n3  E05009319                Bow East         E09000030 Tower Hamlets 536928\n4  E05009320                Bow West         E09000030 Tower Hamlets 536261\n5  E05009321           Bromley North         E09000030 Tower Hamlets 537617\n6  E05009322           Bromley South         E09000030 Tower Hamlets 537535\n7  E05009323            Canary Wharf         E09000030 Tower Hamlets 537488\n8  E05009324          Island Gardens         E09000030 Tower Hamlets 537960\n9  E05009325                Lansbury         E09000030 Tower Hamlets 538149\n10 E05009326               Limehouse         E09000030 Tower Hamlets 536542\n    BNG_N     LONG     LAT Shape_Leng                               GlobalID\n1  182735 -0.04653 51.5272   4596.711 {ECA3FE65-ECBD-416D-8ACD-06CFCE763148}\n2  180056 -0.01167 51.5026   8655.302 {FC5D2F6D-B45D-425F-9FC8-AC9302A49D63}\n3  183769 -0.02710 51.5362   6838.744 {F7195489-A08A-4CDB-9AD3-589F9E8D5720}\n4  183181 -0.03694 51.5311   6114.871 {7FA851A2-53A2-4B25-A116-A8ED23B50B96}\n5  182704 -0.01759 51.5265   3892.041 {479BB007-2506-4F94-B4EF-9BED621B387A}\n6  182093 -0.01901 51.5210   3603.544 {BDAB15AA-F556-40FA-A5A4-77B8C1C898F2}\n7  179863 -0.02056 51.5010   5934.088 {F8FE1F74-465C-4409-9CA2-23FB59C0B870}\n8  178641 -0.01424 51.4899   4450.479 {A14A94A1-FF5B-46B6-9640-3AFAB6FAB091}\n9  181610 -0.01035 51.5165   5979.077 {89F93DA3-1DB3-420F-AC37-453F49482043}\n10 180835 -0.03380 51.5099   3275.725 {4CE013FC-4E13-4DFF-9CF8-153E4D4362EF}\n                  ward22nm all_pop white_british white_irish white_gypsy\n1            Bethnal Green   19703          6870         338           7\n2  Blackwall & Cubitt Town   13956          4365         208           2\n3                 Bow East   14781          7055         280          15\n4                 Bow West   12939          6167         260          12\n5            Bromley North    9329          2208         111          22\n6            Bromley South    8748          1979          75          16\n7             Canary Wharf   12831          3563         185           2\n8           Island Gardens   13409          5030         188           2\n9                 Lansbury   14993          4140         125          12\n10               Limehouse    6242          2354         135           1\n   white_other mixed_white_asian mixed_white_african mixed_white_caribbean\n1         2042               281                 124                   174\n2         2454               207                 103                   191\n3         1539               179                 105                   302\n4         1229               166                  73                   208\n5          688                82                  40                   144\n6          609                71                  65                   105\n7         2519               185                  96                   132\n8         2543               184                  68                   128\n9         1188                88                 107                   293\n10        1095                72                  46                    43\n   mixed_other asian_bangladeshi asian_chinese asian_indian asian_pakistani\n1          226              6406           466          364             169\n2          189              1995          1247          962             172\n3          222              2529           227          302              85\n4          185              2746           232          225              69\n5          131              3901           226          165              80\n6           71              3860           214          123              94\n7          190              2017          1232          847             123\n8          146              1854          1000          858             124\n9          115              5865           352          201             132\n10          82              1113           369          273              39\n   asian_other black_african black_caribbean black_other other_arab other\n1          436           705             320         315        202   257\n2          499           588             246         147        169   213\n3          233           572             578         264        122   172\n4          214           380             362         195         80   136\n5          169           536             357         224         97   149\n6          226           487             308         240         98   106\n7          452           435             219         174        212   247\n8          404           311             203          84        117   166\n9          318           891             477         375        126   188\n10         156           203              89          59         54    60\n                            SHAPE asian_bangladeshi_prop    LMI_Ii      LMI_eIi\n1  MULTIPOLYGON (((536275 1824...              0.3251282 28.400091 -0.033787002\n2  MULTIPOLYGON (((538731.4 17...              0.1429493  6.410822 -0.005144343\n3  MULTIPOLYGON (((537638.7 18...              0.1710980  4.547442 -0.007928661\n4  MULTIPOLYGON (((537375.7 18...              0.2122266 12.767600 -0.013075467\n5  MULTIPOLYGON (((538299.6 18...              0.4181584 21.632012 -0.058105443\n6  MULTIPOLYGON (((538284.3 18...              0.4412437 33.215970 -0.065154746\n7  MULTIPOLYGON (((538157 1805...              0.1571974  6.594147 -0.006478715\n8  MULTIPOLYGON (((538731.4 17...              0.1382653  3.683294 -0.004739251\n9  MULTIPOLYGON (((538302.5 18...              0.3911826 30.069710 -0.050379322\n10 MULTIPOLYGON (((536349.2 18...              0.1783082 13.291473 -0.008738364\n    LMI_varIi   LMI_zIi     LMI_sigP  obs_diff  LMI_diff cluster_type\n1   4.5702873 13.300389 2.302674e-40 0.2987531 27.622378    High-High\n2   1.1975739  5.862879 4.549109e-09 0.1165742  5.633110    High-High\n3   0.9163467  4.758760 1.947861e-06 0.1447230  3.769730    High-High\n4   1.5033436 10.423767 1.931491e-25 0.1858516 11.989888    High-High\n5   6.3758219  8.590013 8.695953e-18 0.3917834 20.854300    High-High\n6  10.6742781 10.186588 2.276128e-24 0.4148687 32.438258    High-High\n7   1.1280251  6.214780 5.139675e-10 0.1308224  5.816435    High-High\n8   1.6579452  2.864242 4.180087e-03 0.1118903  2.905581    High-High\n9   8.3840761 10.402283 2.420639e-25 0.3648075 29.291998    High-High\n10  1.0091030 13.240085 5.149045e-40 0.1519332 12.513761    High-High\n   asian_bangladeshi_gi\n1             13.300389\n2              5.862879\n3              4.758760\n4             10.423767\n5              8.590013\n6             10.186588\n7              6.214780\n8              2.864242\n9             10.402283\n10            13.240085\n\n\nBy printing the results of our test, we can see that the local Getis-Ord test is a bit different from a local Moran’s I test as it only contains a single value: the z-score. The z-score is a standardised value relating to whether high values or low values are clustering together, which we call the \\(Gi*\\) statistic. Now we have joined this output, a list of \\(Gi*\\) values, to our ethnicity_london_sdf spatial dataframe, we can map the result:\n\n\n\nR code\n\n# plot the different clusters\ntm_shape(ethnicity_london_sdf) +\n  tm_polygons(\n    col = \"asian_bangladeshi_gi\",\n    style = \"pretty\",\n    midpoint = 0,\n    title = \"Local Gi* statistic\",\n    palette = \"-RdYlBu\"\n  ) +\n  tm_layout(\n    main.title = \"Cluster Map Asian-Bangladeshis in London\",\n    main.title.fontface = 2,\n    fontfamily = \"Helvetica\",\n    legend.outside = TRUE,\n    legend.outside.position = \"right\",\n    legend.title.size = 1,\n    legend.title.fontface = 2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"bottom\")\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 5, 10, 15, 20),\n    position = c(\"left\", \"bottom\")\n  )\n\n\n\n\n\nFigure 19: Local \\(Gi*\\) map of the self-identified Asian-Bangladeshi population group.\n\n\n\n\nOur map shows quite clear hot spots of self-identified Asian-Bangladeshis across London: the higher the z-score, the stronger the clustering.\n\n\n\n\n\n\nIf we want to only map the statistically significant clusters, we need to use these z-scores to filter out only statistically significant clusters. When using a 95 percent confidence level your values should be between -1.96 and +1.96 standard deviations."
  },
  {
    "objectID": "08-autocorrelation.html#assignment-w08",
    "href": "08-autocorrelation.html#assignment-w08",
    "title": "1 Analysing Spatial Patterns III: Spatial Autocorrelation",
    "section": "",
    "text": "Through conducting our spatial autocorrelation tests, we can confirm the presence of clusters in our asian_bangladeshi_prop variable and assign a significance value to these clusters.\nNow you have the code, you will be able to repeat this analysis on any variable in the future. For this week’s assignment, we therefore want you to find out whether also our self-identified white_british ethnic group shows signs of spatial clustering.\nIn order to do this, you have to:\n\nCreate a choropleth map showing the distribution of the variable’s values.\nNormalise the variable by dividing it by the total population.\nCalculate a global spatial autocorrelation statistic and explain what it shows.\nCreate a local Moran’s I map showing the cluster types.\nCreate a local Getis-Ord hotspot map.\nCompare these results to the output of our asian_bangladeshi_prop values. Do both variables have clusters? Do we see similar clusters in similar locations?\nRun the analysis using a different neighbour definition. What happens? Do the results change?"
  },
  {
    "objectID": "08-autocorrelation.html#wm-w08",
    "href": "08-autocorrelation.html#wm-w08",
    "title": "1 Analysing Spatial Patterns III: Spatial Autocorrelation",
    "section": "",
    "text": "If you are interested in moving beyond spatial autocorrelation, for instance how to account for spatial autocorrelation in statistical models (e.g. with Geographically Weighted Regression or Spatial Regression), have a look at the workbook Mapping and Modelling Geographic Data in R by Bristol-based Professor Richard Harris. Start with The Spatial Variable section, move to the Measuring spatial autocorrelation section before looking at the Geographically Weighted Statistics and Spatial Regression sections."
  },
  {
    "objectID": "08-autocorrelation.html#byl-w08",
    "href": "08-autocorrelation.html#byl-w08",
    "title": "1 Analysing Spatial Patterns III: Spatial Autocorrelation",
    "section": "",
    "text": "And that is how you can measure spatial dependence in your dataset through different spatial autocorrelation measures. Next week we will focus on the last topic within our set of core spatial analysis methods and techniques, but this week we have covered enough! Probably time to get back to that pesky reading list."
  },
  {
    "objectID": "00-index.html",
    "href": "00-index.html",
    "title": "GEOG0030 Geocomputation 2023-2024",
    "section": "",
    "text": "Welcome to Geocomputation. This module will introduce you both to the principles of spatial analysis as well as provide you with a comprehensive introduction to the use of programming. Over the next ten weeks, you will learn about the theory, methods and tools of spatial analysis through relevant case studies. We will start by using QGIS before moving to the R programming language. You will learn how to find, manage and clean spatial, demographic and socioeconomic datasets, and then analyse them using core spatial analysis techniques.\n\n\n\nMoodle is the central point of contact for GEOG0030 and it is where all important information will be communicated such as key module and assessment information. This workbook contains links to all reading material as well as the contents for all computer tutorials\n\n\n\nThe topics covered over the next ten weeks are:\n\n\n\nWeek\nSection\nTopic\n\n\n\n\n1\nFoundational Concepts\nGeocomputation: An Introduction\n\n\n2\nFoundational Concepts\nGIScience and GIS software\n\n\n3\nFoundational Concepts\nCartography and Visualisation\n\n\n4\nFoundational Concepts\nProgramming for Data Analysis\n\n\n5\nFoundational Concepts\nProgramming for Spatial Analysis\n\n\n\nReading week\nReading week\n\n\n6\nCore Spatial Analysis\nAnalysing Spatial Patterns I: Geometric Operations and Spatial Queries\n\n\n7\nCore Spatial Analysis\nAnalysing Spatial Patterns II: Spatial Autocorrelation\n\n\n8\nCore Spatial Analysis\nAnalysing Spatial Patterns III: Point Pattern Analysis\n\n\n9\nAdvanced Spatial Analysis\nRasters, Zonal Statistics, and Interpolation\n\n\n10\nAdvanced Spatial Analysis\nTransport Network Analysis\n\n\n\n\n\n\nSpatial analysis can yield fascinating insights into geographical relationships, albeit at times it can be challenging, particularly when we combine this with learning how to program at the same time. You will most likely encounter error messages, experience software crashes, and spend time to identify bugs in your code. However, the rewards of learning how to programmatically solve complex spatial problems will be very much worth it in the end.\nIf you need specific assistance with this module, please:\n\nAsk a question at the end of a lecture or during the computer practicals.\nCheck the Moodle assessment tab for queries relating to this module’s assessment.\nAttend the scheduled Geocomputation Additional Support Hours.\nBook a slot in our Academic Support and Feedback hours.\n\n\n\n\nThis year’s workbook is compiled using:\n\nThe GEOG0030: Geocomputation 2022-2023 workbook by Justin van Dijk\nThe GEOG0030: Geocomputation 2021-2022 workbook by Justin van Dijk\nThe GEOG0030: Geocomputation 2020-2021 workbook by Jo Wilkin\n\nThis year’s workbook also takes inspiration and design elements from:\n\nThe Spatial Data Science for Social Geography course by Martin Fleischmann\n\n\n\n\n\n\n\n\n\n\nThis year’s version features the following major updates:\n\nFull rewrite of the workbook using Quarto.\nUpdate to using 2021 Census data.\nRewriting sections relying on deprecated and outdated core packages: raster, rgdal, rgeos.\nMove to GeoPackage over shapefile.\nInclude WorldPop data for raster analysis.\nInclude an extensive introduction to using lookup tables.\nUpdate to use native pipe |&gt; over dplyr’s implementation (%&gt;%).\nAdd dark mode switch."
  },
  {
    "objectID": "00-index.html#welcome",
    "href": "00-index.html#welcome",
    "title": "GEOG0030 Geocomputation 2023-2024",
    "section": "",
    "text": "Welcome to Geocomputation. This module will introduce you both to the principles of spatial analysis as well as provide you with a comprehensive introduction to the use of programming. Over the next ten weeks, you will learn about the theory, methods and tools of spatial analysis through relevant case studies. We will start by using QGIS before moving to the R programming language. You will learn how to find, manage and clean spatial, demographic and socioeconomic datasets, and then analyse them using core spatial analysis techniques."
  },
  {
    "objectID": "00-index.html#moodle",
    "href": "00-index.html#moodle",
    "title": "GEOG0030 Geocomputation 2023-2024",
    "section": "",
    "text": "Moodle is the central point of contact for GEOG0030 and it is where all important information will be communicated such as key module and assessment information. This workbook contains links to all reading material as well as the contents for all computer tutorials"
  },
  {
    "objectID": "00-index.html#module-overview",
    "href": "00-index.html#module-overview",
    "title": "GEOG0030 Geocomputation 2023-2024",
    "section": "",
    "text": "The topics covered over the next ten weeks are:\n\n\n\nWeek\nSection\nTopic\n\n\n\n\n1\nFoundational Concepts\nGeocomputation: An Introduction\n\n\n2\nFoundational Concepts\nGIScience and GIS software\n\n\n3\nFoundational Concepts\nCartography and Visualisation\n\n\n4\nFoundational Concepts\nProgramming for Data Analysis\n\n\n5\nFoundational Concepts\nProgramming for Spatial Analysis\n\n\n\nReading week\nReading week\n\n\n6\nCore Spatial Analysis\nAnalysing Spatial Patterns I: Geometric Operations and Spatial Queries\n\n\n7\nCore Spatial Analysis\nAnalysing Spatial Patterns II: Spatial Autocorrelation\n\n\n8\nCore Spatial Analysis\nAnalysing Spatial Patterns III: Point Pattern Analysis\n\n\n9\nAdvanced Spatial Analysis\nRasters, Zonal Statistics, and Interpolation\n\n\n10\nAdvanced Spatial Analysis\nTransport Network Analysis"
  },
  {
    "objectID": "00-index.html#troubleshooting",
    "href": "00-index.html#troubleshooting",
    "title": "GEOG0030 Geocomputation 2023-2024",
    "section": "",
    "text": "Spatial analysis can yield fascinating insights into geographical relationships, albeit at times it can be challenging, particularly when we combine this with learning how to program at the same time. You will most likely encounter error messages, experience software crashes, and spend time to identify bugs in your code. However, the rewards of learning how to programmatically solve complex spatial problems will be very much worth it in the end.\nIf you need specific assistance with this module, please:\n\nAsk a question at the end of a lecture or during the computer practicals.\nCheck the Moodle assessment tab for queries relating to this module’s assessment.\nAttend the scheduled Geocomputation Additional Support Hours.\nBook a slot in our Academic Support and Feedback hours."
  },
  {
    "objectID": "00-index.html#acknowledgements",
    "href": "00-index.html#acknowledgements",
    "title": "GEOG0030 Geocomputation 2023-2024",
    "section": "",
    "text": "This year’s workbook is compiled using:\n\nThe GEOG0030: Geocomputation 2022-2023 workbook by Justin van Dijk\nThe GEOG0030: Geocomputation 2021-2022 workbook by Justin van Dijk\nThe GEOG0030: Geocomputation 2020-2021 workbook by Jo Wilkin\n\nThis year’s workbook also takes inspiration and design elements from:\n\nThe Spatial Data Science for Social Geography course by Martin Fleischmann"
  },
  {
    "objectID": "00-index.html#major-updates",
    "href": "00-index.html#major-updates",
    "title": "GEOG0030 Geocomputation 2023-2024",
    "section": "",
    "text": "This year’s version features the following major updates:\n\nFull rewrite of the workbook using Quarto.\nUpdate to using 2021 Census data.\nRewriting sections relying on deprecated and outdated core packages: raster, rgdal, rgeos.\nMove to GeoPackage over shapefile.\nInclude WorldPop data for raster analysis.\nInclude an extensive introduction to using lookup tables.\nUpdate to use native pipe |&gt; over dplyr’s implementation (%&gt;%).\nAdd dark mode switch."
  },
  {
    "objectID": "05-spatial.html",
    "href": "05-spatial.html",
    "title": "1 Programming for Spatial Analysis",
    "section": "",
    "text": "This week we are going to look at how to use R and RStudio as a piece of GIS software. Like last week, we will be completing an analysis on our London theft crime dataset. However, rather than looking at overall theft in London by month, we will add a spatial component to our analysis.\n\n\nThe slides for this week’s lecture can be downloaded here: [Link]\n\n\n\n\n\n\nLongley, P. et al. 2015. Geographic Information Science & systems, Chapter 13: Spatial Analysis, pp. 290-318. [Link]\nLovelace, R., Nowosad, J. and Muenchow, J. 2021. Geocomputation with R, Chapter 2: Geographic Data in R. [Link]\nLovelace, R., Nowosad, J. and Muenchow, J. 2021. Geocomputation with R, Chapter 3: Attribute data operations. [Link]\nLovelace, R., Nowosad, J. and Muenchow, J. 2021. Geocomputation with R, Chapter 8: Making maps with R. [Link]\n\n\n\n\n\nPoorthuis, A. and Zook, M. 2020. Being smarter about space: Drawing lessons from spatial science. Annals of the American Association of Geographers 110(2): 349-359. [Link]\nDe Smith, M, Goodchild, M. and Longley, P. 2018. Geospatial analsyis. A Comprehensive guide to principles techniques and software tools. Chapter 9: Big Data and geospatial analysis. [Link]\nRadil, S. 2016. Spatial analysis of crime. Chapter 24: The Handbook of Measurement Issues in Criminology and Criminal Justice, pp. 536-554. [Link]\n\n\n\n\n\nTo analyse crime over space, we will go through several steps of data preparation and data linkage to ultimately aggregate our data to the Middle layer Super Output Area (MSOA) level. After this, we will map the crime rate for 2021 using the tmap library.\n\n\n\n\n\n\nOAs, LSOAs and MSOAs make up the different levels of the census statistical geographies. Middle layer Super Output Areas (MSOAs) are made up of groups of LSOAs, usually four or five. They comprise between 2,000 and 6,000 households and have a usually resident population between 5,000 and 15,000 persons. For details see the explanation on the webpage of the Office for National Statistics\n\n\n\n\n\nOpen a new script within your GEOG0030 project and save this script as wk5-crime-spatial-processing.r. At the top of your script, add the following metadata:\n\n\n\nR code\n\n# Analysing theft in London by MSOA\n# Date: January 2024\n\n\nNow let us add all of the libraries we will be using today to the top of our script:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\n\n\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   1.0.2 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\n\n\n\n\nYou have already been introduced to the tidyverse library last week, but now we are adding sf to read and load our spatial data as well as tmap to visualise our spatial data. We are going to first load the crime-theft-2021-london.csv we saved last week.\n\n\n\nR code\n\n# read in our csv file\nall_theft_df &lt;- read_csv(\"data/raw/crime/crime-theft-2021-london.csv\")\n\n\nRows: 38229 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (9): crime_id, month, reported_by, falls_within, location, lsoa_code, ls...\ndbl (2): longitude, latitude\nlgl (1): context\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nYou can inspect the dataframe by using the View() function. If you do this carefully, you would notice that some of the crimes do not have a location associated with them. The first thing we therefore need to do is filtering these rows out:\n\n\n\nR code\n\n# filter out crimes that have no location information\nall_theft_df &lt;- filter(all_theft_df, location != \"No Location\")\n\n\nNext, we need a dataset containing the MSOAs for London. Normally, you would navigate to the Open Geography Portal, download a copy of all MSOA polygons, filter out the MSOAs that you need, and add in the 2021 population data. To save us some time today, however, you can download a pre-filtered MSOA file below. Unzip the file and copy the GeoPackage to your data/raw/boundaries folder.\n\n\n\nFile\nType\nLink\n\n\n\n\nMSOAs London 2021\nGeoPackage\nDownload\n\n\n\nNow let’s load the MSOA2021_London.gpkg. We will store this as a variable called msoa_population and use the sf library to load the data:\n\n\n\nR code\n\n# read in our MSOA GeoPackage\nmsoa_population &lt;- st_read(\"data/raw/boundaries/MSOA2021_London.gpkg\")\n\n\nReading layer `MSOA2021_London' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/raw/boundaries/MSOA2021_London.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1002 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n\n\nYou should also see the msoa_population variable appear in your environment window.\n\n\n\nAs this is the first time we have loaded spatial data into R, let’s go for a little exploration of how we can interact with our spatial dataframe. The first thing we want to do when we load spatial data is, of course, map it to confirm if everything is in order. To do this, we can use a really simple command from R’s base library: plot(). As we do not necessarily want to plot this data every time we run this script in the future, we can type this command into the console:\n\n\n\nR code\n\n# plot our MSOA data\nplot(msoa_population)\n\n\n\n\n\nYou should see your msoa_population plot appear in your Plots window. As you will see, your MSOA dataset is plotted ‘thematically’ by each of the fields within the dataset, including the pop2021 field.\n\n\n\n\n\n\nThis plot() function is not to be used to make maps but can be used as a quick way of inspecting your spatial data.\n\n\n\nWe need to find out more information about our msoa_population data. Let’s next check out our class of our data. Again, in the console type:\n\n\n\nR code\n\n# inspect\nclass(msoa_population)\n\n\n[1] \"sf\"         \"data.frame\"\n\n\nWe should see our data is an sf dataframe, which is great because it means we can utilise our tidyverse libraries with our msoa_population. We can also use the attributes() function we looked at last week to find out a little more about the spatial part of our dataframe:\n\n\n\nR code\n\n# inspect\nattributes(msoa_population)\n\n\n$names\n[1] \"msoa21cd\" \"msoa21nm\" \"name\"     \"pop2021\"  \"geom\"    \n\n$row.names\n  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n [91]  91  92  93  94  95  96  97  98  99 100\n [ reached getOption(\"max.print\") -- omitted 902 entries ]\n\n$class\n[1] \"sf\"         \"data.frame\"\n\n$sf_column\n[1] \"geom\"\n\n$agr\nmsoa21cd msoa21nm     name  pop2021 \n    &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt; \nLevels: constant aggregate identity\n\n\nWe can see how many rows we have, the names of our rows and a few more pieces of information about our msoa_population data, for example, we can see that the specific $sf_column i.e. our spatial information) in our dataset is called geom.\nWe can investigate this column a little more by selecting this column within our console to return. In the console type:\n\n\n\nR code\n\n# inspect geometry column\nmsoa_population$geom\n\n\nGeometry set for 1002 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\nFirst 5 geometries:\n\n\nMULTIPOLYGON (((534858 165834.9, 534889.1 16550...\n\n\nMULTIPOLYGON (((544600.9 182911.6, 544750.3 182...\n\n\nMULTIPOLYGON (((531567.9 176323.4, 531511.3 176...\n\n\nMULTIPOLYGON (((523236 179252.4, 523253.4 17914...\n\n\nMULTIPOLYGON (((532509.5 184173.5, 532568.3 184...\n\n\nYou should see new information about our geom column display in your console. From this selection we can find out the dataset’s:\n\ngeometry type\ndimension\nbbox (bounding box)\nCRS (coordinate reference system)\npartial; definition of the first five geometries of the dataset\n\nThis is really useful as one of the first things we want to know about our spatial data is what coordinate system it is projected with. Just like our lsoa_population dataset that we used in previous weeks, the msoa_population data was created and exported within the British National Grid.\nWe can also find out this information, but with more details, with the st_crs() function from the sf library.\n\n\n\nR code\n\n# inspect CRS\nst_crs(msoa_population)\n\n\nCoordinate Reference System:\n  User input: OSGB36 / British National Grid \n  wkt:\nPROJCRS[\"OSGB36 / British National Grid\",\n    BASEGEOGCRS[\"OSGB36\",\n        DATUM[\"Ordnance Survey of Great Britain 1936\",\n            ELLIPSOID[\"Airy 1830\",6377563.396,299.3249646,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4277]],\n    CONVERSION[\"British National Grid\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",49,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",-2,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996012717,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",400000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",-100000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"United Kingdom (UK) - offshore to boundary of UKCS within 49°45'N to 61°N and 9°W to 2°E; onshore Great Britain (England, Wales and Scotland). Isle of Man onshore.\"],\n        BBOX[49.75,-9,61.01,2.01]],\n    ID[\"EPSG\",27700]]\n\n\nYou notice that we actually get a lot more information about our CRS beyond simply the code using this function. This function is really important to us as users of spatial data as it allows us to retrieve and set the CRS of our spatial data when the projection is not specified in the data but we do know what projection system should be used.\nThe final thing we might want to do before we get started with our data analysis is to simply look at the data table part of our dataset, i.e. what we called the Attribute Table in QGIS, but here it is simply the table part of our dataframe. To do so, you can either use the View() function in the console or click on the msoa_population variable within our environment.\n\n\n\nNow we have our data loaded, our next step is to process our data to create what we need as our final output for analysis: a spatial dataframe that contains a theft crime rate for each MSOA. We only two types of spatial or spatially-relevant data in our all_theft_df that can help us with this:\n\nThe approximate WGS84 latitude and longitude.\nThe Lower Super Output Area (LSOA) in which the crime it occurred.\n\nFrom Week 3’s practical, we know we can map our points using the coordinates and then provide a count by using a point-in-polygon operation, but because the crime data already have an LSOA code we will be using an Attribute Join today to show you the use of lookup tables.\n\n\n\n\n\n\nIn situations like this when you actually have the point location data, the best solution is probably to conduct a point-in-polygon analysis yourself rather than relying on a lookup table. However, because we do not always have access to point location data and you are likely to encounter situations where you need a lookup table, there won’t be any point-in-polygon action today.\n\n\n\n\n\nTo get the number of crimes that occurred in each 2021 MSOA linked to our population data, we need to link them together. However, we have two issues. First, our data is available at the LSOA level. Second, and to complicate things further, the all_theft_df dataset is based on 2011 LSOA geographies. This means that we need to take two steps:\n\nUpdate our 2011 LSOA codes to their 2021 counterparts.\nAggregate the resulting 2021 LSOA counts to their parent MSOA.\n\nFrom a GIScience perspective, there are many ways to do this but today we will be using look-up tables. Look-up tables are an extremely common tool in database management and programming, providing a robust approach to storing additional information about a feature (such as a row within a dataframe) in a separate table that can quite literally be ‘looked up’ when needed for a specific application.\n\n\n\n\n\n\nThe data.police.uk website suggests that only from June 2023, the 2021 LSOA codes are used by default.\n\n\n\nTo be able to do this, we first need to find a look-up table that contains a list of 2011 LSOAs and their corresponding 2021 LSOAs. Lucky for us the Office for National Statistics provides this for us on their Open Geography Portal. They have a table that contains exactly what we’re looking for: LSOA (2011) to LSOA (2021) to Local Authority District (2022) Lookup for England and Wales (Version 2). As the description on the website tells us:\n\n“This is an exact fit lookup file between Lower layer Super Output Areas as at December 2011 and Lower layer Super Output Areas as at December 2021 and Local Authority Districts as at December 2022 in England and Wales. This product has been provided with a ‘change indicator’ field, that define the lookup between 2011 and 2021 LSOA. This field indicates which super output areas have changed between 2011 and 2021.”\n\nDownload the ONS lookup table. Subsequently, unzip and move this file to your data -&gt; raw -&gt; boundaries folder.\n\n\n\nFile\nType\nLink\n\n\n\n\nONS LSOA 2011 - LSOA 2021 lookup\ncsv\nDownload\n\n\n\nLoad the dataset using the read_csv() function.\n\n\n\nR code\n\n# read the lookup table\nlsoa_lookup &lt;- read_csv(\"data/raw/boundaries/lsoa11_lsoa21.csv\")\n\n\nRows: 35796 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): LSOA11CD, LSOA11NM, LSOA21CD, LSOA21NM, CHGIND, LAD22CD, LAD22NM, L...\ndbl (1): ObjectId\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNow we have our lookup table, we can assign a relevant 2021 LSOA code to each of the 2011 LSOA codes in our all_theft_df dataframe. To do so, we are going to use one of the join() functions from the dplyr library.\n\n\n\n\n\n\nWe have already learnt how to complete Attribute Joins in QGIS via the Joins tab in the Properties window so it should come of no surprise that we can do exactly the same process within R. To conduct a join between two dataframes, we use the same principles of selecting a unique but matching field within our dataframes to join them together.\nWithin R, you have two main options to complete a dataframe join:\n\nThe first is to use the base R library and its merge() function:\n\nBy default the dataframes are merged on the columns with names they both have, but you can also provide the columns to match separate by using the parameters: by.x and by.y.\nYour code would look something like: merge(x, y, by.x = \"xColName\", by.y = \"yColName\"), with x and y each representing a dataframe.\nThe rows in the two dataframes that match on the specified columns are extracted, and joined together.\nIf there is more than one match, all possible matches contribute one row each, but you can also tell merge whether you want all rows, including ones without a match, or just rows that match, with the arguments all.x and all.\n\nThe second option is to use the dplyr library:\n\ndplyr uses SQL database syntax for its join functions.\nThere are four types of joins possible with the dplyr library.\n\ninner_join(): includes all rows that exist both within x and y.\nleft_join(): includes all rows in x.\nright_join(): includes all rows in y.\nfull_join(): includes all rows in x and y.\n\nFiguring out which one you need will be on a case by case basis.\nAgain, if the join columns have the same name, all you need is left_join(x, y).\nIf they do not have the same name, you need a by argument, such as left_join(x, y, by = c(\"xName\" = \"yName\")). Left of the equals is the column for the first dataframe, right of the equals is the name of the column for the second dataframe.\n\n\n\n\n\nAs we have seen from the list of fields above, we know that we have at least two fields that should match across the datasets: our LSOA codes and LSOA names. We of course need to identify the precise fields that contain these values in each of our dataframes, i.e. LSOA11CD and LSOA11NM in our lsoa_lookup dataframe and lsoa_code and lsoa_name in our all_theft_df dataframe, but once we know what fields we can use, we can go ahead and join our two dataframes together.\n\n\n\n\n\n\nIf you have a one-to-one lookup table, e.g. one LSOA11 geography corresponds to exactly one LSOA21 entry, this process is very easy. However, between the 2011 and 2021 Census different changes have happened: some LSOA11 have been split into multiple LSOA21 polygons and in other cases LSOA11 have been merged together into a single LSOA21 polygon. This means we need to do some extra work to make sure we not accidentally adjust the number of crimes that are captured in the data.\n\n\n\nUnfortunately, the LSOA lookup that we have does not link 2011 LSOAs to 2021 LSOAs on a one-to-one basis. In fact, different types of relationships exists that are flagged in the CHGIND column:\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nU\nNo Change from 2011 to 2021. This means that direct comparisons can be made between these 2011 and 2021 LSOA.\n\n\nS\nSplit. This means that the 2011 LSOA has been split into two or more 2021 LSOA. There will be one record for each of the 2021 LSOA that the 2011 LSOA has been split into. This means direct comparisons can be made between estimates for the single 2011 LSOA and the estimates from the aggregated 2021 LSOA.\n\n\nM\nMerged. 2011 LSOA have been merged with another one or more 2011 LSOA to form a single 2021 LSOA. This means direct comparisons can be made between the aggregated 2011 LSOAs’ estimates and the single 2021 LSOA’s estimates.\n\n\nX\nThe relationship between 2011 and 2021 LSOA is irregular and fragmented. This has occurred where 2011 LSOA have been redesigned because of local authority district boundary changes, or to improve their social homogeneity. These can’t be easily mapped to equivalent 2021 LSOA like the regular splits (S) and merges (M), and therefore like for like comparisons of estimates for 2011 LSOA and 2021 LSOA are not possible.\n\n\n\nAlthough there are different ways of going about this, today we will:\n\nDivide the total crimes for 2011 LSOAs that have been split equally across 2021 LSOAs.\nCombine total crimes for LSOAs that have been merged.\nIgnore the suggested 2021 LSOAs for which there has been an irregular or fragmented relationship.\n\n\n\n\nR code\n\n# for unchanged LSOAs keep weightings of individual crimes the same\nlsoa_lookup_same &lt;- lsoa_lookup |&gt;\n    filter(CHGIND == \"U\") |&gt;\n    group_by(LSOA11CD) |&gt;\n    mutate(n = n())\n\n# for merged LSOAs: keep weightings of individual crimes the same\nlsoa_lookup_merge &lt;- lsoa_lookup |&gt;\n    filter(CHGIND == \"M\") |&gt;\n    group_by(LSOA11CD) |&gt;\n    mutate(n = n())\n\n# for split LSOAs: weigh individual crimes proportionally to the number of 2021\n# LSOAs\nlsoa_lookup_split &lt;- lsoa_lookup |&gt;\n    filter(CHGIND == \"S\") |&gt;\n    group_by(LSOA11CD) |&gt;\n    mutate(n = 1/n())\n\n# re-combine the lookup with updated weightings\nlsoa_lookup &lt;- rbind(lsoa_lookup_same, lsoa_lookup_merge, lsoa_lookup_split)\n\n# inspect\nlsoa_lookup\n\n\n# A tibble: 35,786 × 10\n# Groups:   LSOA11CD [34,747]\n   LSOA11CD LSOA1…¹ LSOA2…² LSOA2…³ CHGIND LAD22CD LAD22NM LAD22…⁴ Objec…⁵     n\n   &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 E010000… City o… E01000… City o… U      E09000… City o… &lt;NA&gt;          1     1\n 2 E010000… City o… E01000… City o… U      E09000… City o… &lt;NA&gt;          2     1\n 3 E010000… City o… E01000… City o… U      E09000… City o… &lt;NA&gt;          3     1\n 4 E010000… City o… E01000… City o… U      E09000… City o… &lt;NA&gt;          4     1\n 5 E010000… Barkin… E01000… Barkin… U      E09000… Barkin… &lt;NA&gt;          5     1\n 6 E010000… Barkin… E01000… Barkin… U      E09000… Barkin… &lt;NA&gt;          6     1\n 7 E010000… Barkin… E01000… Barkin… U      E09000… Barkin… &lt;NA&gt;          7     1\n 8 E010000… Barkin… E01000… Barkin… U      E09000… Barkin… &lt;NA&gt;          8     1\n 9 E010000… Barkin… E01000… Barkin… U      E09000… Barkin… &lt;NA&gt;          9     1\n10 E010000… Barkin… E01000… Barkin… U      E09000… Barkin… &lt;NA&gt;         10     1\n# … with 35,776 more rows, and abbreviated variable names ¹​LSOA11NM, ²​LSOA21CD,\n#   ³​LSOA21NM, ⁴​LAD22NMW, ⁵​ObjectId\n\n\n\n\n\n\n\n\nThe code above uses something called a pipe function: |&gt;. A pipe is used to push the outcome of one function/process into another automatically. It might sound a little confusing at first, but once you start using it, it really can make your code quicker and easier to read and write. Most importantly: it stops us from having to create lots of additional variables to store outputs along the way.\n\n\n\nNow we have adjusted our weightings we can perform our first join. We need to decide which dataset is going to be our target dataset (i.e. the dataset we attach new data too). It makes sense to use the all_theft_df because we need to keep all records in this dataset, but we do not necessarily need all records in the lsoa_lookup dataset for LSOAs for which no crime has been recorded.\n\n\n\nR code\n\n# join to crime data\nall_theft_df_join &lt;- all_theft_df |&gt;\n  left_join(lsoa_lookup, by = c(\"lsoa_code\" = \"LSOA11CD\")) |&gt;\n  filter(!is.na(LSOA21CD))\n\n\n\n\n\n\n\n\nBesides joining our LSOA lookup table to our data, we also filter effectively out all crime records that still not have not been assigned a 2021 LSOA code.\n\n\n\nYou should be able to determine that all_theft_df_join contains 20 variables: 12 from all_theft_df, plus 8 from lsoa_lookup. This seems to suggest the join was successful. However, if we were to count the number of rows in our original all_theft_df dataframe and compare it to our all_theft_df_join dataframe, we would notice something strange: our number of crimes have increased somehow.\n\n\n\nR code\n\n# number of crimes original dataset\nnrow(all_theft_df)\n\n\n[1] 37015\n\n# number of crimes joint dataset\nnrow(all_theft_df_join)\n\n[1] 46666\n\n\nThe change in number of crimes is caused by our one-to-many relationships: one 2011 LSOA can relate to multiple 2021 LSOAs and therewith this one row of data gets duplicated. Fortunately, we saw this coming and we already adjusted the weightings:\n\n\n\nR code\n\n# number of crimes original dataset\nnrow(all_theft_df)\n\n\n[1] 37015\n\n# number of actual crimes joint dataset\nsum(all_theft_df_join$n)\n\n[1] 37010\n\n\n\n\n\n\n\n\nWhereas the number of crimes is very similar, there is still a difference of 5 crimes. This difference is caused by our last filter(), where 2011 LSOAs that have not been assigned a 2021 LSOA code are filtered out. In these cases there is actually a recording error and no valid 2011 LSOA code is used. We therefore choose to ignore these 5 data points.\n\n\n\nWe are getting there, we just need to aggregate our LSOAs to their parent MSOA. Download the ONS lookup table. Subsequently, unzip and move this file to your data -&gt; raw -&gt; boundaries folder.\n\n\n\nFile\nType\nLink\n\n\n\n\nONS LSOA 2021 - MSOA 2021 lookup\ncsv\nDownload\n\n\n\n\n\n\nR code\n\n# read the lookup table\nmsoa_lookup &lt;- read_csv(\"data/raw/boundaries/lsoa21_msoa21.csv\")\n\n\nRows: 35675 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): lsoa21cd, msoa21cd\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWe can now simply attach the msoa_lookup table to our all_theft_df:\n\n\n\nR code\n\n# join to crime data\nall_theft_df_join &lt;- all_theft_df_join |&gt;\n  left_join(msoa_lookup, by = c(\"LSOA21CD\" = \"lsoa21cd\"))\n\n\nNow we have our joined dataset, we can finally move forward with aggregating our data to the MSOA level. Before we do this, it would be good if we could clean up our dataframe to only the relevant data that we need moving forward. To be able to ‘clean’ our dataframe, we have two choices in terms of the code we might want to run. First, we could look to drop certain columns from our dataframe. Alternatively, we could create a subset of the columns we want to keep from our dataframe and store this as a new variable or simply overwrite the currently stored variable. To do either of these types of data transformation, we need to know more about how we can interact with a dataframe in terms of indexing, selecting and slicing.\n\n\n\n\nEverything we will be doing today as we progress with our dataframe cleaning and processing relies on us understanding how to interact with and transform our dataframe. This interaction itself relies on knowing about how indexing works in R as well as how to select and slice your dataframe to extract the relevant cells, rows or columns and then manipulate them. Whilst there are traditional programming approaches to this using the base R library, dplyr is making this type of data wrangling much easier. The following video provides an excellent explanation from both a base R perspective as well as using the dplyr library.\n\n\nThe most basic approach to selecting and slicing within programming relies on the principle of using indexes within our data structures. Indexes actually apply to any type of data structure, from single atomic vectors to complicated dataframes as we use here. Indexing is the numbering associated with each element of a data structure. For example, if we create a simple vector that stores several strings:\n\n\n\nR code\n\n# store a simple vector of strings\nsimple_vector &lt;- c(\"Aa\", \"Bb\", \"Cc\", \"Dd\", \"Ee\", \"Ff\", \"Gg\")\n\n\nR will assign each element (i.e. string) within this simple vector with a number: Aa = 1, Bb = 2, Cc = 3 and so on. Now we can go ahead and select each element by using the base selection syntax which is using square brackets after your element’s variable name, as so:\n\n\n\nR code\n\n# select the first element of our variable\nsimple_vector[1]\n\n\n[1] \"Aa\"\n\n\nWhich should return the first element, our first string containing Aa. You could change the number in the square brackets to any number up to 7 and you would return each specific element in our vector. However, say you do not want the first element of our vector but the second to fifth elements. To achieve this, we conduct what is known in programming as a slicing operation, where, using the [] syntax, we add a colon : to tell R where to start and where to end in creating a selection, known as a slice:\n\n\n\nR code\n\n# select the second to fifth element of our vector\nsimple_vector[2:5]\n\n\n[1] \"Bb\" \"Cc\" \"Dd\" \"Ee\"\n\n\nYou should now see our 2nd to 5th elements returned. Now what is super cool about selection and slicing is that we can add in a simple - (minus) sign to essentially reverse our selection. So for example, we want to return everything but the 3rd element:\n\n\n\nR code\n\n# select everything but the third element of our vector\nsimple_vector[-3]\n\n\n[1] \"Aa\" \"Bb\" \"Dd\" \"Ee\" \"Ff\" \"Gg\"\n\n\nAnd with a slice, we can use the minus to slice out parts of our vector, for example, remove the 2nd to the 5th elements (note the use of a minus sign for both):\n\n\n\nR code\n\n# select outside the second to the fifth element of our vector\nsimple_vector[-2:-5]\n\n\n[1] \"Aa\" \"Ff\" \"Gg\"\n\n\n\n\n\n\n\n\nThis use of square brackets for selection syntax is common across many programming languages, including Python, but there are often some differences you will need to be aware of if you pursue other languages. For example:\n\nPython always starts its index from 0, whereas R’s index starts at 1.\nR is unable to index the characters within strings. This is something you can do in Python, but in R, we will need to use a function such as substring().\n\n\n\n\nWe can also apply these selection techniques to dataframes, but we will have a little more functionality as our dataframes are made from both rows and columns. This means when it comes to selections, we can utilise an amended selection syntax that follows a specific format to select individual rows, columns, slices of each, or just a single cell: [rows, columns]\nThere are many ways we can use this syntax, which we will show below using our lsoa_lookup dataframe. To select a single column from your dataframe, you can use one of two approaches. First we can follow the syntax above carefully and simply set our column parameter in our syntax above to the number 2:\n\n\n\nR code\n\n# select the second column from the dataframe\nlsoa_lookup[, 2]\n\n\n# A tibble: 35,786 × 1\n   LSOA11NM                 \n   &lt;chr&gt;                    \n 1 City of London 001A      \n 2 City of London 001B      \n 3 City of London 001C      \n 4 City of London 001E      \n 5 Barking and Dagenham 016A\n 6 Barking and Dagenham 015A\n 7 Barking and Dagenham 015B\n 8 Barking and Dagenham 016B\n 9 Barking and Dagenham 016C\n10 Barking and Dagenham 015D\n# … with 35,776 more rows\n\n\nYou should see your second column display in your console. Second, we can actually select our column by only typing in the number (no need for the comma). By default, when there is only one argument present in the selection brackets, R will select the column from the dataframe, not the row:\n\n\n\nR code\n\n# select the second column from the dataframe\nlsoa_lookup[2]\n\n\n# A tibble: 35,786 × 1\n   LSOA11NM                 \n   &lt;chr&gt;                    \n 1 City of London 001A      \n 2 City of London 001B      \n 3 City of London 001C      \n 4 City of London 001E      \n 5 Barking and Dagenham 016A\n 6 Barking and Dagenham 015A\n 7 Barking and Dagenham 015B\n 8 Barking and Dagenham 016B\n 9 Barking and Dagenham 016C\n10 Barking and Dagenham 015D\n# … with 35,776 more rows\n\n\nTo select a specific row, we need to add in a comma after our number:\n\n\n\nR code\n\n# select the second row from the dataframe\nlsoa_lookup[2, ]\n\n\n# A tibble: 1 × 10\n# Groups:   LSOA11CD [1]\n  LSOA11CD  LSOA1…¹ LSOA2…² LSOA2…³ CHGIND LAD22CD LAD22NM LAD22…⁴ Objec…⁵     n\n  &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 E01000002 City o… E01000… City o… U      E09000… City o… &lt;NA&gt;          2     1\n# … with abbreviated variable names ¹​LSOA11NM, ²​LSOA21CD, ³​LSOA21NM, ⁴​LAD22NMW,\n#   ⁵​ObjectId\n\n\nYou should see your second row appear. Now, to select a specific cell in our dataframe, we simply provide both arguments in our selection parameters:\n\n\n\nR code\n\n# select the second column, row from the dataframe\nlsoa_lookup[2, 2]\n\n\n# A tibble: 1 × 1\n  LSOA11NM           \n  &lt;chr&gt;              \n1 City of London 001B\n\n\nWhat is also helpful in R is that we can select our columns by their field names by passing these field names to our selection brackets as a string. For a single column:\n\n\n\nR code\n\n# select the second column from the dataframe\nlsoa_lookup[\"LSOA11NM\"]\n\n\n# A tibble: 35,786 × 1\n   LSOA11NM                 \n   &lt;chr&gt;                    \n 1 City of London 001A      \n 2 City of London 001B      \n 3 City of London 001C      \n 4 City of London 001E      \n 5 Barking and Dagenham 016A\n 6 Barking and Dagenham 015A\n 7 Barking and Dagenham 015B\n 8 Barking and Dagenham 016B\n 9 Barking and Dagenham 016C\n10 Barking and Dagenham 015D\n# … with 35,776 more rows\n\n\nOr for more than one columns, we can supply a combined vector:\n\n\n\nR code\n\n# select the first, second column from the dataframe\nlsoa_lookup[c(\"LSOA11CD\", \"LSOA11NM\")]\n\n\n# A tibble: 35,786 × 2\n# Groups:   LSOA11CD [34,747]\n   LSOA11CD  LSOA11NM                 \n   &lt;chr&gt;     &lt;chr&gt;                    \n 1 E01000001 City of London 001A      \n 2 E01000002 City of London 001B      \n 3 E01000003 City of London 001C      \n 4 E01000005 City of London 001E      \n 5 E01000006 Barking and Dagenham 016A\n 6 E01000007 Barking and Dagenham 015A\n 7 E01000008 Barking and Dagenham 015B\n 8 E01000009 Barking and Dagenham 016B\n 9 E01000011 Barking and Dagenham 016C\n10 E01000012 Barking and Dagenham 015D\n# … with 35,776 more rows\n\n\nTo retrieve the second to the fourth column in our dataframe, we can use:\n\n\n\nR code\n\n# select the second to the fourth column from our dataframe\nlsoa_lookup[, 2:4]\n\n\n# A tibble: 35,786 × 3\n   LSOA11NM                  LSOA21CD  LSOA21NM                 \n   &lt;chr&gt;                     &lt;chr&gt;     &lt;chr&gt;                    \n 1 City of London 001A       E01000001 City of London 001A      \n 2 City of London 001B       E01000002 City of London 001B      \n 3 City of London 001C       E01000003 City of London 001C      \n 4 City of London 001E       E01000005 City of London 001E      \n 5 Barking and Dagenham 016A E01000006 Barking and Dagenham 016A\n 6 Barking and Dagenham 015A E01000007 Barking and Dagenham 015A\n 7 Barking and Dagenham 015B E01000008 Barking and Dagenham 015B\n 8 Barking and Dagenham 016B E01000009 Barking and Dagenham 016B\n 9 Barking and Dagenham 016C E01000011 Barking and Dagenham 016C\n10 Barking and Dagenham 015D E01000012 Barking and Dagenham 015D\n# … with 35,776 more rows\n\n# select the second to the fourth column from our dataframe\nlsoa_lookup[2:4]\n\n# A tibble: 35,786 × 3\n   LSOA11NM                  LSOA21CD  LSOA21NM                 \n   &lt;chr&gt;                     &lt;chr&gt;     &lt;chr&gt;                    \n 1 City of London 001A       E01000001 City of London 001A      \n 2 City of London 001B       E01000002 City of London 001B      \n 3 City of London 001C       E01000003 City of London 001C      \n 4 City of London 001E       E01000005 City of London 001E      \n 5 Barking and Dagenham 016A E01000006 Barking and Dagenham 016A\n 6 Barking and Dagenham 015A E01000007 Barking and Dagenham 015A\n 7 Barking and Dagenham 015B E01000008 Barking and Dagenham 015B\n 8 Barking and Dagenham 016B E01000009 Barking and Dagenham 016B\n 9 Barking and Dagenham 016C E01000011 Barking and Dagenham 016C\n10 Barking and Dagenham 015D E01000012 Barking and Dagenham 015D\n# … with 35,776 more rows\n\n\nWe can also provide a combined list of the columns we want to extract:\n\n\n\nR code\n\n# select the second to the seventh columns from our dataframe\nlsoa_lookup[c(2, 3, 4, 7)]\n\n\n# A tibble: 35,786 × 4\n   LSOA11NM                  LSOA21CD  LSOA21NM                  LAD22NM        \n   &lt;chr&gt;                     &lt;chr&gt;     &lt;chr&gt;                     &lt;chr&gt;          \n 1 City of London 001A       E01000001 City of London 001A       City of London \n 2 City of London 001B       E01000002 City of London 001B       City of London \n 3 City of London 001C       E01000003 City of London 001C       City of London \n 4 City of London 001E       E01000005 City of London 001E       City of London \n 5 Barking and Dagenham 016A E01000006 Barking and Dagenham 016A Barking and Da…\n 6 Barking and Dagenham 015A E01000007 Barking and Dagenham 015A Barking and Da…\n 7 Barking and Dagenham 015B E01000008 Barking and Dagenham 015B Barking and Da…\n 8 Barking and Dagenham 016B E01000009 Barking and Dagenham 016B Barking and Da…\n 9 Barking and Dagenham 016C E01000011 Barking and Dagenham 016C Barking and Da…\n10 Barking and Dagenham 015D E01000012 Barking and Dagenham 015D Barking and Da…\n# … with 35,776 more rows\n\n\nWe can apply this slicing approach to our rows:\n\n\n\nR code\n\n# select the second to the fourth row from our dataframe\nlsoa_lookup[2:4, ]\n\n\n# A tibble: 3 × 10\n# Groups:   LSOA11CD [3]\n  LSOA11CD  LSOA1…¹ LSOA2…² LSOA2…³ CHGIND LAD22CD LAD22NM LAD22…⁴ Objec…⁵     n\n  &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 E01000002 City o… E01000… City o… U      E09000… City o… &lt;NA&gt;          2     1\n2 E01000003 City o… E01000… City o… U      E09000… City o… &lt;NA&gt;          3     1\n3 E01000005 City o… E01000… City o… U      E09000… City o… &lt;NA&gt;          4     1\n# … with abbreviated variable names ¹​LSOA11NM, ²​LSOA21CD, ³​LSOA21NM, ⁴​LAD22NMW,\n#   ⁵​ObjectId\n\n\nAs well as a negative selection:\n\n\n\nR code\n\n# select outside the second to the fourth row from our dataframe\nlsoa_lookup[-2:-4, ]\n\n\n# A tibble: 35,783 × 10\n# Groups:   LSOA11CD [34,744]\n   LSOA11CD LSOA1…¹ LSOA2…² LSOA2…³ CHGIND LAD22CD LAD22NM LAD22…⁴ Objec…⁵     n\n   &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 E010000… City o… E01000… City o… U      E09000… City o… &lt;NA&gt;          1     1\n 2 E010000… Barkin… E01000… Barkin… U      E09000… Barkin… &lt;NA&gt;          5     1\n 3 E010000… Barkin… E01000… Barkin… U      E09000… Barkin… &lt;NA&gt;          6     1\n 4 E010000… Barkin… E01000… Barkin… U      E09000… Barkin… &lt;NA&gt;          7     1\n 5 E010000… Barkin… E01000… Barkin… U      E09000… Barkin… &lt;NA&gt;          8     1\n 6 E010000… Barkin… E01000… Barkin… U      E09000… Barkin… &lt;NA&gt;          9     1\n 7 E010000… Barkin… E01000… Barkin… U      E09000… Barkin… &lt;NA&gt;         10     1\n 8 E010000… Barkin… E01000… Barkin… U      E09000… Barkin… &lt;NA&gt;         11     1\n 9 E010000… Barkin… E01000… Barkin… U      E09000… Barkin… &lt;NA&gt;         12     1\n10 E010000… Barkin… E01000… Barkin… U      E09000… Barkin… &lt;NA&gt;         13     1\n# … with 35,773 more rows, and abbreviated variable names ¹​LSOA11NM, ²​LSOA21CD,\n#   ³​LSOA21NM, ⁴​LAD22NMW, ⁵​ObjectId\n\n\n\n\n\nInstead of using the square brackets [] syntax, we can also use dplyr functions that we can use to select or slice our dataframes accordingly:\n\nFor columns, we use the select() function that enables us to select one or more columns using their column names.\nFor rows, we use the slice() function that enables us to select one or more rows using their position (i.e. similar to the process above).\n\nFor both functions, we can also use the negative - approach we saw in the base R approach to ‘reverse a selection’.\n\n\n\nR code\n\n# select column two\ndplyr::select(lsoa_lookup, 2)\n\n\nAdding missing grouping variables: `LSOA11CD`\n\n\n# A tibble: 35,786 × 2\n# Groups:   LSOA11CD [34,747]\n   LSOA11CD  LSOA11NM                 \n   &lt;chr&gt;     &lt;chr&gt;                    \n 1 E01000001 City of London 001A      \n 2 E01000002 City of London 001B      \n 3 E01000003 City of London 001C      \n 4 E01000005 City of London 001E      \n 5 E01000006 Barking and Dagenham 016A\n 6 E01000007 Barking and Dagenham 015A\n 7 E01000008 Barking and Dagenham 015B\n 8 E01000009 Barking and Dagenham 016B\n 9 E01000011 Barking and Dagenham 016C\n10 E01000012 Barking and Dagenham 015D\n# … with 35,776 more rows\n\n# select everything outside column two\ndplyr::select(lsoa_lookup, -2)\n\n# A tibble: 35,786 × 9\n# Groups:   LSOA11CD [34,747]\n   LSOA11CD  LSOA21CD  LSOA21NM     CHGIND LAD22CD LAD22NM LAD22…¹ Objec…²     n\n   &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;        &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 E01000001 E01000001 City of Lon… U      E09000… City o… &lt;NA&gt;          1     1\n 2 E01000002 E01000002 City of Lon… U      E09000… City o… &lt;NA&gt;          2     1\n 3 E01000003 E01000003 City of Lon… U      E09000… City o… &lt;NA&gt;          3     1\n 4 E01000005 E01000005 City of Lon… U      E09000… City o… &lt;NA&gt;          4     1\n 5 E01000006 E01000006 Barking and… U      E09000… Barkin… &lt;NA&gt;          5     1\n 6 E01000007 E01000007 Barking and… U      E09000… Barkin… &lt;NA&gt;          6     1\n 7 E01000008 E01000008 Barking and… U      E09000… Barkin… &lt;NA&gt;          7     1\n 8 E01000009 E01000009 Barking and… U      E09000… Barkin… &lt;NA&gt;          8     1\n 9 E01000011 E01000011 Barking and… U      E09000… Barkin… &lt;NA&gt;          9     1\n10 E01000012 E01000012 Barking and… U      E09000… Barkin… &lt;NA&gt;         10     1\n# … with 35,776 more rows, and abbreviated variable names ¹​LAD22NMW, ²​ObjectId\n\n# select the LSOA11CD column\ndplyr::select(lsoa_lookup, LSOA11CD)\n\n# A tibble: 35,786 × 1\n# Groups:   LSOA11CD [34,747]\n   LSOA11CD \n   &lt;chr&gt;    \n 1 E01000001\n 2 E01000002\n 3 E01000003\n 4 E01000005\n 5 E01000006\n 6 E01000007\n 7 E01000008\n 8 E01000009\n 9 E01000011\n10 E01000012\n# … with 35,776 more rows\n\n# select everything outside the LSOA11CD\ndplyr::select(lsoa_lookup, -LSOA11CD)\n\nAdding missing grouping variables: `LSOA11CD`\n\n\n# A tibble: 35,786 × 10\n# Groups:   LSOA11CD [34,747]\n   LSOA11CD LSOA1…¹ LSOA2…² LSOA2…³ CHGIND LAD22CD LAD22NM LAD22…⁴ Objec…⁵     n\n   &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 E010000… City o… E01000… City o… U      E09000… City o… &lt;NA&gt;          1     1\n 2 E010000… City o… E01000… City o… U      E09000… City o… &lt;NA&gt;          2     1\n 3 E010000… City o… E01000… City o… U      E09000… City o… &lt;NA&gt;          3     1\n 4 E010000… City o… E01000… City o… U      E09000… City o… &lt;NA&gt;          4     1\n 5 E010000… Barkin… E01000… Barkin… U      E09000… Barkin… &lt;NA&gt;          5     1\n 6 E010000… Barkin… E01000… Barkin… U      E09000… Barkin… &lt;NA&gt;          6     1\n 7 E010000… Barkin… E01000… Barkin… U      E09000… Barkin… &lt;NA&gt;          7     1\n 8 E010000… Barkin… E01000… Barkin… U      E09000… Barkin… &lt;NA&gt;          8     1\n 9 E010000… Barkin… E01000… Barkin… U      E09000… Barkin… &lt;NA&gt;          9     1\n10 E010000… Barkin… E01000… Barkin… U      E09000… Barkin… &lt;NA&gt;         10     1\n# … with 35,776 more rows, and abbreviated variable names ¹​LSOA11NM, ²​LSOA21CD,\n#   ³​LSOA21NM, ⁴​LAD22NMW, ⁵​ObjectId\n\n\nIn addition to these index-based functions, within dplyr, we also have filter() that enables us to easily filter rows within our dataframe based on specific conditions. In addition, dplyr provides lots of functions that we can use directly with these selections to apply certain data wrangling processes to only specific parts of our dataframe, such as mutate() or count().\n\n\n\n\n\n\nWe will be using quite a few of these functions in the remainder of this module, so it is highly recommend to download the dplyr cheat sheet to keep track of the most common functions.\n\n\n\nAs we have seen above, whilst there are two approaches to selection using either base R library or the dplyr library, we will continue to focus on using functions directly from the dplyr library to ensure efficiently and compatibility within our code. Within dplyr, as you also saw, whether we want to keep or drop columns, we always use the same function: select().\nTo use this function, we provide our function with a single or list of the columns we want to keep or if we want to drop them, we use the same approach, but add a - before our selection. Let’s see how we can extract just the relevant columns we will need for our future analysis. Note that we will overwrite our all_theft_df_join variable.\nIn your script, add the following code to extract only the relevant columns we need for our future analysis:\n\n\n\nR code\n\n# reduce our dataframe using the select function\nall_theft_df_join &lt;- dplyr::select(all_theft_df_join, crime_id, LSOA21CD, msoa21cd,\n    n)\n\n\nYou should now see that your all_theft_df_join dataframe should only contain four variables. You can go and view this dataframe or call the head() function on the data in the console if you like to check out this new formatting.\n\n\n\n\nTo aggregate our crime by MSOA, we need to use a combination of dplyr functions. First, we need to group our crime by each 2021 MSOA and then create a new variable that contains the sum of thefts occurring in each MSOA. To do so, we will use the group_by() function and the mutate() function. This is the same group_by() function we already used to adjust the LSOA weightings. The group_by() function creates a ‘grouped’ copy of the table (in memory), then any dplyr function used on this grouped table will manipulate each group separately (i.e. our weighted crime counts) and then combine the results to a single output:\n\n\n\nR code\n\nall_theft_df_join &lt;- all_theft_df_join |&gt;\n    group_by(msoa21cd) |&gt;\n    mutate(msoa_theft = sum(n)) |&gt;\n    ungroup()\n\n\nInspect the dataframe using the View() function. You will notice that many values in the msoa_theft column are the same: this makes sense because they relate to the same MSOA. This also means that we probably should just keep only keep distinct values:\n\n\n\nR code\n\nall_theft_df_msoa &lt;- all_theft_df_join |&gt;\n    distinct(msoa21cd, msoa_theft)\n\n\nWrite out the completed theft table to a new csv file for future reference:\n\n\n\nR code\n\n# save as csv\nwrite_csv(all_theft_df_msoa, \"data/data/MSOA2021_theft.csv\")\n\n\n\n\n\nWe are now getting to the final stages of our data processing, we just need to join our completed theft table, all_theft_df_msoa to our msoa_population spatial dataframe and then compute a theft crime rate. This will then allow us to map our crime rates by MSOA, exactly what we set to achieve within this practical.\n\n\n\nR code\n\n# join theft to the MSOA population dataset\ntheft_msoa_sdf &lt;- msoa_population |&gt;\n    left_join(all_theft_df_msoa, by = \"msoa21cd\")\n\n\nTo double-check our join, we want to do one extra step of quality checks and check that each of our MSOAs has at least one occurrence of crime over the twelve month period. We do this by computing a new column that totals the number of thefts. By identifying any MSOAs that have zero entries (NA), we can double-check with our original all_theft_df_msoa to see if this is the correct data for that MSOA or if there has been an error in our join. What we will need to do is adjust the values present within these MSOAs prior to our visualisation analysis: these should not have NA as their value but rather 0.\n\n\n\nR code\n\n# replace all NAs in our dataframe with 0\ntheft_msoa_sdf[is.na(theft_msoa_sdf)] = 0\n\n\nThe final step we need to take before we can map our theft data is, of course, compute a crime rate. We have our pop2021 column within our theft_msoa_sdf dataframe, so now we can add a crime rate as follows:\n\n\n\nR code\n\n# calculate crime rate\ntheft_msoa_sdf &lt;- theft_msoa_sdf |&gt;\n    mutate(crime_rate = (msoa_theft/as.numeric(pop2021)) * 10000)\n\n\nHave a look at your new theft_msoa_sdf spatial dataframe. Does it look as you would expect? Now we have our final dataframe, we can go ahead and make our maps.\n\n\n\nFor making our maps, we will be using one of two main visualisation libraries that can be used for spatial data: tmap. tmap is a library written around thematic map visualisation. The package offers a flexible, layer-based, and easy to use approach to create thematic maps, such as choropleths and bubble maps. What is really great about tmap is that it comes with one quick plotting method for a map called: qtm().\nWe can use this function to plot the theft crime rate really quickly. Within your script, use the qtm function to create a map of theft crime rate in London in 2021.\n\n\n\n\n\n\nBefore continuing do confirm whether your theft_msoa_sdf is indeed still of class sf. In some instances it is possible that this changed when manipulating the attributes. You can simply check this by running class(theft_msoa_sdf). If your dataframe is not of class sf, you can force it into one by running theft_msoa_sdf &lt;- st_as_sf(theft_msoa_sdf)).\n\n\n\n\n\n\nR code\n\n# quick thematic map\nqtm(theft_msoa_sdf, fill = \"crime_rate\")\n\n\n\n\n\nFigure 1: Quick thematic map.\n\n\n\n\nIn this case, the fill() argument is how we tell tmap to create a choropleth map based on the values in the column we provide it with. If we simply set it to NULL, we would only draw the borders of our polygons. Within our qtm function, we can pass quite a few different parameters that would enable us to change specific aesthetics of our map. If you look up the documentation for the function, you will see a list of these parameters. We can, for example, set the lines of our MSOA polygons to white by adding the borders parameter:\n\n\n\nR code\n\n# quick thematic map\nqtm(theft_msoa_sdf, fill = \"crime_rate\", borders = \"white\")\n\n\n\n\n\nFigure 2: Quick thematic map with white borders.\n\n\n\n\nThe map does not really look great. We can continue to add and change parameters in our qtm() function to create a map we are satisfied with. However, the issue with the qtm() function is that it is quite limited in its functionality and mostly used to quickly inspect your data. Instead, when we want to develop more complex maps using the tmap library, we want to use their main plotting method which uses a function called tm_shape(), which we build on using the layered grammar of graphics approach.\n\n\n\n\n\n\nWhen it comes to setting colours within a map or any graphic, we can either pass through a colour as a word, a HEX code or a pre-defined palette. You can find out more here, which is a great quick reference to just some of the possible colours and palettes you will be able to use in R.\n\n\n\nThe main approach to creating maps in tmap is to use the grammar of graphics to build up a map based on what is called the tm_shape() function. Essentially this function, when populated with a spatial dataframe, takes the spatial information of our data (including the projection and geometry of our data) and creates a spatial object. This object contains some information about our original spatial dataframe that we can override (such as the projection) within this function’s parameters, but ultimately, by using this function, you are instructing R that this is the object from which to “draw my shape”.\nTo actually draw the shape, we next need to add a layer to specify the type of shape we want R to draw from this information - in our case, our polygon data. We need to add a function therefore that tells R to “draw my spatial object as X” and within this “layer”, you can also specific additional information to tell R how to draw your layer. You can then add in additional layers, including other spatial objects (and their related shapes) that you want drawn on your map, plus a specify your layout options through a layout layer.\nLet’s see how we can build up our first map in tmap.\n\n\n\nR code\n\n# shape, polygons\ntm_shape(theft_msoa_sdf) + tm_polygons()\n\n\n\n\n\nFigure 3: Building up a map layer by layer.\n\n\n\n\nAs you should now see, we have now mapped the spatial polygons of our theft_msoa_sdf spatial dataframe. However, this is not the map we want: we want to have our polygons represented by a choropleth map where the colours reflect the theft crime rate, rather than the default grey polygons we see before us. To do so, we use the col= parameter that is within our tm_polygons() shape.\n\n\n\n\n\n\nThe col parameter within tm_polygons() is used to fill our polygons with a specific fill type, of either:\n\na single color value (e.g. red)\nthe name of a data variable that is contained in the spatial data file Either the data variable contains color values, or values (numeric or categorical) that will be depicted by a specific color palette.\n\n\n\n\nLet’s go ahead and pass our crime_rate column within the col() parameter and see what we get:\n\n\n\nR code\n\n# shape, polygons\ntm_shape(theft_msoa_sdf) +\n  # specify column\n  tm_polygons(\n    col = \"crime_rate\"\n  )\n\n\n\n\n\nFigure 4: Building up a map layer by layer.\n\n\n\n\nWe are slowly getting there. But there are two things we can notice straight away that do not look right about our data. The first is that our classification breaks do not really reflect the variation in our dataset. This is because tmap has defaulted to its default break type: pretty breaks, whereas, as we know, using an approach such as natural breaks, aka jenks, may reveal better variation in our data.\nUsing the documentation for tm_polygons(), it looks like the following parameters are relevant to help us create the right classification for our map:\n\nn: state the number of classification breaks you want.\nstyle: state the style of breaks you want, e.g. fixed, sd, equal, quantile.\nbreaks: state the numeric breaks you want to use when using the fixed style approach.\n\nLet’s say we want to change our choropleth map to have five classes, determined via the quantile method. We simply need to add the n and style parameters into our tm_polygons() layer:\n\n\n\nR code\n\n# shape, polygons\ntm_shape(theft_msoa_sdf) +\n  # specify column, classes\n  tm_polygons(\n    col = \"crime_rate\",\n    n = 5,\n    style = \"quantile\"\n  )\n\n\n\n\n\nFigure 5: Building up a map layer by layer.\n\n\n\n\nWe now have a choropleth that reflects better the distribution of our data, but we can make them a little prettier by rounding the values. To do so, we can change the style of the map to fixed and then supply a new argument for breaks that contains the rounded classification breaks:\n\n\n\nR code\n\n# shape, polygons\ntm_shape(theft_msoa_sdf) +\n  # specify column, classes, breaks\n  tm_polygons(\n    col = \"crime_rate\",\n    n = 5,\n    style = \"fixed\",\n    breaks = c(0, 5, 10, 15, 50, 5000)\n  )\n\n\n\n\n\nFigure 6: Building up a map layer by layer.\n\n\n\n\nThat looks a little better from the classification side of things.\n\n\n\nTo style our map takes a further understanding and familiarity with our tmap library, but it is only something you will only really learn by having to make your own maps. As a result, we will not go into explaining exactly every aspect of map styling but instead provide you with some example code that you can use as well as experiment with to try to see how you can adjust aspects of the map to your preferences.\nFundamentally, the key functions to be aware of:\n\ntm_layout(): contains parameters to style titles, fonts, the legend;\ntm_compass(): contains parameters to create and style a North arrow or compass;\ntm_scale_bar(): contains parameters to create and style a scale bar.\n\nTo be able to start styling our map, we need to interrogate each of these functions and their parameters to trial and error options to ultimately create a map we are happy with:\n\n\n\nR code\n\n# shape, polygon\ntm_shape(theft_msoa_sdf) +\n  # specify column, classes, breaks, borders, legend title\n  tm_polygons(\n    col = \"crime_rate\",\n    n = 5,\n    style = \"fixed\",\n    breaks = c(0, 5, 10, 15, 50, 5000),\n    palette = \"Blues\",\n    title = \"Rate per 10,000 people\"\n  ) +\n  # add title\n  tm_layout(\n    main.title = \"Theft in London - 2021\",\n    main.title.fontface = 2,\n    fontfamily = \"Helvetica\",\n    legend.outside = TRUE,\n    legend.position = c(\"left\", \"top\"),\n    legend.title.size = 1,\n    legend.title.fontface = 2\n  ) +\n  # add North arrow\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"bottom\")\n  ) +\n  # add scale bar\n  tm_scale_bar(\n    breaks = c(0, 5, 10, 15, 20),\n    position = c(\"left\", \"bottom\")\n  )\n\n\n\n\n\nFigure 7: Building up a map layer by layer.\n\n\n\n\n\n\n\nOnce we are finished making our map, we can go ahead and export it to our maps folder. To do so, we need to save our map-making code to as a variable and then use the tmap_save() function to save the output of this code to a picture within our maps folder.\n\n\n\nR code\n\n# shape, polygons, specify column, specify classes, specify breaks, map elements\nmsoa_map &lt;-\n  # shape, polygon\n  tm_shape(theft_msoa_sdf) +\n  # specify column, classes, breaks, borders, legend title\n  tm_polygons(\n    col = \"crime_rate\",\n    n = 5,\n    style = \"fixed\",\n    breaks = c(0, 5, 10, 15, 50, 5000),\n    palette = \"Blues\",\n    title = \"Rate per 10,000 people\"\n  ) +\n  # add title\n  tm_layout(\n    main.title = \"Theft - 2021\",\n    main.title.fontface = 2,\n    fontfamily = \"Helvetica\",\n    legend.outside = TRUE,\n    legend.position = c(\"left\", \"top\"),\n    legend.title.size = 1,\n    legend.title.fontface = 2\n  ) +\n  # add North arrow\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"bottom\")\n  ) +\n  # add scale bar\n  tm_scale_bar(\n    breaks = c(0, 5, 10, 15, 20),\n    position = c(\"left\", \"bottom\")\n  )\n\n# save as image\ntmap_save(msoa_map, filename = \"data/maps/msoa_theft_map.png\")\n\n\n\n\n\n\nNow we have prepared our dataset and made our first maps in R, we can play with the different settings.\n\nExperiment by changing the colours of the map, changing the legend title name, changing the type of North arrow, etc.\nWe have used a quantile method to classify our data. Do you think that is appropriate? Any other ways you could think of on representing these MSOA crime rates better?\n\n\n\n\n\n\nGit is a version control system, originally developed to help groups of developers work collaboratively on big software projects. One way to think about it is in terms of ‘Track Changes’ used for documents, only this time it is applied to code - and much more powerful. A great resource to help you get started with Git is Happy Git and GitHub for the useR. Highly recommended.\n\n\n\n\nAnd that is how you use R as a GIS in its most basic form. More RGIS in the coming weeks, but this concludes the tutorial for this week. As it is reading week next week, it is probably a good idea to turn your attention to the articles and chapters on the reading list!"
  },
  {
    "objectID": "05-spatial.html#slides-w05",
    "href": "05-spatial.html#slides-w05",
    "title": "1 Programming for Spatial Analysis",
    "section": "",
    "text": "The slides for this week’s lecture can be downloaded here: [Link]"
  },
  {
    "objectID": "05-spatial.html#reading-w05",
    "href": "05-spatial.html#reading-w05",
    "title": "1 Programming for Spatial Analysis",
    "section": "",
    "text": "Longley, P. et al. 2015. Geographic Information Science & systems, Chapter 13: Spatial Analysis, pp. 290-318. [Link]\nLovelace, R., Nowosad, J. and Muenchow, J. 2021. Geocomputation with R, Chapter 2: Geographic Data in R. [Link]\nLovelace, R., Nowosad, J. and Muenchow, J. 2021. Geocomputation with R, Chapter 3: Attribute data operations. [Link]\nLovelace, R., Nowosad, J. and Muenchow, J. 2021. Geocomputation with R, Chapter 8: Making maps with R. [Link]\n\n\n\n\n\nPoorthuis, A. and Zook, M. 2020. Being smarter about space: Drawing lessons from spatial science. Annals of the American Association of Geographers 110(2): 349-359. [Link]\nDe Smith, M, Goodchild, M. and Longley, P. 2018. Geospatial analsyis. A Comprehensive guide to principles techniques and software tools. Chapter 9: Big Data and geospatial analysis. [Link]\nRadil, S. 2016. Spatial analysis of crime. Chapter 24: The Handbook of Measurement Issues in Criminology and Criminal Justice, pp. 536-554. [Link]"
  },
  {
    "objectID": "05-spatial.html#crime-in-london-iii",
    "href": "05-spatial.html#crime-in-london-iii",
    "title": "1 Programming for Spatial Analysis",
    "section": "",
    "text": "To analyse crime over space, we will go through several steps of data preparation and data linkage to ultimately aggregate our data to the Middle layer Super Output Area (MSOA) level. After this, we will map the crime rate for 2021 using the tmap library.\n\n\n\n\n\n\nOAs, LSOAs and MSOAs make up the different levels of the census statistical geographies. Middle layer Super Output Areas (MSOAs) are made up of groups of LSOAs, usually four or five. They comprise between 2,000 and 6,000 households and have a usually resident population between 5,000 and 15,000 persons. For details see the explanation on the webpage of the Office for National Statistics\n\n\n\n\n\nOpen a new script within your GEOG0030 project and save this script as wk5-crime-spatial-processing.r. At the top of your script, add the following metadata:\n\n\n\nR code\n\n# Analysing theft in London by MSOA\n# Date: January 2024\n\n\nNow let us add all of the libraries we will be using today to the top of our script:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\n\n\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   1.0.2 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\n\n\n\n\nYou have already been introduced to the tidyverse library last week, but now we are adding sf to read and load our spatial data as well as tmap to visualise our spatial data. We are going to first load the crime-theft-2021-london.csv we saved last week.\n\n\n\nR code\n\n# read in our csv file\nall_theft_df &lt;- read_csv(\"data/raw/crime/crime-theft-2021-london.csv\")\n\n\nRows: 38229 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (9): crime_id, month, reported_by, falls_within, location, lsoa_code, ls...\ndbl (2): longitude, latitude\nlgl (1): context\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nYou can inspect the dataframe by using the View() function. If you do this carefully, you would notice that some of the crimes do not have a location associated with them. The first thing we therefore need to do is filtering these rows out:\n\n\n\nR code\n\n# filter out crimes that have no location information\nall_theft_df &lt;- filter(all_theft_df, location != \"No Location\")\n\n\nNext, we need a dataset containing the MSOAs for London. Normally, you would navigate to the Open Geography Portal, download a copy of all MSOA polygons, filter out the MSOAs that you need, and add in the 2021 population data. To save us some time today, however, you can download a pre-filtered MSOA file below. Unzip the file and copy the GeoPackage to your data/raw/boundaries folder.\n\n\n\nFile\nType\nLink\n\n\n\n\nMSOAs London 2021\nGeoPackage\nDownload\n\n\n\nNow let’s load the MSOA2021_London.gpkg. We will store this as a variable called msoa_population and use the sf library to load the data:\n\n\n\nR code\n\n# read in our MSOA GeoPackage\nmsoa_population &lt;- st_read(\"data/raw/boundaries/MSOA2021_London.gpkg\")\n\n\nReading layer `MSOA2021_London' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/raw/boundaries/MSOA2021_London.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1002 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n\n\nYou should also see the msoa_population variable appear in your environment window.\n\n\n\nAs this is the first time we have loaded spatial data into R, let’s go for a little exploration of how we can interact with our spatial dataframe. The first thing we want to do when we load spatial data is, of course, map it to confirm if everything is in order. To do this, we can use a really simple command from R’s base library: plot(). As we do not necessarily want to plot this data every time we run this script in the future, we can type this command into the console:\n\n\n\nR code\n\n# plot our MSOA data\nplot(msoa_population)\n\n\n\n\n\nYou should see your msoa_population plot appear in your Plots window. As you will see, your MSOA dataset is plotted ‘thematically’ by each of the fields within the dataset, including the pop2021 field.\n\n\n\n\n\n\nThis plot() function is not to be used to make maps but can be used as a quick way of inspecting your spatial data.\n\n\n\nWe need to find out more information about our msoa_population data. Let’s next check out our class of our data. Again, in the console type:\n\n\n\nR code\n\n# inspect\nclass(msoa_population)\n\n\n[1] \"sf\"         \"data.frame\"\n\n\nWe should see our data is an sf dataframe, which is great because it means we can utilise our tidyverse libraries with our msoa_population. We can also use the attributes() function we looked at last week to find out a little more about the spatial part of our dataframe:\n\n\n\nR code\n\n# inspect\nattributes(msoa_population)\n\n\n$names\n[1] \"msoa21cd\" \"msoa21nm\" \"name\"     \"pop2021\"  \"geom\"    \n\n$row.names\n  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n [91]  91  92  93  94  95  96  97  98  99 100\n [ reached getOption(\"max.print\") -- omitted 902 entries ]\n\n$class\n[1] \"sf\"         \"data.frame\"\n\n$sf_column\n[1] \"geom\"\n\n$agr\nmsoa21cd msoa21nm     name  pop2021 \n    &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt; \nLevels: constant aggregate identity\n\n\nWe can see how many rows we have, the names of our rows and a few more pieces of information about our msoa_population data, for example, we can see that the specific $sf_column i.e. our spatial information) in our dataset is called geom.\nWe can investigate this column a little more by selecting this column within our console to return. In the console type:\n\n\n\nR code\n\n# inspect geometry column\nmsoa_population$geom\n\n\nGeometry set for 1002 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\nFirst 5 geometries:\n\n\nMULTIPOLYGON (((534858 165834.9, 534889.1 16550...\n\n\nMULTIPOLYGON (((544600.9 182911.6, 544750.3 182...\n\n\nMULTIPOLYGON (((531567.9 176323.4, 531511.3 176...\n\n\nMULTIPOLYGON (((523236 179252.4, 523253.4 17914...\n\n\nMULTIPOLYGON (((532509.5 184173.5, 532568.3 184...\n\n\nYou should see new information about our geom column display in your console. From this selection we can find out the dataset’s:\n\ngeometry type\ndimension\nbbox (bounding box)\nCRS (coordinate reference system)\npartial; definition of the first five geometries of the dataset\n\nThis is really useful as one of the first things we want to know about our spatial data is what coordinate system it is projected with. Just like our lsoa_population dataset that we used in previous weeks, the msoa_population data was created and exported within the British National Grid.\nWe can also find out this information, but with more details, with the st_crs() function from the sf library.\n\n\n\nR code\n\n# inspect CRS\nst_crs(msoa_population)\n\n\nCoordinate Reference System:\n  User input: OSGB36 / British National Grid \n  wkt:\nPROJCRS[\"OSGB36 / British National Grid\",\n    BASEGEOGCRS[\"OSGB36\",\n        DATUM[\"Ordnance Survey of Great Britain 1936\",\n            ELLIPSOID[\"Airy 1830\",6377563.396,299.3249646,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4277]],\n    CONVERSION[\"British National Grid\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",49,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",-2,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996012717,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",400000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",-100000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"United Kingdom (UK) - offshore to boundary of UKCS within 49°45'N to 61°N and 9°W to 2°E; onshore Great Britain (England, Wales and Scotland). Isle of Man onshore.\"],\n        BBOX[49.75,-9,61.01,2.01]],\n    ID[\"EPSG\",27700]]\n\n\nYou notice that we actually get a lot more information about our CRS beyond simply the code using this function. This function is really important to us as users of spatial data as it allows us to retrieve and set the CRS of our spatial data when the projection is not specified in the data but we do know what projection system should be used.\nThe final thing we might want to do before we get started with our data analysis is to simply look at the data table part of our dataset, i.e. what we called the Attribute Table in QGIS, but here it is simply the table part of our dataframe. To do so, you can either use the View() function in the console or click on the msoa_population variable within our environment.\n\n\n\nNow we have our data loaded, our next step is to process our data to create what we need as our final output for analysis: a spatial dataframe that contains a theft crime rate for each MSOA. We only two types of spatial or spatially-relevant data in our all_theft_df that can help us with this:\n\nThe approximate WGS84 latitude and longitude.\nThe Lower Super Output Area (LSOA) in which the crime it occurred.\n\nFrom Week 3’s practical, we know we can map our points using the coordinates and then provide a count by using a point-in-polygon operation, but because the crime data already have an LSOA code we will be using an Attribute Join today to show you the use of lookup tables.\n\n\n\n\n\n\nIn situations like this when you actually have the point location data, the best solution is probably to conduct a point-in-polygon analysis yourself rather than relying on a lookup table. However, because we do not always have access to point location data and you are likely to encounter situations where you need a lookup table, there won’t be any point-in-polygon action today.\n\n\n\n\n\nTo get the number of crimes that occurred in each 2021 MSOA linked to our population data, we need to link them together. However, we have two issues. First, our data is available at the LSOA level. Second, and to complicate things further, the all_theft_df dataset is based on 2011 LSOA geographies. This means that we need to take two steps:\n\nUpdate our 2011 LSOA codes to their 2021 counterparts.\nAggregate the resulting 2021 LSOA counts to their parent MSOA.\n\nFrom a GIScience perspective, there are many ways to do this but today we will be using look-up tables. Look-up tables are an extremely common tool in database management and programming, providing a robust approach to storing additional information about a feature (such as a row within a dataframe) in a separate table that can quite literally be ‘looked up’ when needed for a specific application.\n\n\n\n\n\n\nThe data.police.uk website suggests that only from June 2023, the 2021 LSOA codes are used by default.\n\n\n\nTo be able to do this, we first need to find a look-up table that contains a list of 2011 LSOAs and their corresponding 2021 LSOAs. Lucky for us the Office for National Statistics provides this for us on their Open Geography Portal. They have a table that contains exactly what we’re looking for: LSOA (2011) to LSOA (2021) to Local Authority District (2022) Lookup for England and Wales (Version 2). As the description on the website tells us:\n\n“This is an exact fit lookup file between Lower layer Super Output Areas as at December 2011 and Lower layer Super Output Areas as at December 2021 and Local Authority Districts as at December 2022 in England and Wales. This product has been provided with a ‘change indicator’ field, that define the lookup between 2011 and 2021 LSOA. This field indicates which super output areas have changed between 2011 and 2021.”\n\nDownload the ONS lookup table. Subsequently, unzip and move this file to your data -&gt; raw -&gt; boundaries folder.\n\n\n\nFile\nType\nLink\n\n\n\n\nONS LSOA 2011 - LSOA 2021 lookup\ncsv\nDownload\n\n\n\nLoad the dataset using the read_csv() function.\n\n\n\nR code\n\n# read the lookup table\nlsoa_lookup &lt;- read_csv(\"data/raw/boundaries/lsoa11_lsoa21.csv\")\n\n\nRows: 35796 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): LSOA11CD, LSOA11NM, LSOA21CD, LSOA21NM, CHGIND, LAD22CD, LAD22NM, L...\ndbl (1): ObjectId\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNow we have our lookup table, we can assign a relevant 2021 LSOA code to each of the 2011 LSOA codes in our all_theft_df dataframe. To do so, we are going to use one of the join() functions from the dplyr library.\n\n\n\n\n\n\nWe have already learnt how to complete Attribute Joins in QGIS via the Joins tab in the Properties window so it should come of no surprise that we can do exactly the same process within R. To conduct a join between two dataframes, we use the same principles of selecting a unique but matching field within our dataframes to join them together.\nWithin R, you have two main options to complete a dataframe join:\n\nThe first is to use the base R library and its merge() function:\n\nBy default the dataframes are merged on the columns with names they both have, but you can also provide the columns to match separate by using the parameters: by.x and by.y.\nYour code would look something like: merge(x, y, by.x = \"xColName\", by.y = \"yColName\"), with x and y each representing a dataframe.\nThe rows in the two dataframes that match on the specified columns are extracted, and joined together.\nIf there is more than one match, all possible matches contribute one row each, but you can also tell merge whether you want all rows, including ones without a match, or just rows that match, with the arguments all.x and all.\n\nThe second option is to use the dplyr library:\n\ndplyr uses SQL database syntax for its join functions.\nThere are four types of joins possible with the dplyr library.\n\ninner_join(): includes all rows that exist both within x and y.\nleft_join(): includes all rows in x.\nright_join(): includes all rows in y.\nfull_join(): includes all rows in x and y.\n\nFiguring out which one you need will be on a case by case basis.\nAgain, if the join columns have the same name, all you need is left_join(x, y).\nIf they do not have the same name, you need a by argument, such as left_join(x, y, by = c(\"xName\" = \"yName\")). Left of the equals is the column for the first dataframe, right of the equals is the name of the column for the second dataframe.\n\n\n\n\n\nAs we have seen from the list of fields above, we know that we have at least two fields that should match across the datasets: our LSOA codes and LSOA names. We of course need to identify the precise fields that contain these values in each of our dataframes, i.e. LSOA11CD and LSOA11NM in our lsoa_lookup dataframe and lsoa_code and lsoa_name in our all_theft_df dataframe, but once we know what fields we can use, we can go ahead and join our two dataframes together.\n\n\n\n\n\n\nIf you have a one-to-one lookup table, e.g. one LSOA11 geography corresponds to exactly one LSOA21 entry, this process is very easy. However, between the 2011 and 2021 Census different changes have happened: some LSOA11 have been split into multiple LSOA21 polygons and in other cases LSOA11 have been merged together into a single LSOA21 polygon. This means we need to do some extra work to make sure we not accidentally adjust the number of crimes that are captured in the data.\n\n\n\nUnfortunately, the LSOA lookup that we have does not link 2011 LSOAs to 2021 LSOAs on a one-to-one basis. In fact, different types of relationships exists that are flagged in the CHGIND column:\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nU\nNo Change from 2011 to 2021. This means that direct comparisons can be made between these 2011 and 2021 LSOA.\n\n\nS\nSplit. This means that the 2011 LSOA has been split into two or more 2021 LSOA. There will be one record for each of the 2021 LSOA that the 2011 LSOA has been split into. This means direct comparisons can be made between estimates for the single 2011 LSOA and the estimates from the aggregated 2021 LSOA.\n\n\nM\nMerged. 2011 LSOA have been merged with another one or more 2011 LSOA to form a single 2021 LSOA. This means direct comparisons can be made between the aggregated 2011 LSOAs’ estimates and the single 2021 LSOA’s estimates.\n\n\nX\nThe relationship between 2011 and 2021 LSOA is irregular and fragmented. This has occurred where 2011 LSOA have been redesigned because of local authority district boundary changes, or to improve their social homogeneity. These can’t be easily mapped to equivalent 2021 LSOA like the regular splits (S) and merges (M), and therefore like for like comparisons of estimates for 2011 LSOA and 2021 LSOA are not possible.\n\n\n\nAlthough there are different ways of going about this, today we will:\n\nDivide the total crimes for 2011 LSOAs that have been split equally across 2021 LSOAs.\nCombine total crimes for LSOAs that have been merged.\nIgnore the suggested 2021 LSOAs for which there has been an irregular or fragmented relationship.\n\n\n\n\nR code\n\n# for unchanged LSOAs keep weightings of individual crimes the same\nlsoa_lookup_same &lt;- lsoa_lookup |&gt;\n    filter(CHGIND == \"U\") |&gt;\n    group_by(LSOA11CD) |&gt;\n    mutate(n = n())\n\n# for merged LSOAs: keep weightings of individual crimes the same\nlsoa_lookup_merge &lt;- lsoa_lookup |&gt;\n    filter(CHGIND == \"M\") |&gt;\n    group_by(LSOA11CD) |&gt;\n    mutate(n = n())\n\n# for split LSOAs: weigh individual crimes proportionally to the number of 2021\n# LSOAs\nlsoa_lookup_split &lt;- lsoa_lookup |&gt;\n    filter(CHGIND == \"S\") |&gt;\n    group_by(LSOA11CD) |&gt;\n    mutate(n = 1/n())\n\n# re-combine the lookup with updated weightings\nlsoa_lookup &lt;- rbind(lsoa_lookup_same, lsoa_lookup_merge, lsoa_lookup_split)\n\n# inspect\nlsoa_lookup\n\n\n# A tibble: 35,786 × 10\n# Groups:   LSOA11CD [34,747]\n   LSOA11CD LSOA1…¹ LSOA2…² LSOA2…³ CHGIND LAD22CD LAD22NM LAD22…⁴ Objec…⁵     n\n   &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 E010000… City o… E01000… City o… U      E09000… City o… &lt;NA&gt;          1     1\n 2 E010000… City o… E01000… City o… U      E09000… City o… &lt;NA&gt;          2     1\n 3 E010000… City o… E01000… City o… U      E09000… City o… &lt;NA&gt;          3     1\n 4 E010000… City o… E01000… City o… U      E09000… City o… &lt;NA&gt;          4     1\n 5 E010000… Barkin… E01000… Barkin… U      E09000… Barkin… &lt;NA&gt;          5     1\n 6 E010000… Barkin… E01000… Barkin… U      E09000… Barkin… &lt;NA&gt;          6     1\n 7 E010000… Barkin… E01000… Barkin… U      E09000… Barkin… &lt;NA&gt;          7     1\n 8 E010000… Barkin… E01000… Barkin… U      E09000… Barkin… &lt;NA&gt;          8     1\n 9 E010000… Barkin… E01000… Barkin… U      E09000… Barkin… &lt;NA&gt;          9     1\n10 E010000… Barkin… E01000… Barkin… U      E09000… Barkin… &lt;NA&gt;         10     1\n# … with 35,776 more rows, and abbreviated variable names ¹​LSOA11NM, ²​LSOA21CD,\n#   ³​LSOA21NM, ⁴​LAD22NMW, ⁵​ObjectId\n\n\n\n\n\n\n\n\nThe code above uses something called a pipe function: |&gt;. A pipe is used to push the outcome of one function/process into another automatically. It might sound a little confusing at first, but once you start using it, it really can make your code quicker and easier to read and write. Most importantly: it stops us from having to create lots of additional variables to store outputs along the way.\n\n\n\nNow we have adjusted our weightings we can perform our first join. We need to decide which dataset is going to be our target dataset (i.e. the dataset we attach new data too). It makes sense to use the all_theft_df because we need to keep all records in this dataset, but we do not necessarily need all records in the lsoa_lookup dataset for LSOAs for which no crime has been recorded.\n\n\n\nR code\n\n# join to crime data\nall_theft_df_join &lt;- all_theft_df |&gt;\n  left_join(lsoa_lookup, by = c(\"lsoa_code\" = \"LSOA11CD\")) |&gt;\n  filter(!is.na(LSOA21CD))\n\n\n\n\n\n\n\n\nBesides joining our LSOA lookup table to our data, we also filter effectively out all crime records that still not have not been assigned a 2021 LSOA code.\n\n\n\nYou should be able to determine that all_theft_df_join contains 20 variables: 12 from all_theft_df, plus 8 from lsoa_lookup. This seems to suggest the join was successful. However, if we were to count the number of rows in our original all_theft_df dataframe and compare it to our all_theft_df_join dataframe, we would notice something strange: our number of crimes have increased somehow.\n\n\n\nR code\n\n# number of crimes original dataset\nnrow(all_theft_df)\n\n\n[1] 37015\n\n# number of crimes joint dataset\nnrow(all_theft_df_join)\n\n[1] 46666\n\n\nThe change in number of crimes is caused by our one-to-many relationships: one 2011 LSOA can relate to multiple 2021 LSOAs and therewith this one row of data gets duplicated. Fortunately, we saw this coming and we already adjusted the weightings:\n\n\n\nR code\n\n# number of crimes original dataset\nnrow(all_theft_df)\n\n\n[1] 37015\n\n# number of actual crimes joint dataset\nsum(all_theft_df_join$n)\n\n[1] 37010\n\n\n\n\n\n\n\n\nWhereas the number of crimes is very similar, there is still a difference of 5 crimes. This difference is caused by our last filter(), where 2011 LSOAs that have not been assigned a 2021 LSOA code are filtered out. In these cases there is actually a recording error and no valid 2011 LSOA code is used. We therefore choose to ignore these 5 data points.\n\n\n\nWe are getting there, we just need to aggregate our LSOAs to their parent MSOA. Download the ONS lookup table. Subsequently, unzip and move this file to your data -&gt; raw -&gt; boundaries folder.\n\n\n\nFile\nType\nLink\n\n\n\n\nONS LSOA 2021 - MSOA 2021 lookup\ncsv\nDownload\n\n\n\n\n\n\nR code\n\n# read the lookup table\nmsoa_lookup &lt;- read_csv(\"data/raw/boundaries/lsoa21_msoa21.csv\")\n\n\nRows: 35675 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): lsoa21cd, msoa21cd\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWe can now simply attach the msoa_lookup table to our all_theft_df:\n\n\n\nR code\n\n# join to crime data\nall_theft_df_join &lt;- all_theft_df_join |&gt;\n  left_join(msoa_lookup, by = c(\"LSOA21CD\" = \"lsoa21cd\"))\n\n\nNow we have our joined dataset, we can finally move forward with aggregating our data to the MSOA level. Before we do this, it would be good if we could clean up our dataframe to only the relevant data that we need moving forward. To be able to ‘clean’ our dataframe, we have two choices in terms of the code we might want to run. First, we could look to drop certain columns from our dataframe. Alternatively, we could create a subset of the columns we want to keep from our dataframe and store this as a new variable or simply overwrite the currently stored variable. To do either of these types of data transformation, we need to know more about how we can interact with a dataframe in terms of indexing, selecting and slicing.\n\n\n\n\nEverything we will be doing today as we progress with our dataframe cleaning and processing relies on us understanding how to interact with and transform our dataframe. This interaction itself relies on knowing about how indexing works in R as well as how to select and slice your dataframe to extract the relevant cells, rows or columns and then manipulate them. Whilst there are traditional programming approaches to this using the base R library, dplyr is making this type of data wrangling much easier. The following video provides an excellent explanation from both a base R perspective as well as using the dplyr library.\n\n\nThe most basic approach to selecting and slicing within programming relies on the principle of using indexes within our data structures. Indexes actually apply to any type of data structure, from single atomic vectors to complicated dataframes as we use here. Indexing is the numbering associated with each element of a data structure. For example, if we create a simple vector that stores several strings:\n\n\n\nR code\n\n# store a simple vector of strings\nsimple_vector &lt;- c(\"Aa\", \"Bb\", \"Cc\", \"Dd\", \"Ee\", \"Ff\", \"Gg\")\n\n\nR will assign each element (i.e. string) within this simple vector with a number: Aa = 1, Bb = 2, Cc = 3 and so on. Now we can go ahead and select each element by using the base selection syntax which is using square brackets after your element’s variable name, as so:\n\n\n\nR code\n\n# select the first element of our variable\nsimple_vector[1]\n\n\n[1] \"Aa\"\n\n\nWhich should return the first element, our first string containing Aa. You could change the number in the square brackets to any number up to 7 and you would return each specific element in our vector. However, say you do not want the first element of our vector but the second to fifth elements. To achieve this, we conduct what is known in programming as a slicing operation, where, using the [] syntax, we add a colon : to tell R where to start and where to end in creating a selection, known as a slice:\n\n\n\nR code\n\n# select the second to fifth element of our vector\nsimple_vector[2:5]\n\n\n[1] \"Bb\" \"Cc\" \"Dd\" \"Ee\"\n\n\nYou should now see our 2nd to 5th elements returned. Now what is super cool about selection and slicing is that we can add in a simple - (minus) sign to essentially reverse our selection. So for example, we want to return everything but the 3rd element:\n\n\n\nR code\n\n# select everything but the third element of our vector\nsimple_vector[-3]\n\n\n[1] \"Aa\" \"Bb\" \"Dd\" \"Ee\" \"Ff\" \"Gg\"\n\n\nAnd with a slice, we can use the minus to slice out parts of our vector, for example, remove the 2nd to the 5th elements (note the use of a minus sign for both):\n\n\n\nR code\n\n# select outside the second to the fifth element of our vector\nsimple_vector[-2:-5]\n\n\n[1] \"Aa\" \"Ff\" \"Gg\"\n\n\n\n\n\n\n\n\nThis use of square brackets for selection syntax is common across many programming languages, including Python, but there are often some differences you will need to be aware of if you pursue other languages. For example:\n\nPython always starts its index from 0, whereas R’s index starts at 1.\nR is unable to index the characters within strings. This is something you can do in Python, but in R, we will need to use a function such as substring().\n\n\n\n\nWe can also apply these selection techniques to dataframes, but we will have a little more functionality as our dataframes are made from both rows and columns. This means when it comes to selections, we can utilise an amended selection syntax that follows a specific format to select individual rows, columns, slices of each, or just a single cell: [rows, columns]\nThere are many ways we can use this syntax, which we will show below using our lsoa_lookup dataframe. To select a single column from your dataframe, you can use one of two approaches. First we can follow the syntax above carefully and simply set our column parameter in our syntax above to the number 2:\n\n\n\nR code\n\n# select the second column from the dataframe\nlsoa_lookup[, 2]\n\n\n# A tibble: 35,786 × 1\n   LSOA11NM                 \n   &lt;chr&gt;                    \n 1 City of London 001A      \n 2 City of London 001B      \n 3 City of London 001C      \n 4 City of London 001E      \n 5 Barking and Dagenham 016A\n 6 Barking and Dagenham 015A\n 7 Barking and Dagenham 015B\n 8 Barking and Dagenham 016B\n 9 Barking and Dagenham 016C\n10 Barking and Dagenham 015D\n# … with 35,776 more rows\n\n\nYou should see your second column display in your console. Second, we can actually select our column by only typing in the number (no need for the comma). By default, when there is only one argument present in the selection brackets, R will select the column from the dataframe, not the row:\n\n\n\nR code\n\n# select the second column from the dataframe\nlsoa_lookup[2]\n\n\n# A tibble: 35,786 × 1\n   LSOA11NM                 \n   &lt;chr&gt;                    \n 1 City of London 001A      \n 2 City of London 001B      \n 3 City of London 001C      \n 4 City of London 001E      \n 5 Barking and Dagenham 016A\n 6 Barking and Dagenham 015A\n 7 Barking and Dagenham 015B\n 8 Barking and Dagenham 016B\n 9 Barking and Dagenham 016C\n10 Barking and Dagenham 015D\n# … with 35,776 more rows\n\n\nTo select a specific row, we need to add in a comma after our number:\n\n\n\nR code\n\n# select the second row from the dataframe\nlsoa_lookup[2, ]\n\n\n# A tibble: 1 × 10\n# Groups:   LSOA11CD [1]\n  LSOA11CD  LSOA1…¹ LSOA2…² LSOA2…³ CHGIND LAD22CD LAD22NM LAD22…⁴ Objec…⁵     n\n  &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 E01000002 City o… E01000… City o… U      E09000… City o… &lt;NA&gt;          2     1\n# … with abbreviated variable names ¹​LSOA11NM, ²​LSOA21CD, ³​LSOA21NM, ⁴​LAD22NMW,\n#   ⁵​ObjectId\n\n\nYou should see your second row appear. Now, to select a specific cell in our dataframe, we simply provide both arguments in our selection parameters:\n\n\n\nR code\n\n# select the second column, row from the dataframe\nlsoa_lookup[2, 2]\n\n\n# A tibble: 1 × 1\n  LSOA11NM           \n  &lt;chr&gt;              \n1 City of London 001B\n\n\nWhat is also helpful in R is that we can select our columns by their field names by passing these field names to our selection brackets as a string. For a single column:\n\n\n\nR code\n\n# select the second column from the dataframe\nlsoa_lookup[\"LSOA11NM\"]\n\n\n# A tibble: 35,786 × 1\n   LSOA11NM                 \n   &lt;chr&gt;                    \n 1 City of London 001A      \n 2 City of London 001B      \n 3 City of London 001C      \n 4 City of London 001E      \n 5 Barking and Dagenham 016A\n 6 Barking and Dagenham 015A\n 7 Barking and Dagenham 015B\n 8 Barking and Dagenham 016B\n 9 Barking and Dagenham 016C\n10 Barking and Dagenham 015D\n# … with 35,776 more rows\n\n\nOr for more than one columns, we can supply a combined vector:\n\n\n\nR code\n\n# select the first, second column from the dataframe\nlsoa_lookup[c(\"LSOA11CD\", \"LSOA11NM\")]\n\n\n# A tibble: 35,786 × 2\n# Groups:   LSOA11CD [34,747]\n   LSOA11CD  LSOA11NM                 \n   &lt;chr&gt;     &lt;chr&gt;                    \n 1 E01000001 City of London 001A      \n 2 E01000002 City of London 001B      \n 3 E01000003 City of London 001C      \n 4 E01000005 City of London 001E      \n 5 E01000006 Barking and Dagenham 016A\n 6 E01000007 Barking and Dagenham 015A\n 7 E01000008 Barking and Dagenham 015B\n 8 E01000009 Barking and Dagenham 016B\n 9 E01000011 Barking and Dagenham 016C\n10 E01000012 Barking and Dagenham 015D\n# … with 35,776 more rows\n\n\nTo retrieve the second to the fourth column in our dataframe, we can use:\n\n\n\nR code\n\n# select the second to the fourth column from our dataframe\nlsoa_lookup[, 2:4]\n\n\n# A tibble: 35,786 × 3\n   LSOA11NM                  LSOA21CD  LSOA21NM                 \n   &lt;chr&gt;                     &lt;chr&gt;     &lt;chr&gt;                    \n 1 City of London 001A       E01000001 City of London 001A      \n 2 City of London 001B       E01000002 City of London 001B      \n 3 City of London 001C       E01000003 City of London 001C      \n 4 City of London 001E       E01000005 City of London 001E      \n 5 Barking and Dagenham 016A E01000006 Barking and Dagenham 016A\n 6 Barking and Dagenham 015A E01000007 Barking and Dagenham 015A\n 7 Barking and Dagenham 015B E01000008 Barking and Dagenham 015B\n 8 Barking and Dagenham 016B E01000009 Barking and Dagenham 016B\n 9 Barking and Dagenham 016C E01000011 Barking and Dagenham 016C\n10 Barking and Dagenham 015D E01000012 Barking and Dagenham 015D\n# … with 35,776 more rows\n\n# select the second to the fourth column from our dataframe\nlsoa_lookup[2:4]\n\n# A tibble: 35,786 × 3\n   LSOA11NM                  LSOA21CD  LSOA21NM                 \n   &lt;chr&gt;                     &lt;chr&gt;     &lt;chr&gt;                    \n 1 City of London 001A       E01000001 City of London 001A      \n 2 City of London 001B       E01000002 City of London 001B      \n 3 City of London 001C       E01000003 City of London 001C      \n 4 City of London 001E       E01000005 City of London 001E      \n 5 Barking and Dagenham 016A E01000006 Barking and Dagenham 016A\n 6 Barking and Dagenham 015A E01000007 Barking and Dagenham 015A\n 7 Barking and Dagenham 015B E01000008 Barking and Dagenham 015B\n 8 Barking and Dagenham 016B E01000009 Barking and Dagenham 016B\n 9 Barking and Dagenham 016C E01000011 Barking and Dagenham 016C\n10 Barking and Dagenham 015D E01000012 Barking and Dagenham 015D\n# … with 35,776 more rows\n\n\nWe can also provide a combined list of the columns we want to extract:\n\n\n\nR code\n\n# select the second to the seventh columns from our dataframe\nlsoa_lookup[c(2, 3, 4, 7)]\n\n\n# A tibble: 35,786 × 4\n   LSOA11NM                  LSOA21CD  LSOA21NM                  LAD22NM        \n   &lt;chr&gt;                     &lt;chr&gt;     &lt;chr&gt;                     &lt;chr&gt;          \n 1 City of London 001A       E01000001 City of London 001A       City of London \n 2 City of London 001B       E01000002 City of London 001B       City of London \n 3 City of London 001C       E01000003 City of London 001C       City of London \n 4 City of London 001E       E01000005 City of London 001E       City of London \n 5 Barking and Dagenham 016A E01000006 Barking and Dagenham 016A Barking and Da…\n 6 Barking and Dagenham 015A E01000007 Barking and Dagenham 015A Barking and Da…\n 7 Barking and Dagenham 015B E01000008 Barking and Dagenham 015B Barking and Da…\n 8 Barking and Dagenham 016B E01000009 Barking and Dagenham 016B Barking and Da…\n 9 Barking and Dagenham 016C E01000011 Barking and Dagenham 016C Barking and Da…\n10 Barking and Dagenham 015D E01000012 Barking and Dagenham 015D Barking and Da…\n# … with 35,776 more rows\n\n\nWe can apply this slicing approach to our rows:\n\n\n\nR code\n\n# select the second to the fourth row from our dataframe\nlsoa_lookup[2:4, ]\n\n\n# A tibble: 3 × 10\n# Groups:   LSOA11CD [3]\n  LSOA11CD  LSOA1…¹ LSOA2…² LSOA2…³ CHGIND LAD22CD LAD22NM LAD22…⁴ Objec…⁵     n\n  &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 E01000002 City o… E01000… City o… U      E09000… City o… &lt;NA&gt;          2     1\n2 E01000003 City o… E01000… City o… U      E09000… City o… &lt;NA&gt;          3     1\n3 E01000005 City o… E01000… City o… U      E09000… City o… &lt;NA&gt;          4     1\n# … with abbreviated variable names ¹​LSOA11NM, ²​LSOA21CD, ³​LSOA21NM, ⁴​LAD22NMW,\n#   ⁵​ObjectId\n\n\nAs well as a negative selection:\n\n\n\nR code\n\n# select outside the second to the fourth row from our dataframe\nlsoa_lookup[-2:-4, ]\n\n\n# A tibble: 35,783 × 10\n# Groups:   LSOA11CD [34,744]\n   LSOA11CD LSOA1…¹ LSOA2…² LSOA2…³ CHGIND LAD22CD LAD22NM LAD22…⁴ Objec…⁵     n\n   &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 E010000… City o… E01000… City o… U      E09000… City o… &lt;NA&gt;          1     1\n 2 E010000… Barkin… E01000… Barkin… U      E09000… Barkin… &lt;NA&gt;          5     1\n 3 E010000… Barkin… E01000… Barkin… U      E09000… Barkin… &lt;NA&gt;          6     1\n 4 E010000… Barkin… E01000… Barkin… U      E09000… Barkin… &lt;NA&gt;          7     1\n 5 E010000… Barkin… E01000… Barkin… U      E09000… Barkin… &lt;NA&gt;          8     1\n 6 E010000… Barkin… E01000… Barkin… U      E09000… Barkin… &lt;NA&gt;          9     1\n 7 E010000… Barkin… E01000… Barkin… U      E09000… Barkin… &lt;NA&gt;         10     1\n 8 E010000… Barkin… E01000… Barkin… U      E09000… Barkin… &lt;NA&gt;         11     1\n 9 E010000… Barkin… E01000… Barkin… U      E09000… Barkin… &lt;NA&gt;         12     1\n10 E010000… Barkin… E01000… Barkin… U      E09000… Barkin… &lt;NA&gt;         13     1\n# … with 35,773 more rows, and abbreviated variable names ¹​LSOA11NM, ²​LSOA21CD,\n#   ³​LSOA21NM, ⁴​LAD22NMW, ⁵​ObjectId\n\n\n\n\n\nInstead of using the square brackets [] syntax, we can also use dplyr functions that we can use to select or slice our dataframes accordingly:\n\nFor columns, we use the select() function that enables us to select one or more columns using their column names.\nFor rows, we use the slice() function that enables us to select one or more rows using their position (i.e. similar to the process above).\n\nFor both functions, we can also use the negative - approach we saw in the base R approach to ‘reverse a selection’.\n\n\n\nR code\n\n# select column two\ndplyr::select(lsoa_lookup, 2)\n\n\nAdding missing grouping variables: `LSOA11CD`\n\n\n# A tibble: 35,786 × 2\n# Groups:   LSOA11CD [34,747]\n   LSOA11CD  LSOA11NM                 \n   &lt;chr&gt;     &lt;chr&gt;                    \n 1 E01000001 City of London 001A      \n 2 E01000002 City of London 001B      \n 3 E01000003 City of London 001C      \n 4 E01000005 City of London 001E      \n 5 E01000006 Barking and Dagenham 016A\n 6 E01000007 Barking and Dagenham 015A\n 7 E01000008 Barking and Dagenham 015B\n 8 E01000009 Barking and Dagenham 016B\n 9 E01000011 Barking and Dagenham 016C\n10 E01000012 Barking and Dagenham 015D\n# … with 35,776 more rows\n\n# select everything outside column two\ndplyr::select(lsoa_lookup, -2)\n\n# A tibble: 35,786 × 9\n# Groups:   LSOA11CD [34,747]\n   LSOA11CD  LSOA21CD  LSOA21NM     CHGIND LAD22CD LAD22NM LAD22…¹ Objec…²     n\n   &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;        &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 E01000001 E01000001 City of Lon… U      E09000… City o… &lt;NA&gt;          1     1\n 2 E01000002 E01000002 City of Lon… U      E09000… City o… &lt;NA&gt;          2     1\n 3 E01000003 E01000003 City of Lon… U      E09000… City o… &lt;NA&gt;          3     1\n 4 E01000005 E01000005 City of Lon… U      E09000… City o… &lt;NA&gt;          4     1\n 5 E01000006 E01000006 Barking and… U      E09000… Barkin… &lt;NA&gt;          5     1\n 6 E01000007 E01000007 Barking and… U      E09000… Barkin… &lt;NA&gt;          6     1\n 7 E01000008 E01000008 Barking and… U      E09000… Barkin… &lt;NA&gt;          7     1\n 8 E01000009 E01000009 Barking and… U      E09000… Barkin… &lt;NA&gt;          8     1\n 9 E01000011 E01000011 Barking and… U      E09000… Barkin… &lt;NA&gt;          9     1\n10 E01000012 E01000012 Barking and… U      E09000… Barkin… &lt;NA&gt;         10     1\n# … with 35,776 more rows, and abbreviated variable names ¹​LAD22NMW, ²​ObjectId\n\n# select the LSOA11CD column\ndplyr::select(lsoa_lookup, LSOA11CD)\n\n# A tibble: 35,786 × 1\n# Groups:   LSOA11CD [34,747]\n   LSOA11CD \n   &lt;chr&gt;    \n 1 E01000001\n 2 E01000002\n 3 E01000003\n 4 E01000005\n 5 E01000006\n 6 E01000007\n 7 E01000008\n 8 E01000009\n 9 E01000011\n10 E01000012\n# … with 35,776 more rows\n\n# select everything outside the LSOA11CD\ndplyr::select(lsoa_lookup, -LSOA11CD)\n\nAdding missing grouping variables: `LSOA11CD`\n\n\n# A tibble: 35,786 × 10\n# Groups:   LSOA11CD [34,747]\n   LSOA11CD LSOA1…¹ LSOA2…² LSOA2…³ CHGIND LAD22CD LAD22NM LAD22…⁴ Objec…⁵     n\n   &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 E010000… City o… E01000… City o… U      E09000… City o… &lt;NA&gt;          1     1\n 2 E010000… City o… E01000… City o… U      E09000… City o… &lt;NA&gt;          2     1\n 3 E010000… City o… E01000… City o… U      E09000… City o… &lt;NA&gt;          3     1\n 4 E010000… City o… E01000… City o… U      E09000… City o… &lt;NA&gt;          4     1\n 5 E010000… Barkin… E01000… Barkin… U      E09000… Barkin… &lt;NA&gt;          5     1\n 6 E010000… Barkin… E01000… Barkin… U      E09000… Barkin… &lt;NA&gt;          6     1\n 7 E010000… Barkin… E01000… Barkin… U      E09000… Barkin… &lt;NA&gt;          7     1\n 8 E010000… Barkin… E01000… Barkin… U      E09000… Barkin… &lt;NA&gt;          8     1\n 9 E010000… Barkin… E01000… Barkin… U      E09000… Barkin… &lt;NA&gt;          9     1\n10 E010000… Barkin… E01000… Barkin… U      E09000… Barkin… &lt;NA&gt;         10     1\n# … with 35,776 more rows, and abbreviated variable names ¹​LSOA11NM, ²​LSOA21CD,\n#   ³​LSOA21NM, ⁴​LAD22NMW, ⁵​ObjectId\n\n\nIn addition to these index-based functions, within dplyr, we also have filter() that enables us to easily filter rows within our dataframe based on specific conditions. In addition, dplyr provides lots of functions that we can use directly with these selections to apply certain data wrangling processes to only specific parts of our dataframe, such as mutate() or count().\n\n\n\n\n\n\nWe will be using quite a few of these functions in the remainder of this module, so it is highly recommend to download the dplyr cheat sheet to keep track of the most common functions.\n\n\n\nAs we have seen above, whilst there are two approaches to selection using either base R library or the dplyr library, we will continue to focus on using functions directly from the dplyr library to ensure efficiently and compatibility within our code. Within dplyr, as you also saw, whether we want to keep or drop columns, we always use the same function: select().\nTo use this function, we provide our function with a single or list of the columns we want to keep or if we want to drop them, we use the same approach, but add a - before our selection. Let’s see how we can extract just the relevant columns we will need for our future analysis. Note that we will overwrite our all_theft_df_join variable.\nIn your script, add the following code to extract only the relevant columns we need for our future analysis:\n\n\n\nR code\n\n# reduce our dataframe using the select function\nall_theft_df_join &lt;- dplyr::select(all_theft_df_join, crime_id, LSOA21CD, msoa21cd,\n    n)\n\n\nYou should now see that your all_theft_df_join dataframe should only contain four variables. You can go and view this dataframe or call the head() function on the data in the console if you like to check out this new formatting.\n\n\n\n\nTo aggregate our crime by MSOA, we need to use a combination of dplyr functions. First, we need to group our crime by each 2021 MSOA and then create a new variable that contains the sum of thefts occurring in each MSOA. To do so, we will use the group_by() function and the mutate() function. This is the same group_by() function we already used to adjust the LSOA weightings. The group_by() function creates a ‘grouped’ copy of the table (in memory), then any dplyr function used on this grouped table will manipulate each group separately (i.e. our weighted crime counts) and then combine the results to a single output:\n\n\n\nR code\n\nall_theft_df_join &lt;- all_theft_df_join |&gt;\n    group_by(msoa21cd) |&gt;\n    mutate(msoa_theft = sum(n)) |&gt;\n    ungroup()\n\n\nInspect the dataframe using the View() function. You will notice that many values in the msoa_theft column are the same: this makes sense because they relate to the same MSOA. This also means that we probably should just keep only keep distinct values:\n\n\n\nR code\n\nall_theft_df_msoa &lt;- all_theft_df_join |&gt;\n    distinct(msoa21cd, msoa_theft)\n\n\nWrite out the completed theft table to a new csv file for future reference:\n\n\n\nR code\n\n# save as csv\nwrite_csv(all_theft_df_msoa, \"data/data/MSOA2021_theft.csv\")\n\n\n\n\n\nWe are now getting to the final stages of our data processing, we just need to join our completed theft table, all_theft_df_msoa to our msoa_population spatial dataframe and then compute a theft crime rate. This will then allow us to map our crime rates by MSOA, exactly what we set to achieve within this practical.\n\n\n\nR code\n\n# join theft to the MSOA population dataset\ntheft_msoa_sdf &lt;- msoa_population |&gt;\n    left_join(all_theft_df_msoa, by = \"msoa21cd\")\n\n\nTo double-check our join, we want to do one extra step of quality checks and check that each of our MSOAs has at least one occurrence of crime over the twelve month period. We do this by computing a new column that totals the number of thefts. By identifying any MSOAs that have zero entries (NA), we can double-check with our original all_theft_df_msoa to see if this is the correct data for that MSOA or if there has been an error in our join. What we will need to do is adjust the values present within these MSOAs prior to our visualisation analysis: these should not have NA as their value but rather 0.\n\n\n\nR code\n\n# replace all NAs in our dataframe with 0\ntheft_msoa_sdf[is.na(theft_msoa_sdf)] = 0\n\n\nThe final step we need to take before we can map our theft data is, of course, compute a crime rate. We have our pop2021 column within our theft_msoa_sdf dataframe, so now we can add a crime rate as follows:\n\n\n\nR code\n\n# calculate crime rate\ntheft_msoa_sdf &lt;- theft_msoa_sdf |&gt;\n    mutate(crime_rate = (msoa_theft/as.numeric(pop2021)) * 10000)\n\n\nHave a look at your new theft_msoa_sdf spatial dataframe. Does it look as you would expect? Now we have our final dataframe, we can go ahead and make our maps.\n\n\n\nFor making our maps, we will be using one of two main visualisation libraries that can be used for spatial data: tmap. tmap is a library written around thematic map visualisation. The package offers a flexible, layer-based, and easy to use approach to create thematic maps, such as choropleths and bubble maps. What is really great about tmap is that it comes with one quick plotting method for a map called: qtm().\nWe can use this function to plot the theft crime rate really quickly. Within your script, use the qtm function to create a map of theft crime rate in London in 2021.\n\n\n\n\n\n\nBefore continuing do confirm whether your theft_msoa_sdf is indeed still of class sf. In some instances it is possible that this changed when manipulating the attributes. You can simply check this by running class(theft_msoa_sdf). If your dataframe is not of class sf, you can force it into one by running theft_msoa_sdf &lt;- st_as_sf(theft_msoa_sdf)).\n\n\n\n\n\n\nR code\n\n# quick thematic map\nqtm(theft_msoa_sdf, fill = \"crime_rate\")\n\n\n\n\n\nFigure 1: Quick thematic map.\n\n\n\n\nIn this case, the fill() argument is how we tell tmap to create a choropleth map based on the values in the column we provide it with. If we simply set it to NULL, we would only draw the borders of our polygons. Within our qtm function, we can pass quite a few different parameters that would enable us to change specific aesthetics of our map. If you look up the documentation for the function, you will see a list of these parameters. We can, for example, set the lines of our MSOA polygons to white by adding the borders parameter:\n\n\n\nR code\n\n# quick thematic map\nqtm(theft_msoa_sdf, fill = \"crime_rate\", borders = \"white\")\n\n\n\n\n\nFigure 2: Quick thematic map with white borders.\n\n\n\n\nThe map does not really look great. We can continue to add and change parameters in our qtm() function to create a map we are satisfied with. However, the issue with the qtm() function is that it is quite limited in its functionality and mostly used to quickly inspect your data. Instead, when we want to develop more complex maps using the tmap library, we want to use their main plotting method which uses a function called tm_shape(), which we build on using the layered grammar of graphics approach.\n\n\n\n\n\n\nWhen it comes to setting colours within a map or any graphic, we can either pass through a colour as a word, a HEX code or a pre-defined palette. You can find out more here, which is a great quick reference to just some of the possible colours and palettes you will be able to use in R.\n\n\n\nThe main approach to creating maps in tmap is to use the grammar of graphics to build up a map based on what is called the tm_shape() function. Essentially this function, when populated with a spatial dataframe, takes the spatial information of our data (including the projection and geometry of our data) and creates a spatial object. This object contains some information about our original spatial dataframe that we can override (such as the projection) within this function’s parameters, but ultimately, by using this function, you are instructing R that this is the object from which to “draw my shape”.\nTo actually draw the shape, we next need to add a layer to specify the type of shape we want R to draw from this information - in our case, our polygon data. We need to add a function therefore that tells R to “draw my spatial object as X” and within this “layer”, you can also specific additional information to tell R how to draw your layer. You can then add in additional layers, including other spatial objects (and their related shapes) that you want drawn on your map, plus a specify your layout options through a layout layer.\nLet’s see how we can build up our first map in tmap.\n\n\n\nR code\n\n# shape, polygons\ntm_shape(theft_msoa_sdf) + tm_polygons()\n\n\n\n\n\nFigure 3: Building up a map layer by layer.\n\n\n\n\nAs you should now see, we have now mapped the spatial polygons of our theft_msoa_sdf spatial dataframe. However, this is not the map we want: we want to have our polygons represented by a choropleth map where the colours reflect the theft crime rate, rather than the default grey polygons we see before us. To do so, we use the col= parameter that is within our tm_polygons() shape.\n\n\n\n\n\n\nThe col parameter within tm_polygons() is used to fill our polygons with a specific fill type, of either:\n\na single color value (e.g. red)\nthe name of a data variable that is contained in the spatial data file Either the data variable contains color values, or values (numeric or categorical) that will be depicted by a specific color palette.\n\n\n\n\nLet’s go ahead and pass our crime_rate column within the col() parameter and see what we get:\n\n\n\nR code\n\n# shape, polygons\ntm_shape(theft_msoa_sdf) +\n  # specify column\n  tm_polygons(\n    col = \"crime_rate\"\n  )\n\n\n\n\n\nFigure 4: Building up a map layer by layer.\n\n\n\n\nWe are slowly getting there. But there are two things we can notice straight away that do not look right about our data. The first is that our classification breaks do not really reflect the variation in our dataset. This is because tmap has defaulted to its default break type: pretty breaks, whereas, as we know, using an approach such as natural breaks, aka jenks, may reveal better variation in our data.\nUsing the documentation for tm_polygons(), it looks like the following parameters are relevant to help us create the right classification for our map:\n\nn: state the number of classification breaks you want.\nstyle: state the style of breaks you want, e.g. fixed, sd, equal, quantile.\nbreaks: state the numeric breaks you want to use when using the fixed style approach.\n\nLet’s say we want to change our choropleth map to have five classes, determined via the quantile method. We simply need to add the n and style parameters into our tm_polygons() layer:\n\n\n\nR code\n\n# shape, polygons\ntm_shape(theft_msoa_sdf) +\n  # specify column, classes\n  tm_polygons(\n    col = \"crime_rate\",\n    n = 5,\n    style = \"quantile\"\n  )\n\n\n\n\n\nFigure 5: Building up a map layer by layer.\n\n\n\n\nWe now have a choropleth that reflects better the distribution of our data, but we can make them a little prettier by rounding the values. To do so, we can change the style of the map to fixed and then supply a new argument for breaks that contains the rounded classification breaks:\n\n\n\nR code\n\n# shape, polygons\ntm_shape(theft_msoa_sdf) +\n  # specify column, classes, breaks\n  tm_polygons(\n    col = \"crime_rate\",\n    n = 5,\n    style = \"fixed\",\n    breaks = c(0, 5, 10, 15, 50, 5000)\n  )\n\n\n\n\n\nFigure 6: Building up a map layer by layer.\n\n\n\n\nThat looks a little better from the classification side of things.\n\n\n\nTo style our map takes a further understanding and familiarity with our tmap library, but it is only something you will only really learn by having to make your own maps. As a result, we will not go into explaining exactly every aspect of map styling but instead provide you with some example code that you can use as well as experiment with to try to see how you can adjust aspects of the map to your preferences.\nFundamentally, the key functions to be aware of:\n\ntm_layout(): contains parameters to style titles, fonts, the legend;\ntm_compass(): contains parameters to create and style a North arrow or compass;\ntm_scale_bar(): contains parameters to create and style a scale bar.\n\nTo be able to start styling our map, we need to interrogate each of these functions and their parameters to trial and error options to ultimately create a map we are happy with:\n\n\n\nR code\n\n# shape, polygon\ntm_shape(theft_msoa_sdf) +\n  # specify column, classes, breaks, borders, legend title\n  tm_polygons(\n    col = \"crime_rate\",\n    n = 5,\n    style = \"fixed\",\n    breaks = c(0, 5, 10, 15, 50, 5000),\n    palette = \"Blues\",\n    title = \"Rate per 10,000 people\"\n  ) +\n  # add title\n  tm_layout(\n    main.title = \"Theft in London - 2021\",\n    main.title.fontface = 2,\n    fontfamily = \"Helvetica\",\n    legend.outside = TRUE,\n    legend.position = c(\"left\", \"top\"),\n    legend.title.size = 1,\n    legend.title.fontface = 2\n  ) +\n  # add North arrow\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"bottom\")\n  ) +\n  # add scale bar\n  tm_scale_bar(\n    breaks = c(0, 5, 10, 15, 20),\n    position = c(\"left\", \"bottom\")\n  )\n\n\n\n\n\nFigure 7: Building up a map layer by layer.\n\n\n\n\n\n\n\nOnce we are finished making our map, we can go ahead and export it to our maps folder. To do so, we need to save our map-making code to as a variable and then use the tmap_save() function to save the output of this code to a picture within our maps folder.\n\n\n\nR code\n\n# shape, polygons, specify column, specify classes, specify breaks, map elements\nmsoa_map &lt;-\n  # shape, polygon\n  tm_shape(theft_msoa_sdf) +\n  # specify column, classes, breaks, borders, legend title\n  tm_polygons(\n    col = \"crime_rate\",\n    n = 5,\n    style = \"fixed\",\n    breaks = c(0, 5, 10, 15, 50, 5000),\n    palette = \"Blues\",\n    title = \"Rate per 10,000 people\"\n  ) +\n  # add title\n  tm_layout(\n    main.title = \"Theft - 2021\",\n    main.title.fontface = 2,\n    fontfamily = \"Helvetica\",\n    legend.outside = TRUE,\n    legend.position = c(\"left\", \"top\"),\n    legend.title.size = 1,\n    legend.title.fontface = 2\n  ) +\n  # add North arrow\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"bottom\")\n  ) +\n  # add scale bar\n  tm_scale_bar(\n    breaks = c(0, 5, 10, 15, 20),\n    position = c(\"left\", \"bottom\")\n  )\n\n# save as image\ntmap_save(msoa_map, filename = \"data/maps/msoa_theft_map.png\")"
  },
  {
    "objectID": "05-spatial.html#assignment-w05",
    "href": "05-spatial.html#assignment-w05",
    "title": "1 Programming for Spatial Analysis",
    "section": "",
    "text": "Now we have prepared our dataset and made our first maps in R, we can play with the different settings.\n\nExperiment by changing the colours of the map, changing the legend title name, changing the type of North arrow, etc.\nWe have used a quantile method to classify our data. Do you think that is appropriate? Any other ways you could think of on representing these MSOA crime rates better?"
  },
  {
    "objectID": "05-spatial.html#wm-w05",
    "href": "05-spatial.html#wm-w05",
    "title": "1 Programming for Spatial Analysis",
    "section": "",
    "text": "Git is a version control system, originally developed to help groups of developers work collaboratively on big software projects. One way to think about it is in terms of ‘Track Changes’ used for documents, only this time it is applied to code - and much more powerful. A great resource to help you get started with Git is Happy Git and GitHub for the useR. Highly recommended."
  },
  {
    "objectID": "05-spatial.html#byl-w05",
    "href": "05-spatial.html#byl-w05",
    "title": "1 Programming for Spatial Analysis",
    "section": "",
    "text": "And that is how you use R as a GIS in its most basic form. More RGIS in the coming weeks, but this concludes the tutorial for this week. As it is reading week next week, it is probably a good idea to turn your attention to the articles and chapters on the reading list!"
  },
  {
    "objectID": "11-data.html",
    "href": "11-data.html",
    "title": "1 Data Sources",
    "section": "",
    "text": "Below you will find a list of resources that you might want to explore when sourcing data for your coursework assignment or your dissertation. This is by no means an exhaustive list, but simply contains some suggestions of websites that you may want to use.\n\n\n\n\n\n\nYou are not limited to using these datasets for your coursework assignment or your dissertation, but these are merely some suggestions .\n\n\n\n\n\nThe following websites contain Open Data or link to Open Data from several respectable data providers:\n\nAfricanUrbanNetwork\nAirBnB Data\nBike Docking Data (ready for R)\nBing Maps worldwide road detections\nCamden Air Action\nConsumer Data Research Centre\nDepartment for Environment, Food & Rural Affairs\nEdina (e.g. OS mastermap)\nEU Tourism Data\nEurostat\nGeofabrik (OSM data)\nGlobal Urban Areas dataset\nGlobal Weather Data\nGoogle Dataset Search\nGoogle Open Buildings\nKaggle Public Datasets\nKing’s College Data on Air Pollution\nLondon Data Store\nLondon Local Authority Maintained Trees\nLondon Tube PM2.5 Levels\nMicrosoft Global Building Footprints\nMicrosoft Research Open Data\nNational Public Transport Access Nodes (NaPTAN)\nNASA EARTHDATA\nNASA SocioEconomic Data and Applications Center (SEDAC)\nNHS Data (ready for R)\nnomis Official Census and Labour Market Statistics\nOffice for National Statistics Geoportal\nOffice for National Statistics\nOpen Topography\nPlanetary Computer Data Catalog\npseudo Census Output Areas 2001-2011-2021\nPublic transport accessibility indicators Great Britain\nTesco Store Data (London)\nTfL Cycling Data\nTfL Open Data\nTidy Tuesday Data (not exclusively spatial data)\nUK Data Service\nUS Census Data\nUS City Open Data Census\nUSGS Earth Explorer\nUTD19 Multi-City Traffic Dataset\nWorldPop GitHub\nWorldPop\n\nSome other websites that could be helpful:\n\nAwesome Public Datasets; general collection of datasets, although not limited to spatial data.\nFree GIS data; long list with lots of GIS datasets on many different topics and covering many different areas.\n\n\n\n\nUndergraduate students can also apply for a Safeguarded dataset held by the Consumer Data Research Centre. There is a process to access these Safeguarded datasets, which is detailed on the CDRC website. Please be aware that it normally takes 4-5 weeks for your application to be processed.\nAs part of the process, you will need to state in your application why you want that specific dataset and what you are planning to do with it. You will also need to have at least thought about the ethical implications of using that data and provide this with your data application.\nSome of the datasets held by the CDRC that you can apply for are:\n\nBicycle Sharing System Docking Station Observations\nCDRC Modelled Ethnicity Proportions - LSOA Geography\nFCA Financial Lives Survey\nSpeedchecker Broadband Internet Speed Tests\n\n\n\n\n\n\n\nGiven that the application can take several weeks, the Safeguarded CDRC datasets may be useful for your undergraduate dissertation but probably not for the GEOG0030 coursework assignment. However, any of the CDRC datasets that are marked as Open Data do not require this application process and you can download these datasets directly after registering on the website."
  },
  {
    "objectID": "11-data.html#open-data",
    "href": "11-data.html#open-data",
    "title": "1 Data Sources",
    "section": "",
    "text": "The following websites contain Open Data or link to Open Data from several respectable data providers:\n\nAfricanUrbanNetwork\nAirBnB Data\nBike Docking Data (ready for R)\nBing Maps worldwide road detections\nCamden Air Action\nConsumer Data Research Centre\nDepartment for Environment, Food & Rural Affairs\nEdina (e.g. OS mastermap)\nEU Tourism Data\nEurostat\nGeofabrik (OSM data)\nGlobal Urban Areas dataset\nGlobal Weather Data\nGoogle Dataset Search\nGoogle Open Buildings\nKaggle Public Datasets\nKing’s College Data on Air Pollution\nLondon Data Store\nLondon Local Authority Maintained Trees\nLondon Tube PM2.5 Levels\nMicrosoft Global Building Footprints\nMicrosoft Research Open Data\nNational Public Transport Access Nodes (NaPTAN)\nNASA EARTHDATA\nNASA SocioEconomic Data and Applications Center (SEDAC)\nNHS Data (ready for R)\nnomis Official Census and Labour Market Statistics\nOffice for National Statistics Geoportal\nOffice for National Statistics\nOpen Topography\nPlanetary Computer Data Catalog\npseudo Census Output Areas 2001-2011-2021\nPublic transport accessibility indicators Great Britain\nTesco Store Data (London)\nTfL Cycling Data\nTfL Open Data\nTidy Tuesday Data (not exclusively spatial data)\nUK Data Service\nUS Census Data\nUS City Open Data Census\nUSGS Earth Explorer\nUTD19 Multi-City Traffic Dataset\nWorldPop GitHub\nWorldPop\n\nSome other websites that could be helpful:\n\nAwesome Public Datasets; general collection of datasets, although not limited to spatial data.\nFree GIS data; long list with lots of GIS datasets on many different topics and covering many different areas."
  },
  {
    "objectID": "11-data.html#safeguarded-data",
    "href": "11-data.html#safeguarded-data",
    "title": "1 Data Sources",
    "section": "",
    "text": "Undergraduate students can also apply for a Safeguarded dataset held by the Consumer Data Research Centre. There is a process to access these Safeguarded datasets, which is detailed on the CDRC website. Please be aware that it normally takes 4-5 weeks for your application to be processed.\nAs part of the process, you will need to state in your application why you want that specific dataset and what you are planning to do with it. You will also need to have at least thought about the ethical implications of using that data and provide this with your data application.\nSome of the datasets held by the CDRC that you can apply for are:\n\nBicycle Sharing System Docking Station Observations\nCDRC Modelled Ethnicity Proportions - LSOA Geography\nFCA Financial Lives Survey\nSpeedchecker Broadband Internet Speed Tests\n\n\n\n\n\n\n\nGiven that the application can take several weeks, the Safeguarded CDRC datasets may be useful for your undergraduate dissertation but probably not for the GEOG0030 coursework assignment. However, any of the CDRC datasets that are marked as Open Data do not require this application process and you can download these datasets directly after registering on the website."
  }
]